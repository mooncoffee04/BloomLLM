{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13614106,"sourceType":"datasetVersion","datasetId":8651939}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:52:12.521207Z","iopub.execute_input":"2025-11-04T19:52:12.521501Z","iopub.status.idle":"2025-11-04T19:52:13.242075Z","shell.execute_reply.started":"2025-11-04T19:52:12.521475Z","shell.execute_reply":"2025-11-04T19:52:13.241174Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"/kaggle/input/capstone-phase-1/checkpoint_labeled_200.csv\n/kaggle/input/capstone-phase-1/jury_results.csv\n/kaggle/input/capstone-phase-1/labeled_sample.csv\n/kaggle/input/capstone-phase-1/jury_final_normalized.csv\n/kaggle/input/capstone-phase-1/bcls.csv\n/kaggle/input/capstone-phase-1/state.db\n/kaggle/input/capstone-phase-1/checkpoint_labeled_400.csv\n/kaggle/input/capstone-phase-1/checkpoint_asap_sample_20251103_184619_meta.json\n/kaggle/input/capstone-phase-1/checkpoint_final_labeled_500_20251103_211458_meta.json\n/kaggle/input/capstone-phase-1/jury_final.csv\n/kaggle/input/capstone-phase-1/checkpoint_labeled_300.csv\n/kaggle/input/capstone-phase-1/checkpoint_final_labeled_500_20251103_211458.pkl\n/kaggle/input/capstone-phase-1/checkpoint_splits_SMALL.pkl\n/kaggle/input/capstone-phase-1/checkpoint_splits_20251103_211458_meta.json\n/kaggle/input/capstone-phase-1/checkpoint_splits_20251103_211458.pkl\n/kaggle/input/capstone-phase-1/checkpoint_labeled_100.csv\n/kaggle/input/capstone-phase-1/checkpoint_labeled_500.csv\n/kaggle/input/capstone-phase-1/checkpoint_asap_sample_20251103_184619.pkl\n/kaggle/input/capstone-phase-1/models/qwen_lora/adapter_model.safetensors\n/kaggle/input/capstone-phase-1/models/qwen_lora/merges.txt\n/kaggle/input/capstone-phase-1/models/qwen_lora/adapter_config.json\n/kaggle/input/capstone-phase-1/models/qwen_lora/README.md\n/kaggle/input/capstone-phase-1/models/qwen_lora/tokenizer.json\n/kaggle/input/capstone-phase-1/models/qwen_lora/vocab.json\n/kaggle/input/capstone-phase-1/models/qwen_lora/tokenizer_config.json\n/kaggle/input/capstone-phase-1/models/qwen_lora/chat_template.jinja\n/kaggle/input/capstone-phase-1/models/qwen_lora/special_tokens_map.json\n/kaggle/input/capstone-phase-1/models/qwen_lora/added_tokens.json\n/kaggle/input/capstone-phase-1/models/qwen_lora/checkpoint-300/adapter_model.safetensors\n/kaggle/input/capstone-phase-1/models/qwen_lora/checkpoint-300/trainer_state.json\n/kaggle/input/capstone-phase-1/models/qwen_lora/checkpoint-300/training_args.bin\n/kaggle/input/capstone-phase-1/models/qwen_lora/checkpoint-300/adapter_config.json\n/kaggle/input/capstone-phase-1/models/qwen_lora/checkpoint-300/README.md\n/kaggle/input/capstone-phase-1/models/qwen_lora/checkpoint-300/scaler.pt\n/kaggle/input/capstone-phase-1/models/qwen_lora/checkpoint-300/scheduler.pt\n/kaggle/input/capstone-phase-1/models/qwen_lora/checkpoint-300/optimizer.pt\n/kaggle/input/capstone-phase-1/models/qwen_lora/checkpoint-300/rng_state.pth\n/kaggle/input/capstone-phase-1/models/phi3_lora/adapter_model.safetensors\n/kaggle/input/capstone-phase-1/models/phi3_lora/adapter_config.json\n/kaggle/input/capstone-phase-1/models/phi3_lora/README.md\n/kaggle/input/capstone-phase-1/models/phi3_lora/tokenizer.json\n/kaggle/input/capstone-phase-1/models/phi3_lora/tokenizer_config.json\n/kaggle/input/capstone-phase-1/models/phi3_lora/chat_template.jinja\n/kaggle/input/capstone-phase-1/models/phi3_lora/special_tokens_map.json\n/kaggle/input/capstone-phase-1/models/phi3_lora/tokenizer.model\n/kaggle/input/capstone-phase-1/models/phi3_lora/added_tokens.json\n/kaggle/input/capstone-phase-1/models/phi3_lora/checkpoint-300/adapter_model.safetensors\n/kaggle/input/capstone-phase-1/models/phi3_lora/checkpoint-300/trainer_state.json\n/kaggle/input/capstone-phase-1/models/phi3_lora/checkpoint-300/training_args.bin\n/kaggle/input/capstone-phase-1/models/phi3_lora/checkpoint-300/adapter_config.json\n/kaggle/input/capstone-phase-1/models/phi3_lora/checkpoint-300/README.md\n/kaggle/input/capstone-phase-1/models/phi3_lora/checkpoint-300/scaler.pt\n/kaggle/input/capstone-phase-1/models/phi3_lora/checkpoint-300/scheduler.pt\n/kaggle/input/capstone-phase-1/models/phi3_lora/checkpoint-300/optimizer.pt\n/kaggle/input/capstone-phase-1/models/phi3_lora/checkpoint-300/rng_state.pth\n/kaggle/input/capstone-phase-1/models/mistral_lora/adapter_model.safetensors\n/kaggle/input/capstone-phase-1/models/mistral_lora/adapter_config.json\n/kaggle/input/capstone-phase-1/models/mistral_lora/README.md\n/kaggle/input/capstone-phase-1/models/mistral_lora/tokenizer.json\n/kaggle/input/capstone-phase-1/models/mistral_lora/tokenizer_config.json\n/kaggle/input/capstone-phase-1/models/mistral_lora/chat_template.jinja\n/kaggle/input/capstone-phase-1/models/mistral_lora/special_tokens_map.json\n/kaggle/input/capstone-phase-1/models/mistral_lora/tokenizer.model\n/kaggle/input/capstone-phase-1/models/mistral_lora/checkpoint-300/adapter_model.safetensors\n/kaggle/input/capstone-phase-1/models/mistral_lora/checkpoint-300/trainer_state.json\n/kaggle/input/capstone-phase-1/models/mistral_lora/checkpoint-300/training_args.bin\n/kaggle/input/capstone-phase-1/models/mistral_lora/checkpoint-300/adapter_config.json\n/kaggle/input/capstone-phase-1/models/mistral_lora/checkpoint-300/README.md\n/kaggle/input/capstone-phase-1/models/mistral_lora/checkpoint-300/scaler.pt\n/kaggle/input/capstone-phase-1/models/mistral_lora/checkpoint-300/scheduler.pt\n/kaggle/input/capstone-phase-1/models/mistral_lora/checkpoint-300/optimizer.pt\n/kaggle/input/capstone-phase-1/models/mistral_lora/checkpoint-300/rng_state.pth\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225156-0gypkdow/run-0gypkdow.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225156-0gypkdow/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225156-0gypkdow/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225156-0gypkdow/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225156-0gypkdow/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225156-0gypkdow/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225156-0gypkdow/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225156-0gypkdow/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230422-vs6laz4j/run-vs6laz4j.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230422-vs6laz4j/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230422-vs6laz4j/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230422-vs6laz4j/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230422-vs6laz4j/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230422-vs6laz4j/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230422-vs6laz4j/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230422-vs6laz4j/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222052-3s1qxy4z/run-3s1qxy4z.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222052-3s1qxy4z/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222052-3s1qxy4z/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222052-3s1qxy4z/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222052-3s1qxy4z/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222052-3s1qxy4z/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222052-3s1qxy4z/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222052-3s1qxy4z/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224241-ihsd5qdk/run-ihsd5qdk.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224241-ihsd5qdk/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224241-ihsd5qdk/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224241-ihsd5qdk/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224241-ihsd5qdk/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224241-ihsd5qdk/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224241-ihsd5qdk/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224241-ihsd5qdk/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225327-r24jez2i/run-r24jez2i.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225327-r24jez2i/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225327-r24jez2i/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225327-r24jez2i/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225327-r24jez2i/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225327-r24jez2i/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221947-errv8k3k/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224612-c4469d7y/run-c4469d7y.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224612-c4469d7y/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224612-c4469d7y/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224612-c4469d7y/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224612-c4469d7y/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224612-c4469d7y/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222156-xn69zz0t/run-xn69zz0t.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222156-xn69zz0t/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222156-xn69zz0t/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222156-xn69zz0t/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222156-xn69zz0t/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222156-xn69zz0t/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222156-xn69zz0t/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222156-xn69zz0t/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222415-dvg4ha0l/run-dvg4ha0l.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222415-dvg4ha0l/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222415-dvg4ha0l/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222415-dvg4ha0l/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222415-dvg4ha0l/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222415-dvg4ha0l/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222415-dvg4ha0l/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222415-dvg4ha0l/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223449-u4trhdmg/run-u4trhdmg.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223449-u4trhdmg/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223449-u4trhdmg/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223449-u4trhdmg/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223449-u4trhdmg/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223449-u4trhdmg/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223449-u4trhdmg/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223449-u4trhdmg/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221730-micinogz/run-micinogz.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221730-micinogz/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221730-micinogz/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221730-micinogz/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221730-micinogz/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221730-micinogz/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221730-micinogz/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221730-micinogz/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223534-w2c1wfrq/run-w2c1wfrq.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223534-w2c1wfrq/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223534-w2c1wfrq/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223534-w2c1wfrq/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223534-w2c1wfrq/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223534-w2c1wfrq/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223534-w2c1wfrq/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223534-w2c1wfrq/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230118-r04n87f6/run-r04n87f6.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230118-r04n87f6/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230118-r04n87f6/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230118-r04n87f6/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230118-r04n87f6/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230118-r04n87f6/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230118-r04n87f6/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230118-r04n87f6/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_220948-qxzt69d9/run-qxzt69d9.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_220948-qxzt69d9/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_220948-qxzt69d9/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_220948-qxzt69d9/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_220948-qxzt69d9/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_220948-qxzt69d9/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_220948-qxzt69d9/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_220948-qxzt69d9/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223135-j5myym2z/run-j5myym2z.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223135-j5myym2z/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223135-j5myym2z/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223135-j5myym2z/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223135-j5myym2z/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223135-j5myym2z/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223135-j5myym2z/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223135-j5myym2z/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224541-mec3xxlj/run-mec3xxlj.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224541-mec3xxlj/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224541-mec3xxlj/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224541-mec3xxlj/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224541-mec3xxlj/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224541-mec3xxlj/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224541-mec3xxlj/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224541-mec3xxlj/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224333-jtwxdpve/run-jtwxdpve.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224333-jtwxdpve/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224333-jtwxdpve/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224333-jtwxdpve/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224333-jtwxdpve/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224333-jtwxdpve/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224333-jtwxdpve/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224333-jtwxdpve/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221252-dko7j0kg/run-dko7j0kg.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221252-dko7j0kg/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221252-dko7j0kg/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221252-dko7j0kg/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221252-dko7j0kg/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221252-dko7j0kg/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221252-dko7j0kg/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_221252-dko7j0kg/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222940-awiehof3/run-awiehof3.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222940-awiehof3/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222940-awiehof3/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222940-awiehof3/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222940-awiehof3/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222940-awiehof3/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222940-awiehof3/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222940-awiehof3/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230126-t0yakk13/run-t0yakk13.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230126-t0yakk13/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230126-t0yakk13/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230126-t0yakk13/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224940-mvl0iiy1/run-mvl0iiy1.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224940-mvl0iiy1/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224940-mvl0iiy1/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224940-mvl0iiy1/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224940-mvl0iiy1/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224940-mvl0iiy1/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224940-mvl0iiy1/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_224940-mvl0iiy1/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225045-dluzy0il/run-dluzy0il.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225045-dluzy0il/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225045-dluzy0il/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225045-dluzy0il/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225045-dluzy0il/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225045-dluzy0il/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225045-dluzy0il/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_225045-dluzy0il/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223358-wrmeadzz/run-wrmeadzz.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223358-wrmeadzz/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223358-wrmeadzz/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223358-wrmeadzz/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223358-wrmeadzz/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223358-wrmeadzz/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223358-wrmeadzz/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223358-wrmeadzz/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222910-0uv3vr96/run-0uv3vr96.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222910-0uv3vr96/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222910-0uv3vr96/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222910-0uv3vr96/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222910-0uv3vr96/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222910-0uv3vr96/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222910-0uv3vr96/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_222910-0uv3vr96/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223143-xy700bpe/run-xy700bpe.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223143-xy700bpe/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223143-xy700bpe/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223143-xy700bpe/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223143-xy700bpe/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223143-xy700bpe/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223143-xy700bpe/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223143-xy700bpe/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230109-clc1aaat/run-clc1aaat.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230109-clc1aaat/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230109-clc1aaat/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230109-clc1aaat/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230109-clc1aaat/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230109-clc1aaat/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230109-clc1aaat/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_230109-clc1aaat/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251104_014804-4lg3n47a/run-4lg3n47a.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251104_014804-4lg3n47a/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251104_014804-4lg3n47a/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251104_014804-4lg3n47a/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251104_014804-4lg3n47a/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251104_014804-4lg3n47a/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251104_014804-4lg3n47a/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251104_014804-4lg3n47a/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223127-5ke5qob7/run-5ke5qob7.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223127-5ke5qob7/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223127-5ke5qob7/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223127-5ke5qob7/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223127-5ke5qob7/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223127-5ke5qob7/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223127-5ke5qob7/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223127-5ke5qob7/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251104_062049-4zmu4947/run-4zmu4947.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251104_062049-4zmu4947/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251104_062049-4zmu4947/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251104_062049-4zmu4947/files/wandb-summary.json\n/kaggle/input/capstone-phase-1/wandb/run-20251104_062049-4zmu4947/files/config.yaml\n/kaggle/input/capstone-phase-1/wandb/run-20251104_062049-4zmu4947/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251104_062049-4zmu4947/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251104_062049-4zmu4947/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223613-lle297qw/run-lle297qw.wandb\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223613-lle297qw/logs/debug.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223613-lle297qw/logs/debug-internal.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223613-lle297qw/files/output.log\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223613-lle297qw/files/requirements.txt\n/kaggle/input/capstone-phase-1/wandb/run-20251103_223613-lle297qw/files/wandb-metadata.json\n/kaggle/input/capstone-phase-1/data/blooms_taxonomy_cognitive_levels.csv\n/kaggle/input/capstone-phase-1/full_project_outputs/checkpoint_labeled_200.csv\n/kaggle/input/capstone-phase-1/full_project_outputs/labeled_sample.csv\n/kaggle/input/capstone-phase-1/full_project_outputs/bcls.csv\n/kaggle/input/capstone-phase-1/full_project_outputs/checkpoint_labeled_400.csv\n/kaggle/input/capstone-phase-1/full_project_outputs/checkpoint_asap_sample_20251103_184619_meta.json\n/kaggle/input/capstone-phase-1/full_project_outputs/checkpoint_final_labeled_500_20251103_211458_meta.json\n/kaggle/input/capstone-phase-1/full_project_outputs/checkpoint_labeled_300.csv\n/kaggle/input/capstone-phase-1/full_project_outputs/checkpoint_final_labeled_500_20251103_211458.pkl\n/kaggle/input/capstone-phase-1/full_project_outputs/checkpoint_splits_20251103_211458_meta.json\n/kaggle/input/capstone-phase-1/full_project_outputs/checkpoint_splits_20251103_211458.pkl\n/kaggle/input/capstone-phase-1/full_project_outputs/checkpoint_labeled_100.csv\n/kaggle/input/capstone-phase-1/full_project_outputs/checkpoint_labeled_500.csv\n/kaggle/input/capstone-phase-1/full_project_outputs/checkpoint_asap_sample_20251103_184619.pkl\n/kaggle/input/capstone-phase-1/full_project_outputs/data/blooms_taxonomy_cognitive_levels.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Replace this:\n# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# With this:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntry:\n    from transformers import BitsAndBytesConfig\n    HAS_BNB = True\nexcept Exception:\n    HAS_BNB = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:18:45.833058Z","iopub.execute_input":"2025-11-04T18:18:45.833606Z","iopub.status.idle":"2025-11-04T18:18:45.837631Z","shell.execute_reply.started":"2025-11-04T18:18:45.833580Z","shell.execute_reply":"2025-11-04T18:18:45.836842Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:52:20.064441Z","iopub.execute_input":"2025-11-04T19:52:20.064729Z","iopub.status.idle":"2025-11-04T19:52:20.160696Z","shell.execute_reply.started":"2025-11-04T19:52:20.064708Z","shell.execute_reply":"2025-11-04T19:52:20.159897Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install -q bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:00:29.318966Z","iopub.execute_input":"2025-11-04T18:00:29.319471Z","iopub.status.idle":"2025-11-04T18:01:43.939615Z","shell.execute_reply.started":"2025-11-04T18:00:29.319449Z","shell.execute_reply":"2025-11-04T18:01:43.938856Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport json\nimport torch\nimport gc\nfrom tqdm import tqdm\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom scipy.stats import spearmanr, mannwhitneyu, pearsonr\nfrom sklearn.metrics import cohen_kappa_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict, Counter\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:05:08.076136Z","iopub.execute_input":"2025-11-04T18:05:08.076524Z","iopub.status.idle":"2025-11-04T18:05:08.082438Z","shell.execute_reply.started":"2025-11-04T18:05:08.076498Z","shell.execute_reply":"2025-11-04T18:05:08.081648Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\"\"\"\nBloomLLM-Jury: MEMORY-EFFICIENT Evaluation Script\n==================================================\nThis version loads ONE model at a time to avoid GPU OOM errors.\n\nKey difference from original:\n- Load model 1 â†’ Evaluate all essays â†’ Unload â†’ Clear memory\n- Load model 2 â†’ Evaluate all essays â†’ Unload â†’ Clear memory\n- Load model 3 â†’ Evaluate all essays â†’ Unload â†’ Clear memory\n- Then aggregate results\n\nThis is slower but works with limited GPU memory.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport json\nimport torch\nimport gc\nfrom tqdm import tqdm\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom scipy.stats import spearmanr, mannwhitneyu, pearsonr\nfrom sklearn.metrics import cohen_kappa_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict, Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nCONFIG = {\n    'models': [\n        {\n            'name': 'Phi',\n            'base_id': 'microsoft/Phi-3-mini-4k-instruct',\n            'lora_path': '/kaggle/working/models/phi3_lora'\n        },\n        {\n            'name': 'Mistral',\n            'base_id': 'mistralai/Mistral-7B-Instruct-v0.3',\n            'lora_path': '/kaggle/working/models/mistral_lora'\n        },\n        {\n            'name': 'Qwen',\n            'base_id': 'Qwen/Qwen2.5-7B-Instruct',\n            'lora_path': '/kaggle/working/models/qwen_lora'\n        }\n    ],\n    'test_data_path': '/kaggle/working/checkpoint_splits_SMALL.pkl',  # UPDATED to use smaller set\n    'output_dir': '/kaggle/working',\n    'generation_params': {\n        'max_new_tokens': 250,\n        'do_sample': False,  # Greedy decoding for consistency\n    },\n}\n\n# ASAP dataset score ranges per essay set (from official documentation)\nASAP_SCORE_RANGES = {\n    1: (1, 6),    # Argumentative\n    2: (1, 6),    # Argumentative\n    3: (0, 3),    # Source-Dependent\n    4: (0, 3),    # Source-Dependent\n    5: (0, 4),    # Source-Dependent\n    6: (0, 4),    # Source-Dependent\n    7: (0, 3),    # Narrative\n    8: (1, 6),    # Narrative\n}\n\n# ============================================================================\n# MODEL LOADING (ONE AT A TIME)\n# ============================================================================\n\ndef load_single_model(base_id, lora_path):\n    \"\"\"Load a single model with proper memory management.\"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"Loading {base_id}...\")\n    print(f\"{'='*80}\")\n    \n    # Clear any existing models\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # Proper 4-bit quantization config\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n    )\n    \n    # Load base model\n    base_model = AutoModelForCausalLM.from_pretrained(\n        base_id,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n    )\n    \n    print(f\"  âœ“ Base model loaded\")\n    \n    # Apply LoRA\n    model = PeftModel.from_pretrained(base_model, lora_path)\n    print(f\"  âœ“ LoRA adapter applied from {lora_path}\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(base_id, trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    print(f\"  âœ“ Tokenizer loaded\")\n    \n    # Print memory usage\n    if torch.cuda.is_available():\n        mem_allocated = torch.cuda.memory_allocated(0) / 1e9\n        mem_reserved = torch.cuda.memory_reserved(0) / 1e9\n        print(f\"  ğŸ“Š GPU Memory: {mem_allocated:.2f}GB allocated, {mem_reserved:.2f}GB reserved\")\n    \n    return model, tokenizer\n\n\ndef unload_model(model, tokenizer):\n    \"\"\"Properly unload model and free memory.\"\"\"\n    del model\n    del tokenizer\n    torch.cuda.empty_cache()\n    gc.collect()\n    print(f\"  âœ“ Model unloaded, memory cleared\")\n\n\n# ============================================================================\n# SCORE EXTRACTION\n# ============================================================================\n\ndef extract_score_robust(text):\n    \"\"\"Extract score and rationale from model output.\"\"\"\n    original_text = text\n    \n    # Clean up text\n    assistant_markers = ['<|assistant|>', '[/INST]', 'assistant:', 'Assistant:']\n    for marker in assistant_markers:\n        if marker in text:\n            text = text.split(marker)[-1]\n    \n    text = text.strip()\n    \n    # Method 1: JSON\n    json_pattern = r'\\{[^}]*\"score\"\\s*:\\s*\\d+[^}]*\\}'\n    json_matches = re.findall(json_pattern, text, re.DOTALL | re.IGNORECASE)\n    \n    for match in json_matches:\n        try:\n            data = json.loads(match)\n            score = int(data.get('score', 0))\n            \n            if 1 <= score <= 5:\n                rationale = data.get('rationale', data.get('reason', data.get('explanation', '')))\n                return score, rationale, \"JSON\"\n        except:\n            continue\n    \n    # Method 2: Patterns\n    score_patterns = [\n        r'\"score\"\\s*:\\s*(\\d+)',\n        r'score\\s*:\\s*(\\d+)',\n        r'Score\\s*:\\s*(\\d+)',\n        r'\\*\\*Score\\*\\*\\s*:\\s*(\\d+)',\n    ]\n    \n    for pattern in score_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            score = int(match.group(1))\n            if 1 <= score <= 5:\n                return score, \"\", \"Pattern\"\n    \n    # Method 3: Ratio\n    ratio_patterns = [r'\\b([1-5])\\s*/\\s*5\\b', r'\\b([1-5])\\s+out\\s+of\\s+5\\b']\n    for pattern in ratio_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            return int(match.group(1)), \"\", \"Ratio\"\n    \n    # Method 4: Any digit 1-5\n    digit_matches = re.findall(r'\\b([1-5])\\b', text)\n    if digit_matches:\n        return int(digit_matches[0]), \"\", \"Weak\"\n    \n    # Failed\n    return 3, \"Extraction failed\", \"Failed\"\n\n\n# ============================================================================\n# INFERENCE\n# ============================================================================\n\ndef construct_prompt(row):\n    \"\"\"Construct evaluation prompt.\"\"\"\n    system_msg = f\"You are an expert educational evaluator specializing in Bloom's Taxonomy. You evaluate student responses for {row['bloom_level']}-level cognitive skills.\"\n    \n    user_msg = f\"\"\"Evaluate this student response for {row['bloom_level']}-level thinking according to Bloom's Taxonomy.\n\n**Prompt**: {row['prompt']}\n\n**Student Response**: {row['response']}\n\nProvide your evaluation in this EXACT JSON format:\n{{\"score\": <integer 1-5>, \"rationale\": \"<brief explanation>\"}}\n\n**Scoring Guide**:\n- 5: Excellent - Fully demonstrates {row['bloom_level']}-level thinking\n- 4: Good - Strong demonstration with minor gaps\n- 3: Adequate - Meets basic requirements\n- 2: Weak - Minimal {row['bloom_level']}-level thinking\n- 1: Poor - Does not demonstrate {row['bloom_level']}-level thinking\n\nRespond ONLY with the JSON object.\"\"\"\n\n    return [\n        {\"role\": \"system\", \"content\": system_msg},\n        {\"role\": \"user\", \"content\": user_msg}\n    ]\n\n\ndef evaluate_with_model(model, tokenizer, row, config):\n    \"\"\"Evaluate a single essay with a model.\"\"\"\n    messages = construct_prompt(row)\n    \n    try:\n        inputs = tokenizer.apply_chat_template(\n            messages,\n            return_tensors=\"pt\",\n            add_generation_prompt=True,\n            padding=True,\n            truncation=True\n        ).to(model.device)\n    except Exception as e:\n        return {\n            'score': 3,\n            'rationale': f\"Tokenization error: {e}\",\n            'extraction_method': 'Error',\n            'raw_output': ''\n        }\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs,\n            max_new_tokens=config['generation_params']['max_new_tokens'],\n            do_sample=config['generation_params']['do_sample'],\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n            use_cache=False,  # Fix for DynamicCache error\n        )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    score, rationale, method = extract_score_robust(response)\n    \n    return {\n        'score': score,\n        'rationale': rationale,\n        'extraction_method': method,\n        'raw_output': response\n    }\n\n\n# ============================================================================\n# MAIN EVALUATION (ONE MODEL AT A TIME)\n# ============================================================================\n\ndef evaluate_jury_memory_efficient(test_df, model_configs, config):\n    \"\"\"\n    Evaluate jury by loading ONE model at a time.\n    This avoids GPU OOM errors.\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"MEMORY-EFFICIENT JURY EVALUATION\")\n    print(\"=\"*80)\n    print(\"\\nStrategy: Load â†’ Evaluate â†’ Unload â†’ Repeat for each model\")\n    print(\"This is slower but works with limited GPU memory.\\n\")\n    \n    # Storage for all results\n    all_model_results = {}  # {model_name: [scores for each essay]}\n    all_rationales = {}     # {model_name: [rationales for each essay]}\n    all_extraction_methods = {}  # {model_name: [methods for each essay]}\n    \n    # Evaluate with each model\n    for model_config in model_configs:\n        model_name = model_config['name']\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"EVALUATING WITH {model_name}\")\n        print(f\"{'='*80}\")\n        \n        # Load this model\n        model, tokenizer = load_single_model(\n            model_config['base_id'],\n            model_config['lora_path']\n        )\n        \n        # Evaluate all essays with this model\n        model_scores = []\n        model_rationales = []\n        model_methods = []\n        \n        print(f\"\\nEvaluating {len(test_df)} essays...\")\n        \n        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=f\"{model_name}\"):\n            result = evaluate_with_model(model, tokenizer, row, config)\n            \n            model_scores.append(result['score'])\n            model_rationales.append(result['rationale'])\n            model_methods.append(result['extraction_method'])\n        \n        # Store results\n        all_model_results[model_name] = model_scores\n        all_rationales[model_name] = model_rationales\n        all_extraction_methods[model_name] = model_methods\n        \n        # Print summary\n        extraction_success = sum(1 for m in model_methods if m in ['JSON', 'Pattern']) / len(model_methods) * 100\n        print(f\"\\n  ğŸ“Š {model_name} Summary:\")\n        print(f\"     Extraction success: {extraction_success:.1f}%\")\n        print(f\"     Mean score: {np.mean(model_scores):.2f} Â± {np.std(model_scores):.2f}\")\n        print(f\"     Score distribution: {Counter(model_scores)}\")\n        \n        # Unload this model before loading next\n        unload_model(model, tokenizer)\n    \n    # Aggregate jury results\n    print(f\"\\n{'='*80}\")\n    print(\"AGGREGATING JURY DECISIONS\")\n    print(f\"{'='*80}\")\n    \n    jury_scores = []\n    for i in range(len(test_df)):\n        essay_scores = [all_model_results[name][i] for name in all_model_results.keys()]\n        jury_score = round(np.mean(essay_scores))\n        jury_scores.append(jury_score)\n    \n    # Compile results\n    results = {\n        'individual_scores': all_model_results,\n        'jury_scores': jury_scores,\n        'rationales': [],\n        'extraction_methods': all_extraction_methods,\n        'true_scores': test_df['essay_score'].tolist(),\n        'bloom_levels': test_df['bloom_level'].tolist(),\n        'essay_sets': test_df['essay_set'].tolist() if 'essay_set' in test_df.columns else [None] * len(test_df),\n    }\n    \n    # Format rationales\n    for i in range(len(test_df)):\n        essay_rationales = {name: all_rationales[name][i] for name in all_rationales.keys()}\n        results['rationales'].append(essay_rationales)\n    \n    return results\n\n\n# ============================================================================\n# ANALYSIS FUNCTIONS (same as before)\n# ============================================================================\n\ndef calculate_inter_jury_agreement(results, model_names):\n    \"\"\"Calculate Cohen's kappa between models.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"INTER-JURY AGREEMENT\")\n    print(\"=\"*80)\n    \n    pairwise_kappas = {}\n    for i, name1 in enumerate(model_names):\n        for j, name2 in enumerate(model_names):\n            if i < j:\n                kappa = cohen_kappa_score(\n                    results['individual_scores'][name1],\n                    results['individual_scores'][name2],\n                    weights='linear'\n                )\n                pairwise_kappas[f\"{name1}-{name2}\"] = kappa\n                print(f\"  {name1:10s} vs {name2:10s}: Îº = {kappa:.3f}\")\n    \n    mean_kappa = np.mean(list(pairwise_kappas.values()))\n    print(f\"\\n  ğŸ“Š Mean Îº: {mean_kappa:.3f}\")\n    \n    return {'mean_kappa': mean_kappa, 'pairwise_kappas': pairwise_kappas}\n\n\ndef analyze_bloom_discrimination(results):\n    \"\"\"Test Bloom level discrimination.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"BLOOM'S LEVEL DISCRIMINATION\")\n    print(\"=\"*80)\n    \n    df = pd.DataFrame({\n        'jury_score': results['jury_scores'],\n        'bloom_level': results['bloom_levels']\n    })\n    \n    print(\"\\nScore Distribution by Bloom Level:\")\n    print(df.groupby('bloom_level')['jury_score'].describe())\n    \n    bloom_levels = df['bloom_level'].unique()\n    \n    if len(bloom_levels) == 2:\n        level1, level2 = bloom_levels\n        scores1 = df[df['bloom_level'] == level1]['jury_score']\n        scores2 = df[df['bloom_level'] == level2]['jury_score']\n        \n        statistic, p_value = mannwhitneyu(scores1, scores2, alternative='two-sided')\n        \n        print(f\"\\nğŸ“Š Mann-Whitney U Test: {level1} vs {level2}\")\n        print(f\"  p-value: {p_value:.4f}\")\n        print(f\"  {'Significant!' if p_value < 0.05 else 'Not significant'}\")\n        \n        return {'p_value': p_value, 'significant': p_value < 0.05}\n    \n    return None\n\n\ndef calculate_asap_correlation(results):\n    \"\"\"Calculate normalized correlation with ASAP.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ASAP CORRELATION\")\n    print(\"=\"*80)\n    \n    jury_norm = [(s - 1) / 4 for s in results['jury_scores']]\n    asap_norm = []\n    \n    for score, essay_set in zip(results['true_scores'], results['essay_sets']):\n        if essay_set and essay_set in ASAP_SCORE_RANGES:\n            min_s, max_s = ASAP_SCORE_RANGES[essay_set]\n            norm = (score - min_s) / (max_s - min_s)\n            asap_norm.append(max(0, min(1, norm)))\n        else:\n            asap_norm.append(score / max(results['true_scores']))\n    \n    rho, p = spearmanr(asap_norm, jury_norm)\n    \n    print(f\"\\nğŸ“Š Spearman Ï: {rho:.3f} (p={p:.4f})\")\n    print(f\"  {'Significant!' if p < 0.05 else 'Not significant'}\")\n    \n    return {'rho': rho, 'p_value': p}\n\n\ndef export_results(results, model_names, output_dir):\n    \"\"\"Export results to CSV.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPORTING RESULTS\")\n    print(\"=\"*80)\n    \n    # Main results\n    results_df = pd.DataFrame({\n        'true_score': results['true_scores'],\n        'jury_score': results['jury_scores'],\n        'bloom_level': results['bloom_levels'],\n        **{f'{name}_score': results['individual_scores'][name] for name in model_names}\n    })\n    \n    results_df.to_csv(f\"{output_dir}/jury_results_complete.csv\", index=False)\n    print(f\"  âœ“ Saved: jury_results_complete.csv\")\n\n\n# ============================================================================\n# MAIN\n# ============================================================================\n\ndef main():\n    \"\"\"Main execution.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"BloomLLM-Jury: Memory-Efficient Evaluation\")\n    print(\"=\"*80)\n    \n    # Load test data\n    print(\"\\n[1/5] Loading test data...\")\n    test_df = pd.read_pickle(CONFIG['test_data_path'])['test']\n    print(f\"  âœ“ Loaded {len(test_df)} essays\")\n    \n    # Evaluate jury (one model at a time)\n    print(\"\\n[2/5] Evaluating jury...\")\n    results = evaluate_jury_memory_efficient(test_df, CONFIG['models'], CONFIG)\n    \n    # Get model names\n    model_names = [m['name'] for m in CONFIG['models']]\n    \n    # Calculate metrics\n    print(\"\\n[3/5] Calculating metrics...\")\n    agreement = calculate_inter_jury_agreement(results, model_names)\n    bloom = analyze_bloom_discrimination(results)\n    asap = calculate_asap_correlation(results)\n    \n    # Export\n    print(\"\\n[4/5] Exporting results...\")\n    export_results(results, model_names, CONFIG['output_dir'])\n    \n    # Summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"SUMMARY\")\n    print(\"=\"*80)\n    print(f\"  Inter-jury Îº: {agreement['mean_kappa']:.3f}\")\n    if bloom:\n        print(f\"  Bloom p-value: {bloom['p_value']:.4f}\")\n    print(f\"  ASAP Ï: {asap['rho']:.3f}\")\n    print(\"\\nâœ“ Evaluation complete!\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T21:07:58.687663Z","iopub.execute_input":"2025-11-04T21:07:58.688768Z","iopub.status.idle":"2025-11-05T02:09:06.633875Z","shell.execute_reply.started":"2025-11-04T21:07:58.688739Z","shell.execute_reply":"2025-11-05T02:09:06.632981Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251104_210758-5hjsgkp8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury/runs/5hjsgkp8' target=\"_blank\">jury-evaluation-run</a></strong> to <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury/runs/5hjsgkp8' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury/runs/5hjsgkp8</a>"},"metadata":{}},{"name":"stdout","text":"================================================================================\nBloomLLM-Jury with WandB Logging\n================================================================================\n\nLoading data...\nâœ“ Loaded 50 essays\n\n================================================================================\nEVALUATING: Phi\n================================================================================\n\nLoading microsoft/Phi-3-mini-4k-instruct...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed685cbdd6054c649a1c1a420576714e"}},"metadata":{}},{"name":"stdout","text":"âœ“ Loaded\n","output_type":"stream"},{"name":"stderr","text":"Phi: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [1:16:14<00:00, 91.48s/it] \n","output_type":"stream"},{"name":"stdout","text":"\nPhi Results:\n  Extraction: 16.0%\n  Mean score: 1.18 Â± 0.62\n  Distribution: Counter({1: 45, 2: 3, 4: 2})\n\n================================================================================\nEVALUATING: Mistral\n================================================================================\n\nLoading mistralai/Mistral-7B-Instruct-v0.3...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe91530fa93946328dce3c6280dc7847"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a3c7413c22f498bb060931289f58877"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"823fdcba79014692b244cb90f60c5393"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6af5d70c845c4beda10900a4c32860fc"}},"metadata":{}},{"name":"stdout","text":"âœ“ Loaded\n","output_type":"stream"},{"name":"stderr","text":"Mistral: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:58<00:00,  1.17s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nMistral Results:\n  Extraction: 0.0%\n  Mean score: 1.00 Â± 0.00\n  Distribution: Counter({1: 50})\n\n================================================================================\nEVALUATING: Qwen\n================================================================================\n\nLoading Qwen/Qwen2.5-7B-Instruct...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a502f4d21dc64db9abea275ec5c0a3ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65d37d7c36684f5c9351e7307f1cff7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5336999d8f23439289af9bffd1533c51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"845005935c8745e8a532713eb7545269"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b42ae829c52e4b229b882bc7a788625c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c170a0633fc4cf1b962089ee8ab3c81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03c2d2fdad5e420c8abd3d02b8406d88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24f184a8ddc9475a979b8c6db5b656be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30d7b92946474ef183d64f2c29d7d0ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75cda0661d3e456d8cf9c1fbd5484464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e55874afe78e4d98942fc0156b874098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0a1ae4715584388b791a845422492a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"856f833971e74bfea2dcd94d9e8593be"}},"metadata":{}},{"name":"stdout","text":"âœ“ Loaded\n","output_type":"stream"},{"name":"stderr","text":"Qwen:   0%|          | 0/50 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nQwen: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [3:38:54<00:00, 262.68s/it]  \n","output_type":"stream"},{"name":"stdout","text":"\nQwen Results:\n  Extraction: 8.0%\n  Mean score: 1.24 Â± 0.81\n  Distribution: Counter({1: 46, 4: 4})\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nAGGREGATING\n================================================================================\n\nMETRICS:\n  Phi vs Mistral: Îº = 0.000\n  Phi vs Qwen: Îº = -0.081\n  Mistral vs Qwen: Îº = 0.000\n\n  Mean Îº: -0.027\n  Bloom discrimination: p = 1.0000\n  ASAP correlation: Ï = -0.165\n\nEXPORTING...\n\n================================================================================\nâœ… COMPLETE!\n================================================================================\n  Inter-jury Îº: -0.027\n  Bloom p-value: 1.0000\n  ASAP Ï: -0.165\n\nğŸ“Š View results: https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury/runs/5hjsgkp8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Mistral_extraction_success</td><td>â–</td></tr><tr><td>Mistral_mean_score</td><td>â–</td></tr><tr><td>Mistral_score</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>Mistral_std_score</td><td>â–</td></tr><tr><td>Phi_extraction_success</td><td>â–</td></tr><tr><td>Phi_mean_score</td><td>â–</td></tr><tr><td>Phi_score</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–ˆâ–â–â–â–â–â–ƒâ–â–â–â–ƒâ–â–â–â–â–â–â–ƒâ–â–</td></tr><tr><td>Phi_std_score</td><td>â–</td></tr><tr><td>Qwen_extraction_success</td><td>â–</td></tr><tr><td>Qwen_mean_score</td><td>â–</td></tr><tr><td>Qwen_score</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>Qwen_std_score</td><td>â–</td></tr><tr><td>asap_rho</td><td>â–</td></tr><tr><td>bloom_p_value</td><td>â–</td></tr><tr><td>essay_idx</td><td>â–†â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–†â–…â–†â–‡â–†â–ƒâ–‚â–…â–ƒâ–…â–†â–ˆâ–…â–â–ƒâ–†â–‡â–†â–‡â–â–ƒâ–…â–â–…â–â–ƒâ–ˆâ–ƒâ–ƒâ–…â–†â–„</td></tr><tr><td>kappa_Mistral_Qwen</td><td>â–</td></tr><tr><td>kappa_Phi_Mistral</td><td>â–</td></tr><tr><td>kappa_Phi_Qwen</td><td>â–</td></tr><tr><td>mean_kappa</td><td>â–</td></tr><tr><td>n_essays</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Mistral_extraction_success</td><td>0</td></tr><tr><td>Mistral_mean_score</td><td>1</td></tr><tr><td>Mistral_method</td><td>Weak</td></tr><tr><td>Mistral_score</td><td>1</td></tr><tr><td>Mistral_std_score</td><td>0</td></tr><tr><td>Phi_extraction_success</td><td>16</td></tr><tr><td>Phi_mean_score</td><td>1.18</td></tr><tr><td>Phi_method</td><td>Pattern</td></tr><tr><td>Phi_score</td><td>1</td></tr><tr><td>Phi_std_score</td><td>0.62258</td></tr><tr><td>Qwen_extraction_success</td><td>8</td></tr><tr><td>Qwen_mean_score</td><td>1.24</td></tr><tr><td>Qwen_method</td><td>Weak</td></tr><tr><td>Qwen_score</td><td>1</td></tr><tr><td>Qwen_std_score</td><td>0.81388</td></tr><tr><td>asap_rho</td><td>-0.1654</td></tr><tr><td>bloom_p_value</td><td>1</td></tr><tr><td>bloom_significant</td><td>False</td></tr><tr><td>essay_idx</td><td>50</td></tr><tr><td>kappa_Mistral_Qwen</td><td>0</td></tr><tr><td>kappa_Phi_Mistral</td><td>0</td></tr><tr><td>kappa_Phi_Qwen</td><td>-0.08069</td></tr><tr><td>mean_kappa</td><td>-0.0269</td></tr><tr><td>n_essays</td><td>50</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">jury-evaluation-run</strong> at: <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury/runs/5hjsgkp8' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury/runs/5hjsgkp8</a><br> View project at: <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251104_210758-5hjsgkp8/logs</code>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Corrected Ealuation","metadata":{}},{"cell_type":"code","source":"\"\"\"\nBloomLLM-Jury: MEMORY-EFFICIENT Evaluation Script WITH WANDB\n=============================================================\nYOUR WORKING CODE - ONLY wandb.log() calls added, nothing else changed\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport json\nimport torch\nimport gc\nfrom tqdm import tqdm\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom scipy.stats import spearmanr, mannwhitneyu, pearsonr\nfrom sklearn.metrics import cohen_kappa_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict, Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport wandb  # ONLY NEW LINE\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nCONFIG = {\n    'models': [\n        # {\n        #     'name': 'Phi',\n        #     'base_id': 'microsoft/Phi-3-mini-4k-instruct',\n        #     'lora_path': '/kaggle/input/capstone-phase-1/models/phi3_lora'  # YOUR WORKING MODELS\n        # },\n        {\n            'name': 'Mistral',\n            'base_id': 'mistralai/Mistral-7B-Instruct-v0.3',\n            'lora_path': '/kaggle/input/capstone-phase-1/models/mistral_lora'\n        },\n        {\n            'name': 'Qwen',\n            'base_id': 'Qwen/Qwen2.5-7B-Instruct',\n            'lora_path': '/kaggle/input/capstone-phase-1/models/qwen_lora'\n        }\n    ],\n    'test_data_path': '/kaggle/input/capstone-phase-1/checkpoint_splits_SMALL.pkl',\n    'output_dir': '/kaggle/working',\n    'generation_params': {\n        'max_new_tokens': 250,\n        'do_sample': False,\n    },\n}\n\nASAP_SCORE_RANGES = {\n    1: (1, 6), 2: (1, 6), 3: (0, 3), 4: (0, 3),\n    5: (0, 4), 6: (0, 4), 7: (0, 3), 8: (1, 6),\n}\n\n# ============================================================================\n# MODEL LOADING (ONE AT A TIME) - UNCHANGED\n# ============================================================================\n\ndef load_single_model(base_id, lora_path):\n    \"\"\"Load a single model with proper memory management.\"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"Loading {base_id}...\")\n    print(f\"{'='*80}\")\n    \n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n    )\n    \n    base_model = AutoModelForCausalLM.from_pretrained(\n        base_id,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n    )\n    \n    print(f\"  âœ“ Base model loaded\")\n    \n    model = PeftModel.from_pretrained(base_model, lora_path)\n    print(f\"  âœ“ LoRA adapter applied from {lora_path}\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(base_id, trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    print(f\"  âœ“ Tokenizer loaded\")\n    \n    if torch.cuda.is_available():\n        mem_allocated = torch.cuda.memory_allocated(0) / 1e9\n        mem_reserved = torch.cuda.memory_reserved(0) / 1e9\n        print(f\"  ğŸ“Š GPU Memory: {mem_allocated:.2f}GB allocated, {mem_reserved:.2f}GB reserved\")\n        \n        # WANDB LOG\n        wandb.log({\n            f\"{base_id.split('/')[-1]}_gpu_allocated\": mem_allocated,\n            f\"{base_id.split('/')[-1]}_gpu_reserved\": mem_reserved\n        })\n    \n    return model, tokenizer\n\n\ndef unload_model(model, tokenizer):\n    \"\"\"Properly unload model and free memory.\"\"\"\n    del model\n    del tokenizer\n    torch.cuda.empty_cache()\n    gc.collect()\n    print(f\"  âœ“ Model unloaded, memory cleared\")\n\n\n# ============================================================================\n# SCORE EXTRACTION - UNCHANGED\n# ============================================================================\n\ndef extract_score_robust(text):\n    \"\"\"Extract score and rationale from model output.\"\"\"\n    original_text = text\n    \n    assistant_markers = ['<|assistant|>', '[/INST]', 'assistant:', 'Assistant:']\n    for marker in assistant_markers:\n        if marker in text:\n            text = text.split(marker)[-1]\n    \n    text = text.strip()\n    \n    # Method 1: JSON\n    json_pattern = r'\\{[^}]*\"score\"\\s*:\\s*\\d+[^}]*\\}'\n    json_matches = re.findall(json_pattern, text, re.DOTALL | re.IGNORECASE)\n    \n    for match in json_matches:\n        try:\n            data = json.loads(match)\n            score = int(data.get('score', 0))\n            \n            if 1 <= score <= 5:\n                rationale = data.get('rationale', data.get('reason', data.get('explanation', '')))\n                return score, rationale, \"JSON\"\n        except:\n            continue\n    \n    # Method 2: Patterns\n    score_patterns = [\n        r'\"score\"\\s*:\\s*(\\d+)',\n        r'score\\s*:\\s*(\\d+)',\n        r'Score\\s*:\\s*(\\d+)',\n        r'\\*\\*Score\\*\\*\\s*:\\s*(\\d+)',\n    ]\n    \n    for pattern in score_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            score = int(match.group(1))\n            if 1 <= score <= 5:\n                return score, \"\", \"Pattern\"\n    \n    # Method 3: Ratio\n    ratio_patterns = [r'\\b([1-5])\\s*/\\s*5\\b', r'\\b([1-5])\\s+out\\s+of\\s+5\\b']\n    for pattern in ratio_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            return int(match.group(1)), \"\", \"Ratio\"\n    \n    # Method 4: Any digit 1-5\n    digit_matches = re.findall(r'\\b([1-5])\\b', text)\n    if digit_matches:\n        return int(digit_matches[0]), \"\", \"Weak\"\n    \n    return 3, \"Extraction failed\", \"Failed\"\n\n\n# ============================================================================\n# INFERENCE - UNCHANGED\n# ============================================================================\n\ndef construct_prompt(row):\n    \"\"\"Construct evaluation prompt.\"\"\"\n    system_msg = f\"You are an expert educational evaluator specializing in Bloom's Taxonomy. You evaluate student responses for {row['bloom_level']}-level cognitive skills.\"\n    \n    user_msg = f\"\"\"Evaluate this student response for {row['bloom_level']}-level thinking according to Bloom's Taxonomy.\n\n**Prompt**: {row['prompt']}\n\n**Student Response**: {row['response']}\n\nProvide your evaluation in this EXACT JSON format:\n{{\"score\": <integer 1-5>, \"rationale\": \"<brief explanation>\"}}\n\n**Scoring Guide**:\n- 5: Excellent - Fully demonstrates {row['bloom_level']}-level thinking\n- 4: Good - Strong demonstration with minor gaps\n- 3: Adequate - Meets basic requirements\n- 2: Weak - Minimal {row['bloom_level']}-level thinking\n- 1: Poor - Does not demonstrate {row['bloom_level']}-level thinking\n\nRespond ONLY with the JSON object.\"\"\"\n\n    return [\n        {\"role\": \"system\", \"content\": system_msg},\n        {\"role\": \"user\", \"content\": user_msg}\n    ]\n\n\ndef evaluate_with_model(model, tokenizer, row, config):\n    \"\"\"Evaluate a single essay with a model.\"\"\"\n    messages = construct_prompt(row)\n    \n    try:\n        inputs = tokenizer.apply_chat_template(\n            messages,\n            return_tensors=\"pt\",\n            add_generation_prompt=True,\n            padding=True,\n            truncation=True\n        ).to(model.device)\n    except Exception as e:\n        return {\n            'score': 3,\n            'rationale': f\"Tokenization error: {e}\",\n            'extraction_method': 'Error',\n            'raw_output': ''\n        }\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs,\n            max_new_tokens=config['generation_params']['max_new_tokens'],\n            do_sample=config['generation_params']['do_sample'],\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n            use_cache=False,\n        )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    score, rationale, method = extract_score_robust(response)\n    \n    return {\n        'score': score,\n        'rationale': rationale,\n        'extraction_method': method,\n        'raw_output': response\n    }\n\n\n# ============================================================================\n# MAIN EVALUATION (ONE MODEL AT A TIME) - WANDB ADDED\n# ============================================================================\n\ndef evaluate_jury_memory_efficient(test_df, model_configs, config):\n    \"\"\"Evaluate jury by loading ONE model at a time.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"MEMORY-EFFICIENT JURY EVALUATION\")\n    print(\"=\"*80)\n    print(\"\\nStrategy: Load â†’ Evaluate â†’ Unload â†’ Repeat for each model\")\n    print(\"This is slower but works with limited GPU memory.\\n\")\n    \n    all_model_results = {}\n    all_rationales = {}\n    all_extraction_methods = {}\n    \n    for model_config in model_configs:\n        model_name = model_config['name']\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"EVALUATING WITH {model_name}\")\n        print(f\"{'='*80}\")\n        \n        model, tokenizer = load_single_model(\n            model_config['base_id'],\n            model_config['lora_path']\n        )\n        \n        model_scores = []\n        model_rationales = []\n        model_methods = []\n        \n        print(f\"\\nEvaluating {len(test_df)} essays...\")\n        \n        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=f\"{model_name}\"):\n            result = evaluate_with_model(model, tokenizer, row, config)\n            \n            model_scores.append(result['score'])\n            model_rationales.append(result['rationale'])\n            model_methods.append(result['extraction_method'])\n        \n        all_model_results[model_name] = model_scores\n        all_rationales[model_name] = model_rationales\n        all_extraction_methods[model_name] = model_methods\n        \n        extraction_success = sum(1 for m in model_methods if m in ['JSON', 'Pattern']) / len(model_methods) * 100\n        mean_score = np.mean(model_scores)\n        std_score = np.std(model_scores)\n        \n        print(f\"\\n  ğŸ“Š {model_name} Summary:\")\n        print(f\"     Extraction success: {extraction_success:.1f}%\")\n        print(f\"     Mean score: {mean_score:.2f} Â± {std_score:.2f}\")\n        print(f\"     Score distribution: {Counter(model_scores)}\")\n\n        \n        import json\n        with open(f'/kaggle/working/{model_name}_outputs.json', 'w') as f:\n            json.dump([\n                {'essay_idx': i, 'score': model_scores[i], 'method': model_methods[i], 'rationale': model_rationales[i]} \n                for i in range(len(model_scores))\n            ], f, indent=2)\n        print(f\"  âœ“ Saved raw outputs to {model_name}_outputs.json\")\n        \n        \n        # WANDB LOG\n        wandb.log({\n            f\"{model_name}_extraction_success\": extraction_success,\n            f\"{model_name}_mean_score\": mean_score,\n            f\"{model_name}_std_score\": std_score,\n        })\n        wandb.log({f\"{model_name}_score_dist\": wandb.Histogram(model_scores)})\n        \n        unload_model(model, tokenizer)\n    \n    print(f\"\\n{'='*80}\")\n    print(\"AGGREGATING JURY DECISIONS\")\n    print(f\"{'='*80}\")\n    \n    jury_scores = []\n    for i in range(len(test_df)):\n        essay_scores = [all_model_results[name][i] for name in all_model_results.keys()]\n        jury_scores.append(round(np.mean(essay_scores)))\n    \n    results = {\n        'individual_scores': all_model_results,\n        'jury_scores': jury_scores,\n        'rationales': [],\n        'extraction_methods': all_extraction_methods,\n        'true_scores': test_df['essay_score'].tolist(),\n        'bloom_levels': test_df['bloom_level'].tolist(),\n        'essay_sets': test_df['essay_set'].tolist() if 'essay_set' in test_df.columns else [None] * len(test_df),\n    }\n    \n    for i in range(len(test_df)):\n        essay_rationales = {name: all_rationales[name][i] for name in all_rationales.keys()}\n        results['rationales'].append(essay_rationales)\n    \n    return results\n\n\n# ============================================================================\n# ANALYSIS FUNCTIONS - WANDB ADDED\n# ============================================================================\n\ndef calculate_inter_jury_agreement(results, model_names):\n    \"\"\"Calculate Cohen's kappa between models.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"INTER-JURY AGREEMENT\")\n    print(\"=\"*80)\n    \n    pairwise_kappas = {}\n    for i, name1 in enumerate(model_names):\n        for j, name2 in enumerate(model_names):\n            if i < j:\n                kappa = cohen_kappa_score(\n                    results['individual_scores'][name1],\n                    results['individual_scores'][name2],\n                    weights='linear'\n                )\n                pairwise_kappas[f\"{name1}-{name2}\"] = kappa\n                print(f\"  {name1:10s} vs {name2:10s}: Îº = {kappa:.3f}\")\n                \n                # WANDB LOG\n                wandb.log({f\"kappa_{name1}_vs_{name2}\": kappa})\n    \n    mean_kappa = np.mean(list(pairwise_kappas.values()))\n    print(f\"\\n  ğŸ“Š Mean Îº: {mean_kappa:.3f}\")\n    \n    # WANDB LOG\n    wandb.log({\"mean_kappa\": mean_kappa})\n    wandb.summary[\"mean_kappa\"] = mean_kappa\n    \n    return {'mean_kappa': mean_kappa, 'pairwise_kappas': pairwise_kappas}\n\n\ndef analyze_bloom_discrimination(results):\n    \"\"\"Test Bloom level discrimination.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"BLOOM'S LEVEL DISCRIMINATION\")\n    print(\"=\"*80)\n    \n    df = pd.DataFrame({\n        'jury_score': results['jury_scores'],\n        'bloom_level': results['bloom_levels']\n    })\n    \n    print(\"\\nScore Distribution by Bloom Level:\")\n    print(df.groupby('bloom_level')['jury_score'].describe())\n    \n    bloom_levels = df['bloom_level'].unique()\n    \n    if len(bloom_levels) == 2:\n        level1, level2 = bloom_levels\n        scores1 = df[df['bloom_level'] == level1]['jury_score']\n        scores2 = df[df['bloom_level'] == level2]['jury_score']\n        \n        statistic, p_value = mannwhitneyu(scores1, scores2, alternative='two-sided')\n        \n        print(f\"\\nğŸ“Š Mann-Whitney U Test: {level1} vs {level2}\")\n        print(f\"  p-value: {p_value:.4f}\")\n        print(f\"  {'Significant!' if p_value < 0.05 else 'Not significant'}\")\n        \n        # WANDB LOG\n        wandb.log({\"bloom_p_value\": p_value, \"bloom_significant\": p_value < 0.05})\n        wandb.summary[\"bloom_p_value\"] = p_value\n        \n        return {'p_value': p_value, 'significant': p_value < 0.05}\n    \n    return None\n\n\ndef calculate_asap_correlation(results):\n    \"\"\"Calculate normalized correlation with ASAP.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ASAP CORRELATION\")\n    print(\"=\"*80)\n    \n    jury_norm = [(s - 1) / 4 for s in results['jury_scores']]\n    asap_norm = []\n    \n    for score, essay_set in zip(results['true_scores'], results['essay_sets']):\n        if essay_set and essay_set in ASAP_SCORE_RANGES:\n            min_s, max_s = ASAP_SCORE_RANGES[essay_set]\n            norm = (score - min_s) / (max_s - min_s)\n            asap_norm.append(max(0, min(1, norm)))\n        else:\n            asap_norm.append(score / max(results['true_scores']))\n    \n    rho, p = spearmanr(asap_norm, jury_norm)\n    \n    print(f\"\\nğŸ“Š Spearman Ï: {rho:.3f} (p={p:.4f})\")\n    print(f\"  {'Significant!' if p < 0.05 else 'Not significant'}\")\n    \n    # WANDB LOG\n    wandb.log({\"asap_rho\": rho, \"asap_p_value\": p})\n    wandb.summary[\"asap_rho\"] = rho\n    \n    return {'rho': rho, 'p_value': p}\n\n\ndef export_results(results, model_names, output_dir):\n    \"\"\"Export results to CSV.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPORTING RESULTS\")\n    print(\"=\"*80)\n    \n    results_df = pd.DataFrame({\n        'true_score': results['true_scores'],\n        'jury_score': results['jury_scores'],\n        'bloom_level': results['bloom_levels'],\n        **{f'{name}_score': results['individual_scores'][name] for name in model_names}\n    })\n    \n    csv_path = f\"{output_dir}/jury_results_complete.csv\"\n    results_df.to_csv(csv_path, index=False)\n    print(f\"  âœ“ Saved: jury_results_complete.csv\")\n    \n    # WANDB LOG\n    wandb.save(csv_path)\n    wandb.log({\"results_table\": wandb.Table(dataframe=results_df)})\n\n\n# ============================================================================\n# MAIN - WANDB INIT ADDED\n# ============================================================================\n\ndef main():\n    \"\"\"Main execution.\"\"\"\n    \n    # WANDB INIT\n    wandb.init(\n        project=\"bloomllm-jury\",\n        name=\"jury-evaluation\",\n        id=\"uzfietg2\",\n        resume=\"allow\",\n        config=CONFIG\n    )\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"BloomLLM-Jury: Memory-Efficient Evaluation\")\n    print(\"=\"*80)\n    \n    print(\"\\n[1/5] Loading test data...\")\n    test_df = pd.read_pickle(CONFIG['test_data_path'])['test']\n    print(f\"  âœ“ Loaded {len(test_df)} essays\")\n    \n    wandb.log({\"n_essays\": len(test_df)})  # WANDB LOG\n    \n    print(\"\\n[2/5] Evaluating jury...\")\n    results = evaluate_jury_memory_efficient(test_df, CONFIG['models'], CONFIG)\n    \n    model_names = [m['name'] for m in CONFIG['models']]\n    \n    print(\"\\n[3/5] Calculating metrics...\")\n    agreement = calculate_inter_jury_agreement(results, model_names)\n    bloom = analyze_bloom_discrimination(results)\n    asap = calculate_asap_correlation(results)\n    \n    print(\"\\n[4/5] Exporting results...\")\n    export_results(results, model_names, CONFIG['output_dir'])\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"SUMMARY\")\n    print(\"=\"*80)\n    print(f\"  Inter-jury Îº: {agreement['mean_kappa']:.3f}\")\n    if bloom:\n        print(f\"  Bloom p-value: {bloom['p_value']:.4f}\")\n    print(f\"  ASAP Ï: {asap['rho']:.3f}\")\n    print(f\"\\nğŸ“Š View results: {wandb.run.url}\")  # WANDB LOG\n    print(\"\\nâœ“ Evaluation complete!\")\n    \n    wandb.finish()  # WANDB FINISH\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T08:31:26.549586Z","iopub.execute_input":"2025-11-05T08:31:26.550211Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing previous runs because reinit is set to 'default'."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Mistral-7B-Instruct-v0.3_gpu_allocated</td><td>â–</td></tr><tr><td>Mistral-7B-Instruct-v0.3_gpu_reserved</td><td>â–</td></tr><tr><td>n_essays</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Mistral-7B-Instruct-v0.3_gpu_allocated</td><td>8.38205</td></tr><tr><td>Mistral-7B-Instruct-v0.3_gpu_reserved</td><td>9.11003</td></tr><tr><td>Phi-3-mini-4k-instruct_gpu_allocated</td><td>2.27042</td></tr><tr><td>Phi-3-mini-4k-instruct_gpu_reserved</td><td>3.79375</td></tr><tr><td>Phi_extraction_success</td><td>100</td></tr><tr><td>Phi_mean_score</td><td>3.38</td></tr><tr><td>Phi_std_score</td><td>0.95687</td></tr><tr><td>n_essays</td><td>50</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">jury-evaluation</strong> at: <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury/runs/uzfietg2' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury/runs/uzfietg2</a><br> View project at: <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251105_082156-uzfietg2/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251105_083126-uzfietg2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Resuming run <strong><a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury/runs/uzfietg2' target=\"_blank\">jury-evaluation</a></strong> to <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury/runs/uzfietg2' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury/runs/uzfietg2</a>"},"metadata":{}},{"name":"stdout","text":"\n================================================================================\nBloomLLM-Jury: Memory-Efficient Evaluation\n================================================================================\n\n[1/5] Loading test data...\n  âœ“ Loaded 50 essays\n\n[2/5] Evaluating jury...\n\n================================================================================\nMEMORY-EFFICIENT JURY EVALUATION\n================================================================================\n\nStrategy: Load â†’ Evaluate â†’ Unload â†’ Repeat for each model\nThis is slower but works with limited GPU memory.\n\n\n================================================================================\nEVALUATING WITH Mistral\n================================================================================\n\n================================================================================\nLoading mistralai/Mistral-7B-Instruct-v0.3...\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"609abb27a2494ec3b24178934b5390fe"}},"metadata":{}},{"name":"stdout","text":"  âœ“ Base model loaded\n  âœ“ LoRA adapter applied from /kaggle/input/capstone-phase-1/models/mistral_lora\n  âœ“ Tokenizer loaded\n  ğŸ“Š GPU Memory: 8.39GB allocated, 9.11GB reserved\n\nEvaluating 50 essays...\n","output_type":"stream"},{"name":"stderr","text":"Mistral:   0%|          | 0/50 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nMistral: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [2:02:32<00:00, 147.05s/it]  \n","output_type":"stream"},{"name":"stdout","text":"\n  ğŸ“Š Mistral Summary:\n     Extraction success: 42.0%\n     Mean score: 2.66 Â± 1.96\n     Score distribution: Counter({1: 29, 5: 20, 4: 1})\n  âœ“ Saved raw outputs to Mistral_outputs.json\n  âœ“ Model unloaded, memory cleared\n\n================================================================================\nEVALUATING WITH Qwen\n================================================================================\n\n================================================================================\nLoading Qwen/Qwen2.5-7B-Instruct...\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"833a707acfc248508168d9b098c0cf3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30da3641bc164a10adcfb206d524c107"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d72ca4b0c964419a5cce2acf5e37d8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5487a627b0d14bb1abf6d59b20b2142f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c2f9755769b4393b7ab15394cc9d81a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5c8723ee216449d89a680518b065c6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86bd3135568043c6a5881266ebbfff14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cec059bb91dd4a5682af0af5368d81b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e659d5e0c8df4862905f06a2e411139b"}},"metadata":{}},{"name":"stdout","text":"  âœ“ Base model loaded\n  âœ“ LoRA adapter applied from /kaggle/input/capstone-phase-1/models/qwen_lora\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"051e2d6d7c424c0aa43d870da0de7f42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2e5ff8ea19f49c2bea5ab5d0dfd0e58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c83eb4ec375b4ce281e7a0e67b052499"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fba37d135a240cd90eca2d37809f607"}},"metadata":{}},{"name":"stdout","text":"  âœ“ Tokenizer loaded\n  ğŸ“Š GPU Memory: 13.98GB allocated, 14.35GB reserved\n\nEvaluating 50 essays...\n","output_type":"stream"},{"name":"stderr","text":"Qwen:   0%|          | 0/50 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nQwen:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [3:10:18<2:06:36, 379.82s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Quick check of what wandb logged\nimport wandb\napi = wandb.Api()\nrun = api.run(\"mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/bloomllm-jury/uzfietg2\")\n\n# Get all metrics\nhistory = run.history()\nprint(history[['Phi_extraction_success', 'Phi_mean_score', 'Phi_std_score']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T08:18:36.478764Z","iopub.execute_input":"2025-11-05T08:18:36.479399Z","iopub.status.idle":"2025-11-05T08:18:37.329808Z","shell.execute_reply.started":"2025-11-05T08:18:36.479371Z","shell.execute_reply":"2025-11-05T08:18:37.329058Z"}},"outputs":[{"name":"stdout","text":"   Phi_extraction_success  Phi_mean_score  Phi_std_score\n0                     NaN             NaN            NaN\n1                     NaN             NaN            NaN\n2                   100.0            3.38        0.95687\n3                     NaN             NaN            NaN\n4                     NaN             NaN            NaN\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install transformers==4.38.0 --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:47:15.167852Z","iopub.execute_input":"2025-11-04T19:47:15.168232Z","iopub.status.idle":"2025-11-04T19:47:27.503171Z","shell.execute_reply.started":"2025-11-04T19:47:15.168204Z","shell.execute_reply":"2025-11-04T19:47:27.502159Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.38.0\n  Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.38.0)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0) (2.32.5)\nCollecting tokenizers<0.19,>=0.14 (from transformers==4.38.0)\n  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.38.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.38.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.38.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.38.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.38.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.38.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.0) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.0) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.38.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.38.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.38.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.38.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.38.0) (2024.2.0)\nDownloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 tokenizers-0.15.2 transformers-4.38.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Restart runtime first, then run:\n!pip uninstall numpy pandas scipy scikit-learn matplotlib seaborn -y\n!pip uninstall transformers bitsandbytes accelerate peft -y\n\n# Install compatible versions together\n!pip install numpy==1.24.3\n!pip install pandas scipy scikit-learn matplotlib seaborn\n!pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n!pip install transformers==4.36.2 bitsandbytes==0.41.3 accelerate==0.25.0 peft==0.7.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:32:32.622202Z","iopub.execute_input":"2025-11-04T19:32:32.622957Z","iopub.status.idle":"2025-11-04T19:35:36.638975Z","shell.execute_reply.started":"2025-11-04T19:32:32.622930Z","shell.execute_reply":"2025-11-04T19:35:36.638025Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.24.3\nUninstalling numpy-1.24.3:\n  Successfully uninstalled numpy-1.24.3\nFound existing installation: pandas 2.2.3\nUninstalling pandas-2.2.3:\n  Successfully uninstalled pandas-2.2.3\nFound existing installation: scipy 1.15.3\nUninstalling scipy-1.15.3:\n  Successfully uninstalled scipy-1.15.3\nFound existing installation: scikit-learn 1.2.2\nUninstalling scikit-learn-1.2.2:\n  Successfully uninstalled scikit-learn-1.2.2\nFound existing installation: matplotlib 3.7.2\nUninstalling matplotlib-3.7.2:\n  Successfully uninstalled matplotlib-3.7.2\nFound existing installation: seaborn 0.12.2\nUninstalling seaborn-0.12.2:\n  Successfully uninstalled seaborn-0.12.2\nFound existing installation: transformers 4.36.0\nUninstalling transformers-4.36.0:\n  Successfully uninstalled transformers-4.36.0\nFound existing installation: bitsandbytes 0.41.0\nUninstalling bitsandbytes-0.41.0:\n  Successfully uninstalled bitsandbytes-0.41.0\nFound existing installation: accelerate 0.25.0\nUninstalling accelerate-0.25.0:\n  Successfully uninstalled accelerate-0.25.0\nFound existing installation: peft 0.11.0\nUninstalling peft-0.11.0:\n  Successfully uninstalled peft-0.11.0\nCollecting numpy==1.24.3\n  Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nUsing cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\nInstalling collected packages: numpy\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, which is not installed.\nlibpysal 4.9.2 requires pandas>=1.4, which is not installed.\nlibpysal 4.9.2 requires scipy>=1.8, which is not installed.\ntreelite 4.4.1 requires scipy, which is not installed.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, which is not installed.\ndask-cuda 25.2.0 requires pandas>=1.3, which is not installed.\ncuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, which is not installed.\ndatasets 4.1.1 requires pandas, which is not installed.\nwoodwork 0.31.0 requires pandas>=2.0.0, which is not installed.\nwoodwork 0.31.0 requires scikit-learn>=1.1.0, which is not installed.\nwoodwork 0.31.0 requires scipy>=1.10.0, which is not installed.\nboruta 0.4.3 requires scikit-learn>=0.17.1, which is not installed.\nboruta 0.4.3 requires scipy>=0.17.0, which is not installed.\nscikit-surprise 1.1.4 requires scipy>=1.6.0, which is not installed.\nfeaturetools 1.31.0 requires pandas>=2.0.0, which is not installed.\nfeaturetools 1.31.0 requires scipy>=1.10.0, which is not installed.\nplotly-express 0.4.1 requires pandas>=0.20.0, which is not installed.\nplotly-express 0.4.1 requires scipy>=0.18, which is not installed.\neasyocr 1.7.2 requires scipy, which is not installed.\nimagehash 4.3.1 requires scipy, which is not installed.\nxgboost 2.0.3 requires scipy, which is not installed.\npandasql 0.7.3 requires pandas, which is not installed.\nydata-profiling 4.17.0 requires matplotlib<=3.10,>=3.5, which is not installed.\nydata-profiling 4.17.0 requires pandas!=1.4.0,<3.0,>1.1, which is not installed.\nydata-profiling 4.17.0 requires scipy<1.16,>=1.4.1, which is not installed.\nydata-profiling 4.17.0 requires seaborn<0.14,>=0.10.1, which is not installed.\narviz 0.21.0 requires matplotlib>=3.5, which is not installed.\narviz 0.21.0 requires pandas>=1.5.0, which is not installed.\narviz 0.21.0 requires scipy>=1.9.0, which is not installed.\ncatboost 1.2.8 requires matplotlib, which is not installed.\ncatboost 1.2.8 requires pandas>=0.24, which is not installed.\ncatboost 1.2.8 requires scipy, which is not installed.\nfury 0.12.0 requires scipy>=1.0, which is not installed.\ntpot 0.12.1 requires pandas>=0.24.2, which is not installed.\ntpot 0.12.1 requires scikit-learn>=0.22.0, which is not installed.\ntpot 0.12.1 requires scipy>=1.3.1, which is not installed.\nshap 0.44.1 requires pandas, which is not installed.\nshap 0.44.1 requires scikit-learn, which is not installed.\nshap 0.44.1 requires scipy, which is not installed.\nscikit-learn-intelex 2025.8.0 requires scikit-learn>=0.22, which is not installed.\ncartopy 0.24.1 requires matplotlib>=3.6, which is not installed.\nmlcrate 0.2.0 requires pandas, which is not installed.\nbayesian-optimization 3.1.0 requires scikit-learn>=1.0.0, which is not installed.\nbayesian-optimization 3.1.0 requires scipy>=1.0.0; python_full_version < \"3.13\", which is not installed.\npyupset 0.1.1.post7 requires matplotlib, which is not installed.\npyupset 0.1.1.post7 requires pandas, which is not installed.\nvisions 0.8.1 requires pandas>=2.0.0, which is not installed.\nopen-spiel 1.6.1 requires scipy>=1.10.1, which is not installed.\npymc3 3.11.4 requires pandas>=0.24.0, which is not installed.\npymc3 3.11.4 requires scipy>=1.2.0, which is not installed.\ndipy 1.11.0 requires scipy>=1.8, which is not installed.\npyldavis 3.4.1 requires pandas>=2.0.0, which is not installed.\npyldavis 3.4.1 requires scikit-learn>=1.0.0, which is not installed.\npyldavis 3.4.1 requires scipy, which is not installed.\nmne 1.10.1 requires matplotlib>=3.7, which is not installed.\nmne 1.10.1 requires scipy>=1.11, which is not installed.\ngeopandas 0.14.4 requires pandas>=1.4.0, which is not installed.\ntheano 1.0.5 requires scipy>=0.14, which is not installed.\nnilearn 0.10.4 requires pandas>=1.1.5, which is not installed.\nnilearn 0.10.4 requires scikit-learn>=1.0.0, which is not installed.\nnilearn 0.10.4 requires scipy>=1.8.0, which is not installed.\neli5 0.13.0 requires scikit-learn>=0.20, which is not installed.\neli5 0.13.0 requires scipy, which is not installed.\nipympl 0.9.7 requires matplotlib<4,>=3.5.0, which is not installed.\nstable-baselines3 2.1.0 requires matplotlib, which is not installed.\nstable-baselines3 2.1.0 requires pandas, which is not installed.\nkaggle-environments 1.18.0 requires scipy>=1.11.2, which is not installed.\nkaggle-environments 1.18.0 requires transformers>=4.33.1, which is not installed.\ncategory-encoders 2.7.0 requires pandas>=1.0.5, which is not installed.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, which is not installed.\ncategory-encoders 2.7.0 requires scipy>=1.0.0, which is not installed.\nlime 0.2.0.1 requires matplotlib, which is not installed.\nlime 0.2.0.1 requires scikit-learn>=0.18, which is not installed.\nlime 0.2.0.1 requires scipy, which is not installed.\ncesium 0.12.4 requires pandas>=0.17.0, which is not installed.\ncesium 0.12.4 requires scikit-learn>=0.22.1, which is not installed.\ncesium 0.12.4 requires scipy>=0.16.0, which is not installed.\ntheano-pymc 1.1.2 requires scipy>=0.14, which is not installed.\nhep-ml 0.8.0 requires pandas>=1.0.0, which is not installed.\nhep-ml 0.8.0 requires scikit-learn>=1.0, which is not installed.\nhep-ml 0.8.0 requires scipy>=1.0.0, which is not installed.\nphik 0.12.5 requires matplotlib>=2.2.3, which is not installed.\nphik 0.12.5 requires pandas>=0.25.1, which is not installed.\nphik 0.12.5 requires scipy>=1.5.2, which is not installed.\nscikit-optimize 0.10.2 requires scikit-learn>=1.0.0, which is not installed.\nscikit-optimize 0.10.2 requires scipy>=1.1.0, which is not installed.\njax 0.5.2 requires scipy>=1.11.1, which is not installed.\ntsfresh 0.21.0 requires pandas>=0.25.0, which is not installed.\ntsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\nprophet 1.1.7 requires matplotlib>=2.0.0, which is not installed.\nprophet 1.1.7 requires pandas>=1.0.4, which is not installed.\ngeemap 0.35.3 requires matplotlib, which is not installed.\ngeemap 0.35.3 requires pandas, which is not installed.\ndopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\nbokeh 3.7.3 requires pandas>=1.2, which is not installed.\nsklearn-pandas 2.2.0 requires pandas>=1.1.4, which is not installed.\nsklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\nsklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nbigframes 2.12.0 requires matplotlib>=3.7.1, which is not installed.\nbigframes 2.12.0 requires pandas>=1.5.3, which is not installed.\nmatplotlib-venn 1.1.2 requires matplotlib, which is not installed.\nmatplotlib-venn 1.1.2 requires scipy, which is not installed.\nlibrosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\nlibrosa 0.11.0 requires scipy>=1.6.0, which is not installed.\nmusic21 9.3.0 requires matplotlib, which is not installed.\nscikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\nmissingno 0.5.2 requires matplotlib, which is not installed.\nmissingno 0.5.2 requires scipy, which is not installed.\nmissingno 0.5.2 requires seaborn, which is not installed.\ncvxpy 1.6.7 requires scipy>=1.11.0, which is not installed.\ngradio 5.38.1 requires pandas<3.0,>=1.0, which is not installed.\ncmdstanpy 1.2.5 requires pandas, which is not installed.\nhyperopt 0.2.7 requires scipy, which is not installed.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\nimbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, which is not installed.\nalbumentations 2.0.8 requires scipy>=1.10.0, which is not installed.\npandas-gbq 0.29.2 requires pandas>=1.1.4, which is not installed.\nyfinance 0.2.65 requires pandas>=1.3.0, which is not installed.\nbqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, which is not installed.\ntensorflow-decision-forests 1.11.0 requires pandas, which is not installed.\nclarabel 0.11.1 requires scipy, which is not installed.\nscs 3.2.7.post2 requires scipy, which is not installed.\nxarray-einstats 0.9.1 requires scipy>=1.11, which is not installed.\nxarray 2025.7.1 requires pandas>=2.2, which is not installed.\nmizani 0.13.5 requires pandas>=2.2.0, which is not installed.\nmizani 0.13.5 requires scipy>=1.8.0, which is not installed.\nplotnine 0.14.5 requires matplotlib>=3.8.0, which is not installed.\nplotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\nplotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\nwordcloud 1.9.4 requires matplotlib, which is not installed.\ndb-dtypes 1.4.3 requires pandas>=1.5.3, which is not installed.\npytensor 2.31.7 requires scipy<2,>=1, which is not installed.\ncufflinks 0.17.3 requires pandas>=0.19.2, which is not installed.\npynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\npynndescent 0.5.13 requires scipy>=1.0, which is not installed.\npanel 1.7.5 requires pandas>=1.2, which is not installed.\nstatsmodels 0.14.5 requires pandas!=2.1.0,>=1.4, which is not installed.\nstatsmodels 0.14.5 requires scipy!=1.9.2,>=1.8, which is not installed.\nlightgbm 4.6.0 requires scipy, which is not installed.\nhdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\nhdbscan 0.8.40 requires scipy>=1.0, which is not installed.\nholoviews 1.21.0 requires pandas>=1.3, which is not installed.\njaxlib 0.5.1 requires scipy>=1.11.1, which is not installed.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, which is not installed.\numap-learn 0.5.9.post2 requires scipy>=1.3.1, which is not installed.\nstumpy 1.13.0 requires scipy>=1.10, which is not installed.\npymc 5.25.1 requires pandas>=0.24.0, which is not installed.\npymc 5.25.1 requires scipy>=1.4.1, which is not installed.\nyellowbrick 1.5 requires matplotlib!=3.0.0,>=2.0.2, which is not installed.\nyellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\nyellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\nmlxtend 0.23.4 requires matplotlib>=3.0.0, which is not installed.\nmlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\nmlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\nosqp 1.0.4 requires scipy>=0.13.2, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.24.3 which is incompatible.\ndatasets 4.1.1 requires fsspec[http]<=2025.9.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\nbayesian-optimization 3.1.0 requires numpy>=1.25; python_full_version < \"3.13\", but you have numpy 1.24.3 which is incompatible.\nmne 1.10.1 requires numpy<3,>=1.25, but you have numpy 1.24.3 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.24.3 which is incompatible.\njax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\ntreescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\nalbumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.9.0 which is incompatible.\nxarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\nxarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\nalbucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\njaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\npymc 5.25.1 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\nblosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.24.3\nCollecting pandas\n  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scipy\n  Downloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nCollecting matplotlib\n  Downloading matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nCollecting seaborn\n  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.24.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nCollecting numpy>=1.23.2 (from pandas)\n  Using cached numpy-2.3.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nDownloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hUsing cached numpy-2.3.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\nInstalling collected packages: numpy, scipy, pandas, scikit-learn, matplotlib, seaborn\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.24.3\n    Uninstalling numpy-1.24.3:\n      Successfully uninstalled numpy-1.24.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.18.0 requires transformers>=4.33.1, which is not installed.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ndatasets 4.1.1 requires fsspec[http]<=2025.9.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\nydata-profiling 4.17.0 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.7 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.4 which is incompatible.\nydata-profiling 4.17.0 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.3 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.0 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.9.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed matplotlib-3.10.7 numpy-2.3.4 pandas-2.3.3 scikit-learn-1.7.2 scipy-1.16.3 seaborn-0.13.2\nLooking in indexes: https://download.pytorch.org/whl/cu121\nCollecting torch==2.1.0\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.1.0%2Bcu121-cp311-cp311-linux_x86_64.whl (2200.6 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m506.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.20.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.15.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.10.0)\nCollecting triton==2.1.0 (from torch==2.1.0)\n  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.3.4)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.2%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hINFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.32.5)\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.16.2%2Bcu121-cp311-cp311-linux_x86_64.whl (6.8 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.16.1%2Bcu121-cp311-cp311-linux_x86_64.whl (6.8 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.16.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nINFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\nCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.2%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hINFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.1.2%2Bcu121-cp311-cp311-linux_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.1.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.1.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2025.10.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\nInstalling collected packages: triton, torch, torchvision, torchaudio\n  Attempting uninstall: triton\n    Found existing installation: triton 3.5.0\n    Uninstalling triton-3.5.0:\n      Successfully uninstalled triton-3.5.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.9.0\n    Uninstalling torch-2.9.0:\n      Successfully uninstalled torch-2.9.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.18.0 requires transformers>=4.33.1, which is not installed.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-2.1.0+cu121 torchaudio-2.1.0+cu121 torchvision-0.16.0+cu121 triton-2.1.0\nCollecting transformers==4.36.2\n  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting bitsandbytes==0.41.3\n  Downloading bitsandbytes-0.41.3-py3-none-any.whl.metadata (9.8 kB)\nCollecting accelerate==0.25.0\n  Using cached accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\nCollecting peft==0.7.1\n  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (2.3.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (2.32.5)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (0.15.2)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (7.1.3)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (2.1.0+cu121)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (1.2.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.1.6)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (2.1.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.2) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.2) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.2) (2025.10.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.25.0) (3.0.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate==0.25.0) (1.3.0)\nDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.41.3-py3-none-any.whl (92.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hUsing cached accelerate-0.25.0-py3-none-any.whl (265 kB)\nDownloading peft-0.7.1-py3-none-any.whl (168 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, accelerate, transformers, peft\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.25.0 bitsandbytes-0.41.3 peft-0.7.1 transformers-4.36.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Restart runtime, then:\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n!pip install transformers==4.36.2 bitsandbytes==0.41.3 accelerate==0.25.0 peft==0.10.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:39:53.382935Z","iopub.execute_input":"2025-11-04T19:39:53.383204Z","iopub.status.idle":"2025-11-04T19:43:34.425785Z","shell.execute_reply.started":"2025-11-04T19:39:53.383181Z","shell.execute_reply":"2025-11-04T19:43:34.423936Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu121\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.9.0)\nINFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==3.1.0 (from torch)\n  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hINFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\nCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchaudio, torchvision\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\nCollecting transformers==4.36.2\n  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting bitsandbytes==0.41.3\n  Downloading bitsandbytes-0.41.3-py3-none-any.whl.metadata (9.8 kB)\nCollecting accelerate==0.25.0\n  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\nCollecting peft==0.10.0\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.36.2)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (2.32.5)\nCollecting tokenizers<0.19,>=0.14 (from transformers==4.36.2)\n  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.2) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (7.1.0)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (2.5.1+cu121)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.36.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.36.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.36.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.36.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.36.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.36.2) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.1.105)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (1.13.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.25.0) (12.5.82)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.25.0) (1.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.2) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.2) (2025.8.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.25.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.36.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.36.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.36.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.36.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.36.2) (2024.2.0)\nDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.41.3-py3-none-any.whl (92.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, huggingface-hub, tokenizers, transformers, accelerate, peft\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.9.0\n    Uninstalling accelerate-1.9.0:\n      Successfully uninstalled accelerate-1.9.0\n  Attempting uninstall: peft\n    Found existing installation: peft 0.16.0\n    Uninstalling peft-0.16.0:\n      Successfully uninstalled peft-0.16.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.25.0 bitsandbytes-0.41.3 huggingface-hub-0.36.0 peft-0.10.0 tokenizers-0.15.2 transformers-4.36.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"  import torch\n  torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T08:31:14.675369Z","iopub.execute_input":"2025-11-05T08:31:14.675710Z","iopub.status.idle":"2025-11-05T08:31:14.680817Z","shell.execute_reply.started":"2025-11-05T08:31:14.675687Z","shell.execute_reply":"2025-11-05T08:31:14.680079Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import os\nos._exit(00)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T06:39:07.192883Z","iopub.execute_input":"2025-11-05T06:39:07.193232Z","execution_failed":"2025-11-05T06:39:09.015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q accelerate sentencepiece wandb tqdm scipy scikit-learn matplotlib seaborn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:45:21.508332Z","iopub.execute_input":"2025-11-04T18:45:21.509059Z","iopub.status.idle":"2025-11-04T18:45:24.885240Z","shell.execute_reply.started":"2025-11-04T18:45:21.509033Z","shell.execute_reply":"2025-11-04T18:45:24.884261Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import transformers\nimport bitsandbytes\nimport peft\nimport torch\n\nprint(f\"transformers: {transformers.__version__}\")\nprint(f\"bitsandbytes: {bitsandbytes.__version__}\")\nprint(f\"peft: {peft.__version__}\")\nprint(f\"torch: {torch.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:08:52.502416Z","iopub.execute_input":"2025-11-04T19:08:52.503086Z","iopub.status.idle":"2025-11-04T19:08:52.507767Z","shell.execute_reply.started":"2025-11-04T19:08:52.503059Z","shell.execute_reply":"2025-11-04T19:08:52.506720Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"transformers: 4.41.2\nbitsandbytes: 0.48.2\npeft: 0.11.0\ntorch: 2.6.0+cu124\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Source - https://stackoverflow.com/questions/79786866/remoteentrynotfounderror-with-downloading-models-from-hugging-face-in-kaggle\n# Posted by ANURAG SINGH BHANDARI\n# Retrieved 5/11/2025, License - CC-BY-SA 4.0\n\n!pip install -U transformers huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T06:36:34.498048Z","iopub.execute_input":"2025-11-05T06:36:34.498590Z","iopub.status.idle":"2025-11-05T06:36:47.930757Z","shell.execute_reply.started":"2025-11-05T06:36:34.498566Z","shell.execute_reply":"2025-11-05T06:36:47.930033Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (1.0.0rc2)\nCollecting huggingface_hub\n  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface_hub, tokenizers, transformers\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface_hub-0.36.0 tokenizers-0.22.1 transformers-4.57.1\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T06:37:40.242269Z","iopub.execute_input":"2025-11-05T06:37:40.242556Z","iopub.status.idle":"2025-11-05T06:37:43.528420Z","shell.execute_reply.started":"2025-11-05T06:37:40.242534Z","shell.execute_reply":"2025-11-05T06:37:43.527619Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.48.2)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import torch\n# Verify P100\nprint(\"=\"*60)\nprint(\"GPU CHECK\")\nprint(\"=\"*60)\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"âœ“ GPU: {gpu_name}\")\n    print(f\"âœ“ Memory: {total_mem:.2f} GB\")\n    \n    if \"P100\" not in gpu_name:\n        print(\"âš ï¸ WARNING: Not using P100! Switch accelerator.\")\n    else:\n        print(\"âœ“ Perfect! P100 is ideal for this task.\")\nelse:\n    print(\"âœ— No GPU found!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T06:40:30.518786Z","iopub.execute_input":"2025-11-05T06:40:30.519410Z","iopub.status.idle":"2025-11-05T06:40:32.094656Z","shell.execute_reply.started":"2025-11-05T06:40:30.519383Z","shell.execute_reply":"2025-11-05T06:40:32.093877Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGPU CHECK\n============================================================\nâœ“ GPU: Tesla P100-PCIE-16GB\nâœ“ Memory: 15.89 GB\nâœ“ Perfect! P100 is ideal for this task.\n============================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\n\n# Get W&B key from Kaggle secrets\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n# Login programmatically (no prompt)\nwandb.login(key=wandb_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T06:40:05.321959Z","iopub.execute_input":"2025-11-05T06:40:05.322261Z","iopub.status.idle":"2025-11-05T06:40:14.154827Z","shell.execute_reply.started":"2025-11-05T06:40:05.322237Z","shell.execute_reply":"2025-11-05T06:40:14.154024Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmishralaavanya\u001b[0m (\u001b[33mmishralaavanya-svkm-s-narsee-monjee-institute-of-managem\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"print('hi')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T06:31:58.312636Z","iopub.execute_input":"2025-11-05T06:31:58.312912Z","iopub.status.idle":"2025-11-05T06:31:58.317389Z","shell.execute_reply.started":"2025-11-05T06:31:58.312890Z","shell.execute_reply":"2025-11-05T06:31:58.316574Z"}},"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}