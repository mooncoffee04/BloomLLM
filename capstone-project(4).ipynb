{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13601340,"sourceType":"datasetVersion","datasetId":8642922},{"sourceId":13601973,"sourceType":"datasetVersion","datasetId":8643385},{"sourceId":13602187,"sourceType":"datasetVersion","datasetId":8643531}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-03T18:08:24.424145Z","iopub.execute_input":"2025-11-03T18:08:24.424346Z","iopub.status.idle":"2025-11-03T18:08:24.432009Z","shell.execute_reply.started":"2025-11-03T18:08:24.424329Z","shell.execute_reply":"2025-11-03T18:08:24.431256Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/asap-aes/training_set_rel3.tsv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Initial Setup","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GEMINI_API_KEY\")\nsecret_value_1 = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T16:53:23.911687Z","iopub.execute_input":"2025-11-03T16:53:23.912666Z","iopub.status.idle":"2025-11-03T16:53:24.081444Z","shell.execute_reply.started":"2025-11-03T16:53:23.912633Z","shell.execute_reply":"2025-11-03T16:53:24.080769Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Download all the files","metadata":{}},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T18:21:57.752517Z","iopub.execute_input":"2025-11-03T18:21:57.752829Z","iopub.status.idle":"2025-11-03T18:21:57.899717Z","shell.execute_reply.started":"2025-11-03T18:21:57.752804Z","shell.execute_reply":"2025-11-03T18:21:57.898820Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import zipfile\nimport os\nfrom IPython.display import FileLink\n\ndef bundle_all_outputs(zip_name='full_project_outputs'):\n    zip_path = f\"/kaggle/working/{zip_name}.zip\"\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk('/kaggle/working/'):\n            for file in files:\n                if not file.endswith('.zip'):  # Avoid nesting zips\n                    file_path = os.path.join(root, file)\n                    arcname = os.path.relpath(file_path, '/kaggle/working/')\n                    zipf.write(file_path, arcname)\n    display(FileLink(zip_path))\n    print(f\"✓ Full bundle ready: {zip_path}\")\n\n# Run this to zip and get download link\nbundle_all_outputs()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T18:19:55.498946Z","iopub.execute_input":"2025-11-03T18:19:55.499539Z","iopub.status.idle":"2025-11-03T18:19:58.962385Z","shell.execute_reply.started":"2025-11-03T18:19:55.499506Z","shell.execute_reply":"2025-11-03T18:19:58.961535Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/full_project_outputs.zip","text/html":"<a href='/kaggle/working/full_project_outputs.zip' target='_blank'>/kaggle/working/full_project_outputs.zip</a><br>"},"metadata":{}},{"name":"stdout","text":"✓ Full bundle ready: /kaggle/working/full_project_outputs.zip\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Methodology","metadata":{}},{"cell_type":"markdown","source":"## Gemini Integration","metadata":{}},{"cell_type":"code","source":"!pip install -q google-generativeai\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\ngenai.configure(api_key=api_key)\nprint(\"✓ Gemini API configured\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:44:40.851058Z","iopub.execute_input":"2025-11-03T19:44:40.851682Z","iopub.status.idle":"2025-11-03T19:44:44.604228Z","shell.execute_reply.started":"2025-11-03T19:44:40.851653Z","shell.execute_reply":"2025-11-03T19:44:44.603169Z"}},"outputs":[{"name":"stdout","text":"✓ Gemini API configured\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"safety_settings = [\n    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n]\nmodel = genai.GenerativeModel(\n    model_name=\"gemini-2.5-flash\",\n    generation_config={\"temperature\": 0.3, \"top_p\": 0.8, \"max_output_tokens\": 200},\n    safety_settings=safety_settings\n)\nprint(\"✓ Model initialized with safety and config\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T18:30:34.611101Z","iopub.execute_input":"2025-11-03T18:30:34.611637Z","iopub.status.idle":"2025-11-03T18:30:34.617154Z","shell.execute_reply.started":"2025-11-03T18:30:34.611614Z","shell.execute_reply":"2025-11-03T18:30:34.616588Z"}},"outputs":[{"name":"stdout","text":"✓ Model initialized with safety and config\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Loading asap-aes dataset","metadata":{}},{"cell_type":"code","source":"asap_path = Path('/kaggle/input/asap-aes')\ntraining_file = asap_path / 'training_set_rel3.tsv'\ndf_asap = pd.read_csv(training_file, sep='\\t', encoding='ISO-8859-1')\n\ndf_asap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T18:35:23.847310Z","iopub.execute_input":"2025-11-03T18:35:23.847717Z","iopub.status.idle":"2025-11-03T18:35:24.094260Z","shell.execute_reply.started":"2025-11-03T18:35:23.847685Z","shell.execute_reply":"2025-11-03T18:35:24.093536Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"       essay_id  essay_set                                              essay  \\\n0             1          1  Dear local newspaper, I think effects computer...   \n1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n4             5          1  Dear @LOCATION1, I know having computers has a...   \n...         ...        ...                                                ...   \n12969     21626          8   In most stories mothers and daughters are eit...   \n12970     21628          8   I never understood the meaning laughter is th...   \n12971     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n12972     21630          8                                 Trippin' on fen...   \n12973     21633          8   Many people believe that laughter can improve...   \n\n       rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n0                   4               4             NaN              8   \n1                   5               4             NaN              9   \n2                   4               3             NaN              7   \n3                   5               5             NaN             10   \n4                   4               4             NaN              8   \n...               ...             ...             ...            ...   \n12969              17              18             NaN             35   \n12970              15              17             NaN             32   \n12971              20              26            40.0             40   \n12972              20              20             NaN             40   \n12973              20              20             NaN             40   \n\n       rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n0                 NaN             NaN            NaN  ...            NaN   \n1                 NaN             NaN            NaN  ...            NaN   \n2                 NaN             NaN            NaN  ...            NaN   \n3                 NaN             NaN            NaN  ...            NaN   \n4                 NaN             NaN            NaN  ...            NaN   \n...               ...             ...            ...  ...            ...   \n12969             NaN             NaN            NaN  ...            4.0   \n12970             NaN             NaN            NaN  ...            4.0   \n12971             NaN             NaN            NaN  ...            5.0   \n12972             NaN             NaN            NaN  ...            4.0   \n12973             NaN             NaN            NaN  ...            4.0   \n\n       rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  \\\n0                NaN            NaN            NaN            NaN   \n1                NaN            NaN            NaN            NaN   \n2                NaN            NaN            NaN            NaN   \n3                NaN            NaN            NaN            NaN   \n4                NaN            NaN            NaN            NaN   \n...              ...            ...            ...            ...   \n12969            4.0            4.0            3.0            NaN   \n12970            4.0            4.0            3.0            NaN   \n12971            5.0            5.0            5.0            4.0   \n12972            4.0            4.0            4.0            NaN   \n12973            4.0            4.0            4.0            NaN   \n\n       rater3_trait2  rater3_trait3  rater3_trait4  rater3_trait5  \\\n0                NaN            NaN            NaN            NaN   \n1                NaN            NaN            NaN            NaN   \n2                NaN            NaN            NaN            NaN   \n3                NaN            NaN            NaN            NaN   \n4                NaN            NaN            NaN            NaN   \n...              ...            ...            ...            ...   \n12969            NaN            NaN            NaN            NaN   \n12970            NaN            NaN            NaN            NaN   \n12971            4.0            4.0            4.0            4.0   \n12972            NaN            NaN            NaN            NaN   \n12973            NaN            NaN            NaN            NaN   \n\n       rater3_trait6  \n0                NaN  \n1                NaN  \n2                NaN  \n3                NaN  \n4                NaN  \n...              ...  \n12969            NaN  \n12970            NaN  \n12971            4.0  \n12972            NaN  \n12973            NaN  \n\n[12974 rows x 28 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>essay_set</th>\n      <th>essay</th>\n      <th>rater1_domain1</th>\n      <th>rater2_domain1</th>\n      <th>rater3_domain1</th>\n      <th>domain1_score</th>\n      <th>rater1_domain2</th>\n      <th>rater2_domain2</th>\n      <th>domain2_score</th>\n      <th>...</th>\n      <th>rater2_trait3</th>\n      <th>rater2_trait4</th>\n      <th>rater2_trait5</th>\n      <th>rater2_trait6</th>\n      <th>rater3_trait1</th>\n      <th>rater3_trait2</th>\n      <th>rater3_trait3</th>\n      <th>rater3_trait4</th>\n      <th>rater3_trait5</th>\n      <th>rater3_trait6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Dear local newspaper, I think effects computer...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n      <td>5</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n      <td>4</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n      <td>5</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>Dear @LOCATION1, I know having computers has a...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12969</th>\n      <td>21626</td>\n      <td>8</td>\n      <td>In most stories mothers and daughters are eit...</td>\n      <td>17</td>\n      <td>18</td>\n      <td>NaN</td>\n      <td>35</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12970</th>\n      <td>21628</td>\n      <td>8</td>\n      <td>I never understood the meaning laughter is th...</td>\n      <td>15</td>\n      <td>17</td>\n      <td>NaN</td>\n      <td>32</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12971</th>\n      <td>21629</td>\n      <td>8</td>\n      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n      <td>20</td>\n      <td>26</td>\n      <td>40.0</td>\n      <td>40</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>12972</th>\n      <td>21630</td>\n      <td>8</td>\n      <td>Trippin' on fen...</td>\n      <td>20</td>\n      <td>20</td>\n      <td>NaN</td>\n      <td>40</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12973</th>\n      <td>21633</td>\n      <td>8</td>\n      <td>Many people believe that laughter can improve...</td>\n      <td>20</td>\n      <td>20</td>\n      <td>NaN</td>\n      <td>40</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>12974 rows × 28 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"df_asap.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T18:35:47.869272Z","iopub.execute_input":"2025-11-03T18:35:47.869578Z","iopub.status.idle":"2025-11-03T18:35:47.879557Z","shell.execute_reply.started":"2025-11-03T18:35:47.869557Z","shell.execute_reply":"2025-11-03T18:35:47.878700Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"essay_id              0\nessay_set             0\nessay                 0\nrater1_domain1        0\nrater2_domain1        0\nrater3_domain1    12846\ndomain1_score         0\nrater1_domain2    11174\nrater2_domain2    11174\ndomain2_score     11174\nrater1_trait1     10682\nrater1_trait2     10682\nrater1_trait3     10682\nrater1_trait4     10682\nrater1_trait5     12251\nrater1_trait6     12251\nrater2_trait1     10682\nrater2_trait2     10682\nrater2_trait3     10682\nrater2_trait4     10682\nrater2_trait5     12251\nrater2_trait6     12251\nrater3_trait1     12846\nrater3_trait2     12846\nrater3_trait3     12846\nrater3_trait4     12846\nrater3_trait5     12846\nrater3_trait6     12846\ndtype: int64"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## Cleaning the dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport time  # For rate limiting\n\n# Load ASAP-AES (standard paths in /kaggle/input/asap-aes/)\nasap_path = Path('/kaggle/input/asap-aes')\ntraining_file = asap_path / 'training_set_rel3.tsv'\ndf_asap = pd.read_csv(training_file, sep='\\t', encoding='ISO-8859-1')\n\n# Define prompt mapping for each essay_set (1-8, from ASAP++ paper)\nprompt_dict = {\n    1: \"The writers had to write a letter to their local newspaper in which they stated their opinion on the effects computers have on people.\",\n    2: \"The writers had to write a persuasive essay reflecting their views on censorship in libraries.\",\n    3: \"The writers had to read an extract from *Rough Road Ahead: Do Not Exceed Posted Speed Limit* by Joe Kurmaskie. They then had to explain how the features of the setting affected the cyclist.\",\n    4: \"The writers had to read an extract from *Winter Hibiscus* by Minfong Ho. They then had to explain why the author concludes the story in the way that she did.\",\n    5: \"The writers had to read an extract from *Narciso Rodriguez* by Narciso Rodriguez. They then had to describe the mood created by the author with supporting evidence from the extract.\",\n    6: \"The writers had to read an extract from *The Mooring Mast* by Marcia Amidon Lusted. They then had to answer a question about the difficulties faced by the builders of the Empire State Building in allowing dirigibles to dock there.\",\n    7: \"Write a story about a time when you, or someone you know, was patient.\",\n    8: \"Write a story in which laughter plays a part.\"\n}\n\n# Clean and sample 500 rows (focus on essay_set and essays)\ndf_asap = df_asap.dropna(subset=['essay_set', 'essay'])\ndf_sample = df_asap.sample(n=500, random_state=42).reset_index(drop=True)\n\n# Assign prompts based on essay_set\ndf_sample['prompt'] = df_sample['essay_set'].map(prompt_dict)\ndf_sample['prompt'] = df_sample['prompt'].fillna(\"Generic essay prompt for Bloom's classification.\")  # Rare fallback\n\ndf_sample['response'] = df_sample['essay']  # Student response\ndf_sample['essay_score'] = df_sample['domain1_score']  # Use primary holistic score\ndf_sample = df_sample[['prompt', 'response', 'essay_score', 'essay_set']]  # Unified format base + essay_set for reference\n\nprint(f\"✓ Loaded {len(df_sample)} ASAP samples with mapped prompts\")\nsave_checkpoint(\"asap_sample\", df_sample, \"Raw ASAP sample for labeling\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T18:46:18.814669Z","iopub.execute_input":"2025-11-03T18:46:18.814969Z","iopub.status.idle":"2025-11-03T18:46:19.082592Z","shell.execute_reply.started":"2025-11-03T18:46:18.814948Z","shell.execute_reply":"2025-11-03T18:46:19.081728Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 500 ASAP samples with mapped prompts\n✓ SAVED (root): asap_sample (Raw ASAP sample for labeling)\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/checkpoint_asap_sample_20251103_184619'"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## Bloom's Taxonomy Dataset Integration","metadata":{}},{"cell_type":"code","source":"pip install python-docx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T18:56:33.382783Z","iopub.execute_input":"2025-11-03T18:56:33.383102Z","iopub.status.idle":"2025-11-03T18:56:37.275247Z","shell.execute_reply.started":"2025-11-03T18:56:33.383079Z","shell.execute_reply":"2025-11-03T18:56:37.274426Z"}},"outputs":[{"name":"stdout","text":"Collecting python-docx\n  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\nRequirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.15.0)\nDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: python-docx\nSuccessfully installed python-docx-1.2.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"\"\"\"\nDOCX to DataFrame Converter for Bloom's Taxonomy Dataset\n\nThis script converts a structured DOCX file containing questions \norganized by Bloom's taxonomy levels into a pandas DataFrame.\n\nStructure expected:\n- Bloom's level headers: Knowledge, Comprehension, Application, Analysis, Synthesis, Evaluation\n- Questions listed under each header\n\"\"\"\n\nimport docx\nimport pandas as pd\nfrom pathlib import Path\n\n\ndef convert_blooms_docx_to_dataframe(docx_path, output_csv=None):\n    \"\"\"\n    Convert a Bloom's taxonomy structured DOCX to a pandas DataFrame.\n    \n    Parameters:\n    -----------\n    docx_path : str or Path\n        Path to the DOCX file\n    output_csv : str or Path, optional\n        If provided, saves the DataFrame to this CSV path\n        \n    Returns:\n    --------\n    pd.DataFrame\n        DataFrame with columns: ['bloom_level', 'question', 'question_id']\n    \"\"\"\n    \n    # Load the document\n    doc = docx.Document(docx_path)\n    \n    # Define Bloom's taxonomy levels (in order)\n    bloom_levels = ['Knowledge', 'Comprehension', 'Application', \n                    'Analysis', 'Synthesis', 'Evaluation']\n    \n    # Storage for parsed data\n    data = []\n    current_level = None\n    question_counter = {level: 0 for level in bloom_levels}\n    \n    # Parse document\n    for para in doc.paragraphs:\n        text = para.text.strip()\n        \n        # Skip empty paragraphs\n        if not text:\n            continue\n        \n        # Check if this is a Bloom's level header\n        if text in bloom_levels:\n            current_level = text\n            print(f\"Found Bloom's level: {current_level}\")\n            continue\n        \n        # If we have a current level, this is a question\n        if current_level:\n            question_counter[current_level] += 1\n            question_id = f\"{current_level[:3].upper()}_{question_counter[current_level]:03d}\"\n            \n            data.append({\n                'question_id': question_id,\n                'bloom_level': current_level,\n                'question': text\n            })\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Print summary statistics\n    print(\"\\n\" + \"=\"*60)\n    print(\"CONVERSION SUMMARY\")\n    print(\"=\"*60)\n    print(f\"Total questions extracted: {len(df)}\")\n    print(\"\\nQuestions per Bloom's level:\")\n    print(df['bloom_level'].value_counts().sort_index())\n    \n    # Save to CSV if requested\n    if output_csv:\n        df.to_csv(output_csv, index=False)\n        print(f\"\\n✓ Saved to: {output_csv}\")\n    \n    return df\n\n\ndef validate_dataframe(df):\n    \"\"\"\n    Perform basic validation checks on the DataFrame.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        The DataFrame to validate\n        \n    Returns:\n    --------\n    dict : Validation results\n    \"\"\"\n    validation = {\n        'total_questions': len(df),\n        'missing_values': df.isnull().sum().to_dict(),\n        'unique_levels': df['bloom_level'].nunique(),\n        'expected_levels': 6,\n        'duplicate_questions': df['question'].duplicated().sum(),\n        'empty_questions': (df['question'].str.strip() == '').sum(),\n        'avg_question_length': df['question'].str.len().mean(),\n        'min_question_length': df['question'].str.len().min(),\n        'max_question_length': df['question'].str.len().max()\n    }\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"DATA QUALITY CHECKS\")\n    print(\"=\"*60)\n    print(f\"Total questions: {validation['total_questions']}\")\n    print(f\"Unique Bloom's levels: {validation['unique_levels']}/{validation['expected_levels']}\")\n    print(f\"Duplicate questions: {validation['duplicate_questions']}\")\n    print(f\"Empty questions: {validation['empty_questions']}\")\n    print(f\"Average question length: {validation['avg_question_length']:.1f} characters\")\n    print(f\"Question length range: {validation['min_question_length']} - {validation['max_question_length']}\")\n    \n    return validation\n\n\ndef preview_samples(df, n=3):\n    \"\"\"\n    Display sample questions from each Bloom's level.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        The DataFrame to sample from\n    n : int\n        Number of samples per level\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"SAMPLE QUESTIONS (3 per level)\")\n    print(\"=\"*60)\n    \n    for level in df['bloom_level'].unique():\n        print(f\"\\n{level.upper()}:\")\n        samples = df[df['bloom_level'] == level].head(n)\n        for idx, row in samples.iterrows():\n            print(f\"  [{row['question_id']}] {row['question'][:100]}...\")\n\n\nif __name__ == \"__main__\":\n    # File paths\n    input_docx = \"/kaggle/input/researchgate-blooms-tax/BCLsDataSet.docx\"\n    output_csv = \"/kaggle/working/bcls.csv\"\n    \n    # Convert DOCX to DataFrame\n    df = convert_blooms_docx_to_dataframe(input_docx, output_csv)\n    \n    # Validate the data\n    validation_results = validate_dataframe(df)\n    \n    # Show samples\n    preview_samples(df, n=3)\n    \n    # Display first few rows\n    print(\"\\n\" + \"=\"*60)\n    print(\"FIRST 10 ROWS\")\n    print(\"=\"*60)\n    print(df.head(10).to_string())\n    \n    print(\"\\n✓ Conversion complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:02:27.388144Z","iopub.execute_input":"2025-11-03T19:02:27.388476Z","iopub.status.idle":"2025-11-03T19:02:27.668355Z","shell.execute_reply.started":"2025-11-03T19:02:27.388443Z","shell.execute_reply":"2025-11-03T19:02:27.667449Z"}},"outputs":[{"name":"stdout","text":"Found Bloom's level: Knowledge\nFound Bloom's level: Comprehension\nFound Bloom's level: Application\nFound Bloom's level: Analysis\nFound Bloom's level: Synthesis\nFound Bloom's level: Evaluation\n\n============================================================\nCONVERSION SUMMARY\n============================================================\nTotal questions extracted: 600\n\nQuestions per Bloom's level:\nbloom_level\nAnalysis         100\nApplication      100\nComprehension    100\nEvaluation       100\nKnowledge        100\nSynthesis        100\nName: count, dtype: int64\n\n✓ Saved to: /kaggle/working/bcls.csv\n\n============================================================\nDATA QUALITY CHECKS\n============================================================\nTotal questions: 600\nUnique Bloom's levels: 6/6\nDuplicate questions: 1\nEmpty questions: 0\nAverage question length: 65.0 characters\nQuestion length range: 16 - 247\n\n============================================================\nSAMPLE QUESTIONS (3 per level)\n============================================================\n\nKNOWLEDGE:\n  [KNO_001] About what proportion of the population of the US is living on farms?...\n  [KNO_002] Correctly label the brain lobes indicated on the diagram below...\n  [KNO_003] Define compound interest....\n\nCOMPREHENSION:\n  [COM_001] Compare Calliope with Howie. Use the word bank....\n  [COM_002] Compare historical events to contemporary situations...\n  [COM_003] Complete an analogy (analogy tasks are inference tasks)...\n\nAPPLICATION:\n  [APP_001] Apply laws of statistics to evaluate the reliability of a written test....\n  [APP_002] Apply shading to produce depth in drawing....\n  [APP_003] Apply the rule of law to a new situation...\n\nANALYSIS:\n  [ANA_001] Analyze safe and dangerous aspects of these features....\n  [ANA_002] Analyze the characteristics of frogs....\n  [ANA_003] Analyze the movements and sounds of a frog....\n\nSYNTHESIS:\n  [SYN_001] Apply and integrate several different strategies to solve a mathematical problem (not according to o...\n  [SYN_002] Can you create new and unusual uses for . . .?...\n  [SYN_003] Can you design a . . . to . . .?...\n\nEVALUATION:\n  [EVA_001] After designing an experiment, examining the results, and drawing conclusions, determines in what wa...\n  [EVA_002] After examining the videotape of a play in a football game, determine the degree to which the defens...\n  [EVA_003] After solving a problem, determine the degree to which the problem was solved as efficiently as poss...\n\n============================================================\nFIRST 10 ROWS\n============================================================\n  question_id bloom_level                                                               question\n0     KNO_001   Knowledge  About what proportion of the population of the US is living on farms?\n1     KNO_002   Knowledge         Correctly label the brain lobes indicated on the diagram below\n2     KNO_003   Knowledge                                              Define compound interest.\n3     KNO_004   Knowledge                                     Define four types of traceability.\n4     KNO_005   Knowledge                                                   Define mercantilism.\n5     KNO_006   Knowledge                          Define stream bank, floodplain and substrate.\n6     KNO_007   Knowledge                               Define the meaning of the Olympic Motto.\n7     KNO_008   Knowledge                          Draw and label a diagram of a typical stream.\n8     KNO_009   Knowledge    Draw the basic outline of a page of notes using the Cornell Method?\n9     KNO_010   Knowledge                  Draw the block diagram of the central processing unit\n\n✓ Conversion complete!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"df_sample.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T19:14:13.836665Z","iopub.execute_input":"2025-11-03T19:14:13.837574Z","iopub.status.idle":"2025-11-03T19:14:13.846356Z","shell.execute_reply.started":"2025-11-03T19:14:13.837539Z","shell.execute_reply":"2025-11-03T19:14:13.845511Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  The writers had to read an extract from *Winte...   \n1  The writers had to read an extract from *Winte...   \n2  The writers had to write a letter to their loc...   \n3  Write a story about a time when you, or someon...   \n4  The writers had to read an extract from *Rough...   \n\n                                            response  essay_score  essay_set  \n0  The author concludes the story with this parag...            2          4  \n1  I think the author concludes the story with th...            2          4  \n2  Computers, a very much talked about subject. D...           10          1  \n3  One day when I was patient is when I was waiti...           17          7  \n4  The setting that affect the cyclist is the con...            1          3  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response</th>\n      <th>essay_score</th>\n      <th>essay_set</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The writers had to read an extract from *Winte...</td>\n      <td>The author concludes the story with this parag...</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The writers had to read an extract from *Winte...</td>\n      <td>I think the author concludes the story with th...</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The writers had to write a letter to their loc...</td>\n      <td>Computers, a very much talked about subject. D...</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Write a story about a time when you, or someon...</td>\n      <td>One day when I was patient is when I was waiti...</td>\n      <td>17</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The writers had to read an extract from *Rough...</td>\n      <td>The setting that affect the cyclist is the con...</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"## Data Labelling","metadata":{}},{"cell_type":"code","source":"# -------------------------------------------------\n# 1. Imports & Gemini setup\n# -------------------------------------------------\nimport pandas as pd\nimport json\nimport time\nfrom tqdm import tqdm\nimport google.generativeai as genai\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\n\n# (your GEMINI_API_KEY already loaded)\ngenai.configure(api_key=UserSecretsClient().get_secret(\"GEMINI_API_KEY\"))\n\n# -------------------------------------------------\n# 2. Load Bloom's taxonomy examples\n# -------------------------------------------------\nblooms_path = Path('/kaggle/input/bloom-levels/bcls.csv')  # adjust if needed\ndf_blooms = pd.read_csv(blooms_path)\n\n# Map original → Revised Bloom\nlevel_map = {\n    'Knowledge': 'Remember',\n    'Comprehension': 'Understand',\n    'Application': 'Apply',\n    'Analysis': 'Analyze',\n    'Synthesis': 'Create',\n    'Evaluation': 'Evaluate'\n}\ndf_blooms['bloom_level'] = df_blooms['bloom_level'].map(level_map)\n\n# -------------------------------------------------\n# 3. Build few-shot examples (3 per level)\n# -------------------------------------------------\nsamples_per_level = 3\ndf_fewshot = (\n    df_blooms.groupby('bloom_level')\n    .apply(lambda g: g.sample(n=min(len(g), samples_per_level), random_state=42))\n    .reset_index(drop=True)\n)\n\ndef build_fewshot_text(df):\n    lines = []\n    for _, r in df.iterrows():\n        lines.append(\n            f\"Question: {r['question']}\\n\"\n            f\"Bloom Level: {r['bloom_level']}\\n\"\n            f\"Score: 5\\n\"\n            f\"Rationale: Matches {r['bloom_level']} verbs.\"\n        )\n    return \"\\n\\n\".join(lines)\n\nfewshot_text = build_fewshot_text(df_fewshot)\n\n# -------------------------------------------------\n# 4. Prompt template (only the *prompt* is inserted)\n# -------------------------------------------------\nlabeling_prompt_template = (\n    \"You are a linguistics expert analyzing the cognitive demand of a question.\\n\"\n    \"Assign **one** of the six Revised Bloom levels (Remember, Understand, Apply, Analyze, Evaluate, Create).\\n\\n\"\n    f\"Examples:\\n{fewshot_text}\\n\\n\"\n    \"Question: {prompt}\\n\\n\"\n    \"Respond **only** with valid JSON:\\n\"\n    \"{{\\n\"\n    \"  \\\"bloom_level\\\": \\\"Level\\\",\\n\"\n    \"  \\\"relevance\\\": 1-5,\\n\"\n    \"  \\\"rationale\\\": \\\"Brief reason (1-2 sentences)\\\"\\n\"\n    \"}}\"\n)\n\n# -------------------------------------------------\n# 5. Gemini model (free-tier correct name from your dashboard)\n# -------------------------------------------------\nmodel = genai.GenerativeModel(\n    model_name=\"gemini-2.5-flash-lite\",  \n    generation_config={\n        \"temperature\": 0.3,\n        \"top_p\": 0.8,\n        \"max_output_tokens\": 200\n    }\n)\n\n# -------------------------------------------------\n# 6. Labeling function (batch + progress + checkpoints)\n# -------------------------------------------------\ndef label_with_gemini(df, batch_size=10, checkpoint_every=100):\n    results = []\n    for start in tqdm(range(0, len(df), batch_size), desc=\"Labeling Batches\"):\n        batch = df.iloc[start:start+batch_size]\n        for _, row in batch.iterrows():\n            prompt = labeling_prompt_template.format(prompt=row['prompt'])\n            try:\n                resp = model.generate_content(prompt)\n                # Safety: guard empty response\n                if not resp.candidates or not resp.candidates[0].content.parts:\n                    raise ValueError(\"Empty response\")\n                txt = resp.candidates[0].content.parts[0].text.strip()\n                txt = txt.replace('```json', '').replace('```', '').strip()\n                data = json.loads(txt)\n\n                row_labeled = row.copy()\n                row_labeled['bloom_level'] = data.get('bloom_level', 'Unknown')\n                row_labeled['label_score'] = data.get('relevance', 0)\n                row_labeled['rationale'] = data.get('rationale', 'Failed')\n                results.append(row_labeled)\n\n            except Exception as e:\n                print(f\"\\nError on row {row.name}: {e}\")\n                row_labeled = row.copy()\n                row_labeled['bloom_level'] = 'Unknown'\n                row_labeled['label_score'] = 0\n                row_labeled['rationale'] = 'Labeling failed'\n                results.append(row_labeled)\n\n            time.sleep(5)  # 15 RPM → ~1 request / 4s (safe buffer)\n\n        # ---- partial checkpoint ----\n        if (start + batch_size) % checkpoint_every == 0 and results:\n            pd.DataFrame(results).to_csv(\n                f\"/kaggle/working/checkpoint_labeled_{start+len(batch)}.csv\",\n                index=False\n            )\n            print(f\"  partial save: {len(results)} rows\")\n\n    return pd.DataFrame(results)\n\n# -------------------------------------------------\n# 7. RUN on the full df_sample\n# -------------------------------------------------\ndf_labeled = label_with_gemini(df_sample, batch_size=10, checkpoint_every=100)\n\n# -------------------------------------------------\n# 8. Final save + split\n# -------------------------------------------------\nsave_checkpoint(\"final_labeled_500\", df_labeled, \"Full Bloom-labeled ASAP (prompt-only)\")\n\ntrain_val, test = train_test_split(\n    df_labeled, test_size=0.2, random_state=42, stratify=df_labeled['essay_set']\n)\ntrain, val = train_test_split(\n    train_val, test_size=0.25, random_state=42, stratify=train_val['essay_set']\n)\n\nsave_checkpoint(\n    \"splits\",\n    {'train': train, 'val': val, 'test': test},\n    \"60/20/20 split, stratified by essay_set\"\n)\n\n# -------------------------------------------------\n# 9. Quick sanity check\n# -------------------------------------------------\nprint(\"\\nFirst 5 labeled rows:\")\nprint(df_labeled[['prompt', 'bloom_level', 'label_score', 'rationale']].head())\nprint(\"\\nLevel distribution:\")\nprint(df_labeled['bloom_level'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T20:28:06.895144Z","iopub.execute_input":"2025-11-03T20:28:06.895925Z","iopub.status.idle":"2025-11-03T21:14:58.413620Z","shell.execute_reply.started":"2025-11-03T20:28:06.895900Z","shell.execute_reply":"2025-11-03T21:14:58.412708Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/17751501.py:38: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda g: g.sample(n=min(len(g), samples_per_level), random_state=42))\nLabeling Batches:  20%|██        | 10/50 [09:21<37:26, 56.17s/it]","output_type":"stream"},{"name":"stdout","text":"  partial save: 100 rows\n","output_type":"stream"},{"name":"stderr","text":"Labeling Batches:  40%|████      | 20/50 [18:43<28:08, 56.29s/it]","output_type":"stream"},{"name":"stdout","text":"  partial save: 200 rows\n","output_type":"stream"},{"name":"stderr","text":"Labeling Batches:  60%|██████    | 30/50 [28:08<18:51, 56.57s/it]","output_type":"stream"},{"name":"stdout","text":"  partial save: 300 rows\n","output_type":"stream"},{"name":"stderr","text":"Labeling Batches:  80%|████████  | 40/50 [37:30<09:21, 56.14s/it]","output_type":"stream"},{"name":"stdout","text":"  partial save: 400 rows\n","output_type":"stream"},{"name":"stderr","text":"Labeling Batches: 100%|██████████| 50/50 [46:51<00:00, 56.23s/it]","output_type":"stream"},{"name":"stdout","text":"  partial save: 500 rows\n✓ SAVED (root): final_labeled_500 (Full Bloom-labeled ASAP (prompt-only))\n✓ SAVED (root): splits (60/20/20 split, stratified by essay_set)\n\nFirst 5 labeled rows:\n                                              prompt bloom_level  label_score  \\\n0  The writers had to read an extract from *Winte...     Analyze            5   \n1  The writers had to read an extract from *Winte...     Analyze            5   \n2  The writers had to write a letter to their loc...      Create            5   \n3  Write a story about a time when you, or someon...      Create            5   \n4  The writers had to read an extract from *Rough...     Analyze            5   \n\n                                           rationale  \n0  The question requires students to break down t...  \n1  The question requires students to break down t...  \n2  The question requires students to produce a ne...  \n3  The question asks the user to 'write a story,'...  \n4  The question requires the writer to break down...  \n\nLevel distribution:\nbloom_level\nAnalyze    276\nCreate     224\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# Revised DIRS for flatter structure\nDIRS = {\n    'models': '/kaggle/working/models',      # Keep for large files\n    'data': '/kaggle/working/data',         # Keep for inputs/processed\n    'prompts': '/kaggle/working/prompts',   # Keep for templates\n    'gold_labels': '/kaggle/working/gold_labels',  # Keep for evals\n    # Flatten these:\n    # 'checkpoints': '/kaggle/working/checkpoints',  # → Use root\n    # 'results': '/kaggle/working/results',         # → Use root\n}\n\n# # Create dirs (skip flattened ones)\n# for dir_path in DIRS.values():\n#     Path(dir_path).mkdir(parents=True, exist_ok=True)\n\n# Update save_checkpoint to root\ndef save_checkpoint(name: str, data, description: str = \"\", metadata: Dict = None):\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    base_path = f\"/kaggle/working/checkpoint_{name}_{timestamp}\"  # Flattened!\n    \n    # Rest unchanged...\n    with open(f\"{base_path}.pkl\", 'wb') as f:\n        pickle.dump(data, f)\n    \n    meta = { 'timestamp': timestamp, 'description': description, 'data_type': type(data).__name__, **(metadata or {}) }\n    with open(f\"{base_path}_meta.json\", 'w') as f:\n        json.dump(meta, f, indent=2)\n    \n    print(f\"✓ SAVED (root): {name} ({description})\")\n    return base_path\n\n# New: Auto-zip all root outputs at end\ndef bundle_all_outputs(zip_name='full_project_outputs'):\n    zip_path = f\"/kaggle/working/{zip_name}.zip\"\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk('/kaggle/working/'):\n            for file in files:\n                if not file.endswith('.zip'):  # Avoid nesting zips\n                    file_path = os.path.join(root, file)\n                    arcname = os.path.relpath(file_path, '/kaggle/working/')\n                    zipf.write(file_path, arcname)\n    display(FileLink(zip_path))\n    print(f\"✓ Full bundle ready: {zip_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T11:09:59.130397Z","iopub.execute_input":"2025-11-04T11:09:59.130972Z","iopub.status.idle":"2025-11-04T11:09:59.155479Z","shell.execute_reply.started":"2025-11-04T11:09:59.130948Z","shell.execute_reply":"2025-11-04T11:09:59.154590Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/3390744708.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Update save_checkpoint to root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d_%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/kaggle/working/checkpoint_{name}_{timestamp}\"\u001b[0m  \u001b[0;31m# Flattened!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Dict' is not defined"],"ename":"NameError","evalue":"name 'Dict' is not defined","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"# Usage: Call at notebook end\nbundle_all_outputs()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T11:08:56.115550Z","iopub.execute_input":"2025-11-04T11:08:56.115796Z","iopub.status.idle":"2025-11-04T11:08:56.164558Z","shell.execute_reply.started":"2025-11-04T11:08:56.115773Z","shell.execute_reply":"2025-11-04T11:08:56.163421Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/4051530365.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Usage: Call at notebook end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbundle_all_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'bundle_all_outputs' is not defined"],"ename":"NameError","evalue":"name 'bundle_all_outputs' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"## Analysis\n\nIt turns out that the dataset ASAP (essays of students) consists of prompts that are either 'create' or 'analyze' ONLY.","metadata":{}},{"cell_type":"markdown","source":"# Fine Tuning with LoRA","metadata":{}},{"cell_type":"code","source":"!pip install -q --force-reinstall \"numpy<2\" pyarrow==14.0.1 datasets==2.16.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T21:36:52.142462Z","iopub.execute_input":"2025-11-03T21:36:52.143124Z","iopub.status.idle":"2025-11-03T21:37:21.929662Z","shell.execute_reply.started":"2025-11-03T21:36:52.143097Z","shell.execute_reply":"2025-11-03T21:37:21.928776Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\ngoogle-cloud-bigtable 2.32.0 requires google-api-core[grpc]<3.0.0,>=2.17.0, but you have google-api-core 1.34.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.1 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\ntokenizers 0.21.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.0.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ntransformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.1 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nos._exit(00)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-03T23:02:15.442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------\n# 1. Install & Imports\n# -------------------------------------------------\n!pip install -q transformers peft bitsandbytes accelerate datasets sentencepiece\n\nimport torch\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset\nimport os\n\n# -------------------------------------------------\n# 2. Load Labeled Train Data\n# -------------------------------------------------\n# Load the saved splits dict\nsplits = pd.read_pickle(\"/kaggle/working/checkpoint_splits_20251103_211458.pkl\")  # or your path\n\n# Extract the train DataFrame\ntrain_df = splits['train']\nval_df = splits['val']\ntest_df = splits['test']\n\nprint(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n\n# -------------------------------------------------\n# 3. Format Instruction Pairs (Bloom in system prompt)\n# -------------------------------------------------\ndef format_instruction(row):\n    system = f\"You are an expert evaluator at the {row['bloom_level']} level of Bloom's Taxonomy.\"\n    user = f\"Prompt: {row['prompt']}\\n\\nStudent Response: {row['response']}\\n\\nScore this response from 1 to 5 and provide a brief rationale.\"\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system},\n            {\"role\": \"user\", \"content\": user}\n        ]\n    }\n\ntrain_data = [format_instruction(row) for _, row in train_df.iterrows()]\nhf_dataset = Dataset.from_list(train_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T23:02:24.970032Z","iopub.execute_input":"2025-11-03T23:02:24.970731Z","iopub.status.idle":"2025-11-03T23:03:52.895817Z","shell.execute_reply.started":"2025-11-03T23:02:24.970705Z","shell.execute_reply":"2025-11-03T23:03:52.895116Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-03 23:03:49.615667: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762211029.639411     127 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762211029.646311     127 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Train: 300, Val: 100, Test: 100\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GEMINI_API_KEY\")\nsecret_value_1 = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_value_2 = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T22:42:15.402964Z","iopub.execute_input":"2025-11-03T22:42:15.403710Z","iopub.status.idle":"2025-11-03T22:42:15.638073Z","shell.execute_reply.started":"2025-11-03T22:42:15.403677Z","shell.execute_reply":"2025-11-03T22:42:15.637391Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# -------------------------------------------------\n# 1. Install & Setup W&B\n# -------------------------------------------------\n!pip install -q wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T22:05:54.032460Z","iopub.execute_input":"2025-11-03T22:05:54.033394Z","iopub.status.idle":"2025-11-03T22:05:57.549510Z","shell.execute_reply.started":"2025-11-03T22:05:54.033361Z","shell.execute_reply":"2025-11-03T22:05:57.548433Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\n\n# Get W&B key from Kaggle secrets\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n# Login programmatically (no prompt)\nwandb.login(key=wandb_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T23:03:59.448274Z","iopub.execute_input":"2025-11-03T23:03:59.448970Z","iopub.status.idle":"2025-11-03T23:04:08.609245Z","shell.execute_reply.started":"2025-11-03T23:03:59.448940Z","shell.execute_reply":"2025-11-03T23:04:08.608655Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmishralaavanya\u001b[0m (\u001b[33mmishralaavanya-svkm-s-narsee-monjee-institute-of-managem\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# -------------------------------------------------\n# OPTIMIZED FOR P100 - Run after switching\n# -------------------------------------------------\n\nimport os\n# No need for CUDA_VISIBLE_DEVICES with single GPU\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\nimport torch\nimport gc\nimport pandas as pd\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    TrainingArguments, \n    Trainer, \n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset\nimport wandb\n\n# Verify P100\nprint(\"=\"*60)\nprint(\"GPU CHECK\")\nprint(\"=\"*60)\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"✓ GPU: {gpu_name}\")\n    print(f\"✓ Memory: {total_mem:.2f} GB\")\n    \n    if \"P100\" not in gpu_name:\n        print(\"⚠️ WARNING: Not using P100! Switch accelerator.\")\n    else:\n        print(\"✓ Perfect! P100 is ideal for this task.\")\nelse:\n    print(\"✗ No GPU found!\")\nprint(\"=\"*60)\n\n# Load data\nimport pickle\nwith open(\"/kaggle/working/checkpoint_splits_20251103_211458.pkl\", \"rb\") as f:\n    splits = pickle.load(f)\n\ntrain_df = splits['train']\nval_df = splits['val']\ntest_df = splits['test']\n\ndef format_instruction(row):\n    system = f\"You are an expert evaluator at the {row['bloom_level']} level of Bloom's Taxonomy.\"\n    user = f\"Prompt: {row['prompt']}\\n\\nStudent Response: {row['response']}\\n\\nScore this response from 1 to 5 and provide a brief rationale.\"\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system},\n            {\"role\": \"user\", \"content\": user}\n        ]\n    }\n\ntrain_data = [format_instruction(row) for _, row in train_df.iterrows()]\nhf_dataset = Dataset.from_list(train_data)\n\nprint(f\"✓ Dataset: {len(hf_dataset)} examples\")\n\n# -------------------------------------------------\n# Training Function - Optimized for P100\n# -------------------------------------------------\n\ndef finetune_model_p100(model_id, output_dir):\n    \"\"\"Training optimized for single P100 GPU\"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Training: {model_id}\")\n    print(f\"{'='*60}\")\n    \n    os.makedirs(output_dir, exist_ok=True)\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    # W&B\n    run = wandb.init(\n        project=\"llm-jury-bloom-p100\",\n        name=f\"{model_id.split('/')[-1]}-lora\",\n        config={\n            \"model\": model_id,\n            \"gpu\": \"P100\",\n            \"train_size\": len(train_df),\n            \"lora_r\": 8,\n            \"lora_alpha\": 16,\n            \"batch_size\": 4,  # Can use 4 on P100!\n            \"grad_accum\": 4,\n            \"steps\": 300,\n            \"max_length\": 512\n        }\n    )\n    \n    try:\n        # Quantization\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\"\n        )\n        \n        # Load model\n        print(\"Loading model...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n            use_cache=False\n        )\n        \n        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.padding_side = \"right\"\n        \n        # LoRA\n        print(\"Applying LoRA...\")\n        model.gradient_checkpointing_enable()\n        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n        \n        lora_config = LoraConfig(\n            r=8,\n            lora_alpha=16,\n            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\"\n        )\n        model = get_peft_model(model, lora_config)\n        model.print_trainable_parameters()\n        \n        # Tokenize\n        def tokenize_fn(examples):\n            texts = []\n            for msgs in examples[\"messages\"]:\n                text = \"\"\n                for msg in msgs:\n                    text += f\"{msg['role']}: {msg['content']}\\n\"\n                texts.append(text)\n            \n            result = tokenizer(\n                texts,\n                truncation=True,\n                max_length=512,\n                padding=\"max_length\",\n                return_tensors=None\n            )\n            result[\"labels\"] = result[\"input_ids\"].copy()\n            return result\n        \n        print(\"Tokenizing...\")\n        tokenized = hf_dataset.map(\n            tokenize_fn,\n            batched=True,\n            remove_columns=hf_dataset.column_names\n        )\n        \n        # Training args - Better for P100\n        args = TrainingArguments(\n            output_dir=output_dir,\n            per_device_train_batch_size=4,  # Increased from 2!\n            gradient_accumulation_steps=4,  # Decreased from 8\n            max_steps=300,\n            learning_rate=2e-4,\n            fp16=True,\n            logging_steps=10,\n            save_steps=150,\n            save_total_limit=1,\n            warmup_steps=10,\n            weight_decay=0.01,\n            report_to=\"wandb\",\n            gradient_checkpointing=True,\n            optim=\"paged_adamw_8bit\"\n        )\n        \n        trainer = Trainer(model=model, args=args, train_dataset=tokenized)\n        \n        print(\"\\nTraining...\")\n        trainer.train()\n        \n        print(f\"\\nSaving to {output_dir}...\")\n        model.save_pretrained(output_dir)\n        tokenizer.save_pretrained(output_dir)\n        \n        print(\"✓ Complete!\")\n        return True\n        \n    except Exception as e:\n        print(f\"✗ Error: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n        \n    finally:\n        wandb.finish()\n        if 'model' in locals():\n            del model\n        if 'trainer' in locals():\n            del trainer\n        gc.collect()\n        torch.cuda.empty_cache()\n\n# -------------------------------------------------\n# Train All 3 Models\n# -------------------------------------------------\n\nmodels = [\n    (\"microsoft/Phi-3-mini-4k-instruct\", \"/kaggle/working/models/phi3_lora\"),\n    (\"mistralai/Mistral-7B-Instruct-v0.3\", \"/kaggle/working/models/mistral_lora\"),\n    (\"Qwen/Qwen2.5-7B-Instruct\", \"/kaggle/working/models/qwen_lora\"),\n]\n\nprint(\"\\n🚀 Training on P100 (16GB)\")\nprint(\"Batch size: 4, Grad accumulation: 4 (effective batch = 16)\")\n\nresults = {}\nfor model_id, out_dir in models:\n    print(f\"\\n{'#'*60}\")\n    print(f\"Model: {model_id}\")\n    print(f\"{'#'*60}\")\n    \n    success = finetune_model_p100(model_id, out_dir)\n    results[model_id] = success\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SUMMARY\")\nprint(\"=\"*60)\nfor model, success in results.items():\n    print(f\"{'✓' if success else '✗'} {model}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T23:04:21.848118Z","iopub.execute_input":"2025-11-03T23:04:21.848821Z","execution_failed":"2025-11-04T10:59:06.765Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGPU CHECK\n============================================================\n✓ GPU: Tesla P100-PCIE-16GB\n✓ Memory: 15.89 GB\n✓ Perfect! P100 is ideal for this task.\n============================================================\n✓ Dataset: 300 examples\n\n🚀 Training on P100 (16GB)\nBatch size: 4, Grad accumulation: 4 (effective batch = 16)\n\n############################################################\nModel: microsoft/Phi-3-mini-4k-instruct\n############################################################\n\n============================================================\nTraining: microsoft/Phi-3-mini-4k-instruct\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251103_230422-vs6laz4j</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100/runs/vs6laz4j' target=\"_blank\">Phi-3-mini-4k-instruct-lora</a></strong> to <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100/runs/vs6laz4j' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100/runs/vs6laz4j</a>"},"metadata":{}},{"name":"stdout","text":"Loading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"840b8769f28849a4be25f7254eaa8066"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1a3800fbf46460080c8e30bb815137c"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c74011a658c408997de7e43ac50fa0c"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8abaee304a5423da26bab605fdbffcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eac80d12625a468986b038327c1c22cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8607edaa33d4a5ba3e7aad68ff4adfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95f4a14679724e79ba04f90d0585a0bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04de41a50f1a4aa4b1b8da78a8b69615"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2165c4f7d7074f90a7900875983c2e5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82e8e5bbf96941468d97e853a571a820"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7d2f7c4f8284944b1aa0f90ddd57f3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2357a0869dd419e9e2f48a6e06863e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"050f9f37edac431e9f1ab001bfa6b1dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8a9a2248d0840feab9a6fb61f6eb527"}},"metadata":{}},{"name":"stdout","text":"Applying LoRA...\ntrainable params: 1,572,864 || all params: 3,822,652,416 || trainable%: 0.0411\nTokenizing...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d6ccc47ff374e18b8970653beba4dc4"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 2:41:49, Epoch 15/16]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>6.303000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>4.726700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.865500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.541500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.482800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.408100</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.425600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.293400</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.305000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.244700</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.263700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.219600</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.218700</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.162400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.244700</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.118700</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.234500</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.135000</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.164700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.176800</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.144500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.190400</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.190400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.152600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.077800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.175900</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.194900</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.151600</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.128600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.150400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nSaving to /kaggle/working/models/phi3_lora...\n✓ Complete!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▃█▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▇███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>5.422841723879424e+16</td></tr><tr><td>train/epoch</td><td>15.8</td></tr><tr><td>train/global_step</td><td>300</td></tr><tr><td>train/grad_norm</td><td>0.28297</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1504</td></tr><tr><td>train_loss</td><td>1.5364</td></tr><tr><td>train_runtime</td><td>9743.7265</td></tr><tr><td>train_samples_per_second</td><td>0.493</td></tr><tr><td>train_steps_per_second</td><td>0.031</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">Phi-3-mini-4k-instruct-lora</strong> at: <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100/runs/vs6laz4j' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100/runs/vs6laz4j</a><br> View project at: <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251103_230422-vs6laz4j/logs</code>"},"metadata":{}},{"name":"stdout","text":"\n############################################################\nModel: mistralai/Mistral-7B-Instruct-v0.3\n############################################################\n\n============================================================\nTraining: mistralai/Mistral-7B-Instruct-v0.3\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251104_014804-4lg3n47a</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100/runs/4lg3n47a' target=\"_blank\">Mistral-7B-Instruct-v0.3-lora</a></strong> to <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100/runs/4lg3n47a' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100/runs/4lg3n47a</a>"},"metadata":{}},{"name":"stdout","text":"Loading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc93531ba6834280a4c84a7cdd701887"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d89708e17fc4a6d8fac59aa3a594d9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b1219068e79420297e05c0c04423a86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29bef9e9c4974e33a3a29727fe7136e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b1d6ba450943c59bd6fc1679e5cc6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3396045cce94827881cc5c03a20f30b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bb27f46743e4d70b939bf393916c4b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0343e0f3072548eab5b856b5f5333adf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fb8dd1420b84be2b719177a39c34cf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71c9ecfa2adb49bf92e13a1e0a75358f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ca6d54a92fd45fa940991792bddc953"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"773a13b2ad83452abc182dfe39efafa1"}},"metadata":{}},{"name":"stdout","text":"Applying LoRA...\ntrainable params: 6,815,744 || all params: 7,254,839,296 || trainable%: 0.0939\nTokenizing...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9658b3b01f71430fbd959c2bb778f9ff"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 4:29:02, Epoch 15/16]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.748400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.524900</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.224400</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.117300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.074500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.098200</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.067800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.977100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.924300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.892000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.843900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.826000</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.752400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.712400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.695300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.574300</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.629200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.511700</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.529600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.451000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.444900</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.404600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.391200</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.327400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.296800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.296100</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.284300</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.252800</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.238700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.234200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nSaving to /kaggle/working/models/mistral_lora...\n✓ Complete!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▄▂▂▁▁▁▂▂▂▃▃▃▃▄▅▄▅▅▆▆▅██▇▇▆▇▆█▆</td></tr><tr><td>train/learning_rate</td><td>▇███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.0368536442568704e+17</td></tr><tr><td>train/epoch</td><td>15.8</td></tr><tr><td>train/global_step</td><td>300</td></tr><tr><td>train/grad_norm</td><td>2.52564</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2342</td></tr><tr><td>train_loss</td><td>0.77819</td></tr><tr><td>train_runtime</td><td>16196.9734</td></tr><tr><td>train_samples_per_second</td><td>0.296</td></tr><tr><td>train_steps_per_second</td><td>0.019</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">Mistral-7B-Instruct-v0.3-lora</strong> at: <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100/runs/4lg3n47a' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100/runs/4lg3n47a</a><br> View project at: <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251104_014804-4lg3n47a/logs</code>"},"metadata":{}},{"name":"stdout","text":"\n############################################################\nModel: Qwen/Qwen2.5-7B-Instruct\n############################################################\n\n============================================================\nTraining: Qwen/Qwen2.5-7B-Instruct\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251104_062049-4zmu4947</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100/runs/4zmu4947' target=\"_blank\">Qwen2.5-7B-Instruct-lora</a></strong> to <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100/runs/4zmu4947' target=\"_blank\">https://wandb.ai/mishralaavanya-svkm-s-narsee-monjee-institute-of-managem/llm-jury-bloom-p100/runs/4zmu4947</a>"},"metadata":{}},{"name":"stdout","text":"Loading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e84d37119404ef1a6c62e703ecdb64a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eccf62dd772419caedc7b910be1a6ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46bf6c2ecf754702bfc799e7556f5b0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dc5ae6bf8bf47bb9e8e5aa39a8f2684"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cbc88c44bb04590a978cb15693ff10e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9169a7c3aea546ec9f3b98406df78257"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c8b4d5ef9094c22bf591b92ac69e203"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7be39b4ea22d4e2cbd472a7518040400"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7e34251ecb0428bad8dc831e66ffcf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5990472fa3a14d3d98253f653b9bac6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4d775691c0a49138bcf12cabf43200e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f64ff019b490477aa85db046cb3ae38f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc03347081034dc19c2c94c242b05d0b"}},"metadata":{}},{"name":"stdout","text":"Applying LoRA...\ntrainable params: 5,046,272 || all params: 7,620,662,784 || trainable%: 0.0662\nTokenizing...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"598c4954540b493cab6b06d4300b5124"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='266' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [266/300 3:53:53 < 30:07, 0.02 it/s, Epoch 13.96/16]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>7.077900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.983200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.535100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.364500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.303600</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.316900</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.286000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.190700</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.158600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.151300</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.158100</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.173900</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.137700</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.117600</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.163800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.048500</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.151200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.057600</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.094000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.063400</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.054000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.073100</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.086200</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.011400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.960600</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.030700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import torch, gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T15:49:00.045728Z","iopub.execute_input":"2025-11-04T15:49:00.046408Z","iopub.status.idle":"2025-11-04T15:49:00.570555Z","shell.execute_reply.started":"2025-11-04T15:49:00.046383Z","shell.execute_reply":"2025-11-04T15:49:00.569813Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"# -------------------------------------------------\n# PART 1: Diagnostic - What's Using Your GPU?\n# -------------------------------------------------\n\nimport torch\nimport gc\nimport subprocess\nimport os\n\ndef check_gpu_processes():\n    \"\"\"See what's actually running on your GPU\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"GPU PROCESS CHECK\")\n    print(\"=\"*60)\n    \n    try:\n        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n        print(result.stdout)\n    except:\n        print(\"nvidia-smi not available\")\n    \n    if torch.cuda.is_available():\n        print(\"\\nPyTorch GPU Memory:\")\n        for i in range(torch.cuda.device_count()):\n            allocated = torch.cuda.memory_allocated(i) / 1024**3\n            reserved = torch.cuda.memory_reserved(i) / 1024**3\n            total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n            free = total - allocated\n            print(f\"GPU {i}:\")\n            print(f\"  Total: {total:.2f} GB\")\n            print(f\"  Allocated: {allocated:.2f} GB\")\n            print(f\"  Reserved: {reserved:.2f} GB\")\n            print(f\"  Free: {free:.2f} GB\")\n\ndef list_gpu_tensors():\n    \"\"\"Find all tensors currently on GPU\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"TENSORS ON GPU\")\n    print(\"=\"*60)\n    \n    import sys\n    tensor_count = 0\n    total_size = 0\n    \n    for obj in gc.get_objects():\n        try:\n            if torch.is_tensor(obj):\n                if obj.is_cuda:\n                    tensor_count += 1\n                    total_size += obj.element_size() * obj.nelement()\n                    if tensor_count <= 10:  # Show first 10\n                        print(f\"  Tensor: {obj.size()}, {obj.dtype}, {obj.element_size() * obj.nelement() / 1024**2:.2f} MB\")\n        except:\n            pass\n    \n    print(f\"\\nTotal GPU tensors: {tensor_count}\")\n    print(f\"Total size: {total_size / 1024**3:.2f} GB\")\n\n# Run diagnostics\ncheck_gpu_processes()\nlist_gpu_tensors()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T22:51:21.904618Z","iopub.execute_input":"2025-11-03T22:51:21.904959Z","iopub.status.idle":"2025-11-03T22:51:22.568591Z","shell.execute_reply.started":"2025-11-03T22:51:21.904934Z","shell.execute_reply":"2025-11-03T22:51:22.567801Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nGPU PROCESS CHECK\n============================================================\nMon Nov  3 22:51:21 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   67C    P0             30W /   70W |   11591MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   60C    P0             29W /   70W |     103MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\n\nPyTorch GPU Memory:\nGPU 0:\n  Total: 14.74 GB\n  Allocated: 7.93 GB\n  Reserved: 10.88 GB\n  Free: 6.81 GB\nGPU 1:\n  Total: 14.74 GB\n  Allocated: 0.00 GB\n  Reserved: 0.00 GB\n  Free: 14.74 GB\n\n============================================================\nTENSORS ON GPU\n============================================================\n  Tensor: torch.Size([32768, 4096]), torch.bfloat16, 256.00 MB\n  Tensor: torch.Size([4096]), torch.bfloat16, 0.01 MB\n  Tensor: torch.Size([29360128, 1]), torch.uint8, 28.00 MB\n  Tensor: torch.Size([29360128, 1]), torch.uint8, 28.00 MB\n  Tensor: torch.Size([29360128, 1]), torch.uint8, 28.00 MB\n  Tensor: torch.Size([4096]), torch.bfloat16, 0.01 MB\n  Tensor: torch.Size([2097152, 1]), torch.uint8, 2.00 MB\n  Tensor: torch.Size([8388608, 1]), torch.uint8, 8.00 MB\n  Tensor: torch.Size([8388608, 1]), torch.uint8, 8.00 MB\n  Tensor: torch.Size([2097152, 1]), torch.uint8, 2.00 MB\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/__init__.py:1113: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n  return isinstance(obj, torch.Tensor)\n","output_type":"stream"},{"name":"stdout","text":"\nTotal GPU tensors: 636\nTotal size: 8.02 GB\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# This will restart the kernel\nimport os\nos._exit(0)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-03T22:55:19.354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------\n# FORCE USE OF GPU 1 (the clean one)\n# -------------------------------------------------\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Use only GPU 1\n\n# Verify it worked\nimport torch\nprint(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\nprint(f\"Currently allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n\n# Now proceed with training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T22:45:32.549219Z","iopub.execute_input":"2025-11-03T22:45:32.549544Z","iopub.status.idle":"2025-11-03T22:45:32.557708Z","shell.execute_reply.started":"2025-11-03T22:45:32.549518Z","shell.execute_reply":"2025-11-03T22:45:32.556849Z"}},"outputs":[{"name":"stdout","text":"Using GPU: Tesla T4\nAvailable memory: 14.74 GB\nCurrently allocated: 7.48 GB\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install -q transformers peft bitsandbytes wandb sentencepiece","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T11:18:53.967818Z","iopub.execute_input":"2025-11-04T11:18:53.968603Z","iopub.status.idle":"2025-11-04T11:20:12.048528Z","shell.execute_reply.started":"2025-11-04T11:18:53.968577Z","shell.execute_reply":"2025-11-04T11:20:12.047748Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T11:22:17.077720Z","iopub.execute_input":"2025-11-04T11:22:17.078287Z","iopub.status.idle":"2025-11-04T11:22:20.386432Z","shell.execute_reply.started":"2025-11-04T11:22:17.078261Z","shell.execute_reply":"2025-11-04T11:22:20.385669Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.48.2)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Loading Adapters","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ndef load_lora(base_id, lora_path):\n    print(f\"Loading {lora_path}...\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_id, load_in_4bit=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n    )\n    model = PeftModel.from_pretrained(base, lora_path)\n    tokenizer = AutoTokenizer.from_pretrained(base_id)\n    tokenizer.pad_token = tokenizer.eos_token\n    return model, tokenizer\n\n# Load all\nphi, phi_tok = load_lora(\"microsoft/Phi-3-mini-4k-instruct\", \"models/phi3_lora\")\nmistral, mistral_tok = load_lora(\"mistralai/Mistral-7B-Instruct-v0.3\", \"models/mistral_lora\")\nqwen, qwen_tok = load_lora(\"Qwen/Qwen2.5-7B-Instruct\", \"models/qwen_lora\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T11:24:03.444417Z","iopub.execute_input":"2025-11-04T11:24:03.445048Z","iopub.status.idle":"2025-11-04T11:30:53.316813Z","shell.execute_reply.started":"2025-11-04T11:24:03.445019Z","shell.execute_reply":"2025-11-04T11:30:53.315987Z"}},"outputs":[{"name":"stderr","text":"2025-11-04 11:24:09.197795: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762255449.220475     146 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762255449.227700     146 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading models/phi3_lora...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0e3b5c20f7c4ddab9d9e7e95dd20375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cf6d269becd491ab9b0bb8dc1fada3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e02a6e1564341ffb38b85dd14366a4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f481ef882ffa4abf9852afc469d17a26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d78bc0a745d14fd894b3fa33716f2b8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d23bc24cdd9462bab3cb43be3f9d943"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecb7ab06630a41c696ffb0b09978f201"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4213df7a5b448e1a297489d59e1ddf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a760fc90f44361a6ea26a114a7a3ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83f0069fecdc4db79cb4ed8e43df0fed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7021dde5f9454cd69c17bc823bc4c21e"}},"metadata":{}},{"name":"stdout","text":"Loading models/mistral_lora...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbf60d7baea747458b8d2e71d698b7db"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd10e462559544d1809813fbb0c02bda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10a9a8b14e854aa582208b919ecb3e8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"523ccbbc3bcf4d29bff4245b9e09875b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8fe37a450634b1889a4c7580e1c179a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73013408ad3a4afe9a315d9392140c1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0db00169bb794f94966f35e7b69cc74e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f9b484005954b2182500749656007a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cffa7fc22db405582c5e2b14069422e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12b6ceced3984154b2cca02dc0412d57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82e1f5d45eae4fd5a1885aed6ebe3289"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5e4709644804cd0bb8644c72a20af63"}},"metadata":{}},{"name":"stdout","text":"Loading models/qwen_lora...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2761a842b81c4a929a5d5aa9425fd97c"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"278853b1bfa941488a0089b4b0591737"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7b9aec3727a4ee6b09f721a582daf86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9611c98fcdb4e5b964baf50bc4c2995"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45a2b6e6a93d4d298baab43e2b5c0c91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c04012a0b0a4428ebb28feb2b49c1e1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44ff38c3fd9e40859189ce3cf42b9c03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cd51696bc964234a4e83a5e29f2f5bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dd4ca7aa76a442bb3d0297260e2caa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2fcc446796348d9995e5c1ea3f66bbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30102739d9c5479f87bca78217fa882e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc5f1e3c0304bbea2a77eea7558be37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8af4630f7db4a15b7257f995508e115"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import os; os._exit(00)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T11:23:54.997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\ntest_df = pd.read_pickle(\"/kaggle/working/checkpoint_splits_20251103_211458.pkl\")['test']\n\nmodels = [\n    (phi, phi_tok, \"Phi\"),\n    (mistral, mistral_tok, \"Mistral\"),\n    (qwen, qwen_tok, \"Qwen\")\n]\n\ndef jury_score(row):\n    scores = []\n    for model, tok, name in models:  # ← Use global `models`\n        prompt = f\"\"\"You are an expert at the {row['bloom_level']} level.\nPrompt: {row['prompt']}\nResponse: {row['response']}\nScore 1–5. JSON: {{\"score\": int, \"rationale\": str}}\"\"\"\n        inputs = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(\"cuda\")\n        with torch.no_grad():\n            out = model.generate(**inputs, max_new_tokens=100, temperature=0.3)\n        resp = tok.decode(out[0], skip_special_tokens=True)\n        try:\n            import re, json\n            j = re.search(r'\\{.*\\}', resp).group()\n            data = json.loads(j)\n            scores.append(int(data.get('score', 0)))\n        except:\n            scores.append(0)\n    return round(sum(scores)/len(scores)) if scores else 0\n\njury_scores = []\nfor _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Jury Evaluation\"):\n    jury = jury_score(row)\n    jury_scores.append(jury)\n\n# Compute final metric\nrho = test_df['essay_score'].corr(pd.Series(jury_scores), method='spearman')\nprint(f\"FINAL JURY Spearman ρ: {rho:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T11:41:25.698882Z","iopub.execute_input":"2025-11-04T11:41:25.699178Z","iopub.status.idle":"2025-11-04T11:53:02.575295Z","shell.execute_reply.started":"2025-11-04T11:41:25.699149Z","shell.execute_reply":"2025-11-04T11:53:02.574606Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Jury Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:   1%|          | 1/100 [00:01<03:13,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:   2%|▏         | 2/100 [00:14<13:14,  8.11s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:   3%|▎         | 3/100 [00:16<08:54,  5.51s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:   4%|▍         | 4/100 [00:18<06:29,  4.06s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:   5%|▌         | 5/100 [00:29<10:24,  6.57s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:   6%|▌         | 6/100 [00:32<08:33,  5.47s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:   7%|▋         | 7/100 [00:44<11:19,  7.30s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:   8%|▊         | 8/100 [00:55<13:15,  8.65s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:   9%|▉         | 9/100 [01:06<14:15,  9.40s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  10%|█         | 10/100 [01:10<11:22,  7.59s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  11%|█         | 11/100 [01:22<13:13,  8.91s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  12%|█▏        | 12/100 [01:36<15:43, 10.72s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  13%|█▎        | 13/100 [01:47<15:38, 10.78s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  14%|█▍        | 14/100 [02:02<17:17, 12.06s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  15%|█▌        | 15/100 [02:13<16:33, 11.69s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  16%|█▌        | 16/100 [02:19<13:47,  9.85s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  17%|█▋        | 17/100 [02:31<14:32, 10.52s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  18%|█▊        | 18/100 [02:42<14:41, 10.76s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  19%|█▉        | 19/100 [02:45<11:09,  8.27s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  20%|██        | 20/100 [02:47<08:44,  6.55s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  21%|██        | 21/100 [02:54<08:54,  6.76s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  22%|██▏       | 22/100 [02:57<07:05,  5.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  23%|██▎       | 23/100 [03:02<06:52,  5.36s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  24%|██▍       | 24/100 [03:10<07:47,  6.15s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  25%|██▌       | 25/100 [03:13<06:20,  5.08s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  26%|██▌       | 26/100 [03:15<05:06,  4.14s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  27%|██▋       | 27/100 [03:22<06:05,  5.01s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  28%|██▊       | 28/100 [03:24<05:04,  4.23s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  29%|██▉       | 29/100 [03:27<04:24,  3.73s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  30%|███       | 30/100 [03:29<03:45,  3.22s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  31%|███       | 31/100 [03:40<06:29,  5.65s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  32%|███▏      | 32/100 [03:44<05:57,  5.25s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  33%|███▎      | 33/100 [03:47<04:55,  4.41s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  34%|███▍      | 34/100 [03:52<05:04,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  35%|███▌      | 35/100 [04:03<07:09,  6.61s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  36%|███▌      | 36/100 [04:14<08:30,  7.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  37%|███▋      | 37/100 [04:25<09:19,  8.88s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  38%|███▊      | 38/100 [04:28<07:13,  6.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  39%|███▉      | 39/100 [04:31<05:56,  5.85s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  40%|████      | 40/100 [04:34<05:06,  5.11s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  41%|████      | 41/100 [04:46<06:50,  6.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  42%|████▏     | 42/100 [04:56<07:50,  8.11s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  43%|████▎     | 43/100 [05:00<06:18,  6.63s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  44%|████▍     | 44/100 [05:11<07:29,  8.03s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  45%|████▌     | 45/100 [05:22<08:07,  8.87s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  46%|████▌     | 46/100 [05:25<06:22,  7.08s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  47%|████▋     | 47/100 [05:28<05:12,  5.90s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  48%|████▊     | 48/100 [05:33<05:01,  5.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  49%|████▉     | 49/100 [05:46<06:37,  7.79s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  50%|█████     | 50/100 [05:56<07:10,  8.61s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  51%|█████     | 51/100 [06:10<08:22, 10.26s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  52%|█████▏    | 52/100 [06:14<06:30,  8.13s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  53%|█████▎    | 53/100 [06:25<07:12,  9.20s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  54%|█████▍    | 54/100 [06:28<05:29,  7.17s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  55%|█████▌    | 55/100 [06:39<06:16,  8.36s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  56%|█████▌    | 56/100 [06:50<06:46,  9.23s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  57%|█████▋    | 57/100 [07:01<07:00,  9.78s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  58%|█████▊    | 58/100 [07:11<06:54,  9.87s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  59%|█████▉    | 59/100 [07:22<07:00, 10.26s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  60%|██████    | 60/100 [07:34<07:00, 10.52s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  61%|██████    | 61/100 [07:37<05:24,  8.33s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  62%|██████▏   | 62/100 [07:39<04:05,  6.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  63%|██████▎   | 63/100 [07:43<03:33,  5.77s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  64%|██████▍   | 64/100 [07:48<03:21,  5.59s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  65%|██████▌   | 65/100 [07:54<03:16,  5.60s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  66%|██████▌   | 66/100 [07:58<02:54,  5.12s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  67%|██████▋   | 67/100 [08:00<02:22,  4.32s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  68%|██████▊   | 68/100 [08:05<02:19,  4.36s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  69%|██████▉   | 69/100 [08:08<02:03,  3.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  70%|███████   | 70/100 [08:12<02:03,  4.13s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  71%|███████   | 71/100 [08:23<03:00,  6.23s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  72%|███████▏  | 72/100 [08:34<03:35,  7.69s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  73%|███████▎  | 73/100 [08:46<03:56,  8.75s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  74%|███████▍  | 74/100 [08:48<02:58,  6.85s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  75%|███████▌  | 75/100 [08:51<02:19,  5.58s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  76%|███████▌  | 76/100 [08:54<01:56,  4.86s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  77%|███████▋  | 77/100 [09:08<02:55,  7.64s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  78%|███████▊  | 78/100 [09:10<02:13,  6.08s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  79%|███████▉  | 79/100 [09:22<02:39,  7.58s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  80%|████████  | 80/100 [09:25<02:09,  6.49s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  81%|████████  | 81/100 [09:37<02:29,  7.88s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  82%|████████▏ | 82/100 [09:48<02:40,  8.89s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  83%|████████▎ | 83/100 [09:58<02:39,  9.41s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  84%|████████▍ | 84/100 [10:01<01:57,  7.35s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  85%|████████▌ | 85/100 [10:12<02:07,  8.50s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  86%|████████▌ | 86/100 [10:27<02:24, 10.29s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  87%|████████▋ | 87/100 [10:30<01:46,  8.20s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  88%|████████▊ | 88/100 [10:33<01:18,  6.50s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  89%|████████▉ | 89/100 [10:37<01:03,  5.75s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  90%|█████████ | 90/100 [10:39<00:47,  4.79s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  91%|█████████ | 91/100 [10:45<00:45,  5.08s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  92%|█████████▏| 92/100 [10:49<00:39,  4.91s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  93%|█████████▎| 93/100 [10:53<00:32,  4.63s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  94%|█████████▍| 94/100 [11:05<00:40,  6.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  95%|█████████▌| 95/100 [11:08<00:27,  5.53s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  96%|█████████▌| 96/100 [11:18<00:28,  7.02s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  97%|█████████▋| 97/100 [11:20<00:16,  5.47s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  98%|█████████▊| 98/100 [11:22<00:08,  4.40s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation:  99%|█████████▉| 99/100 [11:25<00:04,  4.04s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nJury Evaluation: 100%|██████████| 100/100 [11:36<00:00,  6.97s/it]","output_type":"stream"},{"name":"stdout","text":"FINAL JURY Spearman ρ: nan\n","output_type":"stream"},{"name":"stderr","text":"\n/usr/local/lib/python3.11/dist-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n  return spearmanr(a, b)[0]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import re, json\n\n# Pick first row\nrow = test_df.iloc[0]\nprint(\"PROMPT:\")\nprint(row['prompt'])\nprint(\"\\nRESPONSE:\")\nprint(row['response'][:500] + \"...\" if len(row['response']) > 500 else row['response'])\nprint(f\"\\nTRUE SCORE: {row['essay_score']}\")\nprint(f\"BLOOM LEVEL: {row['bloom_level']}\")\n\nmodel, tok = phi, phi_tok  # or mistral, qwen\n\nprompt = f\"\"\"You are an expert at the {row['bloom_level']} level of Bloom's Taxonomy.\nPrompt: {row['prompt']}\nResponse: {row['response']}\nScore this response from 1 to 5 and provide a brief rationale.\nRespond in JSON: {{\"score\": int, \"rationale\": str}}\"\"\"\n\ninputs = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(\"cuda\")\n\nwith torch.no_grad():\n    out = model.generate(\n        **inputs,\n        max_new_tokens=100,\n        temperature=0.1,\n        do_sample=False\n    )\n\nraw_output = tok.decode(out[0], skip_special_tokens=True)\nprint(\"\\nRAW MODEL OUTPUT:\")\nprint(raw_output)\n\ndef extract_score(text):\n    try:\n        j = re.search(r'\\{.*\\}', text, re.DOTALL).group()\n        data = json.loads(j)\n        return int(data.get('score', 0)), data.get('rationale', '')\n    except:\n        return 0, \"FAILED\"\n\nscore, rationale = extract_score(raw_output)\nprint(f\"\\nEXTRACTED SCORE: {score}\")\nprint(f\"RATIONALE: {rationale}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T11:55:55.003539Z","iopub.execute_input":"2025-11-04T11:55:55.004203Z","iopub.status.idle":"2025-11-04T11:55:55.463784Z","shell.execute_reply.started":"2025-11-04T11:55:55.004174Z","shell.execute_reply":"2025-11-04T11:55:55.463132Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"PROMPT:\nWrite a story about a time when you, or someone you know, was patient.\n\nRESPONSE:\nWhat exactly is patience? Patience means to wait calmly for something. One of the times that I had to be patient was when I wanted my ears pierced. I tryed everything for four years, begging, pleading, even cleaning, but it still didn't work. My mom says I have to be @NUM1 before I can get my ears pierced. So instead of annoying her, I sat back and stayed patient. By doing that, I got my mom to change the age to twelve. So all I have to do is wait until soccer is finished and find a place and sh...\n\nTRUE SCORE: 20\nBLOOM LEVEL: Create\n\nRAW MODEL OUTPUT:\nYou are an expert at the Create level of Bloom's Taxonomy.\nPrompt: Write a story about a time when you, or someone you know, was patient.\nResponse: What exactly is patience? Patience means to wait calmly for something. One of the times that I had to be patient was when I wanted my ears pierced. I tryed everything for four years, begging, pleading, even cleaning, but it still didn't work. My mom says I have to be @NUM1 before I can get my ears pierced. So instead of annoying her, I sat back and stayed patient. By doing that, I got my mom to change the age to twelve. So all I have to do is wait until soccer is finished and find a place and she'll take me. Patience really helps in many different ways and I encourage you to try being patient when you want something, because it will all fall into place.\nScore this response from 1 to 5 and provide a brief rationale.\nRespond in JSON: {\"score\": int, \"rationale\": str}\n\nEXTRACTED SCORE: 0\nRATIONALE: FAILED\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"row = test_df.iloc[0]\nmodel, tok = phi, phi_tok  # Try Phi first\n\nmessages = [\n    {\"role\": \"system\", \"content\": f\"You are an expert evaluator at the {row['bloom_level']} level of Bloom's Taxonomy.\"},\n    {\"role\": \"user\", \"content\": f\"Prompt: {row['prompt']}\\n\\nStudent Response: {row['response']}\\n\\nScore this response from 1 to 5 and provide a brief rationale. Respond in JSON only: {{\\\"score\\\": int, \\\"rationale\\\": str}}\"}\n]\n\n# Apply chat template\ninputs = tok.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n\n# Generate\nwith torch.no_grad():\n    out = model.generate(\n        inputs,\n        max_new_tokens=100,\n        temperature=0.1,\n        do_sample=False\n    )\n\n# Full raw output\nraw = tok.decode(out[0], skip_special_tokens=True)\nprint(\"=== RAW OUTPUT ===\")\nprint(raw)\nprint(\"=== END RAW ===\")\n\n# Try to extract\ntry:\n    json_part = raw.split(\"<|assistant|>\")[-1] if \"<|assistant|>\" in raw else raw\n    print(\"\\nJSON PART:\")\n    print(json_part)\n    j = re.search(r'\\{.*\\}', json_part, re.DOTALL).group()\n    data = json.loads(j)\n    print(f\"\\nPARSED: score={data.get('score')}, rationale={data.get('rationale')}\")\nexcept Exception as e:\n    print(f\"\\nPARSE FAILED: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T11:58:32.903807Z","iopub.execute_input":"2025-11-04T11:58:32.904076Z","iopub.status.idle":"2025-11-04T11:58:37.504918Z","shell.execute_reply.started":"2025-11-04T11:58:32.904057Z","shell.execute_reply":"2025-11-04T11:58:37.504176Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"=== RAW OUTPUT ===\nYou are an expert evaluator at the Create level of Bloom's Taxonomy. Prompt: Write a story about a time when you, or someone you know, was patient.\n\nStudent Response: What exactly is patience? Patience means to wait calmly for something. One of the times that I had to be patient was when I wanted my ears pierced. I tryed everything for four years, begging, pleading, even cleaning, but it still didn't work. My mom says I have to be @NUM1 before I can get my ears pierced. So instead of annoying her, I sat back and stayed patient. By doing that, I got my mom to change the age to twelve. So all I have to do is wait until soccer is finished and find a place and she'll take me. Patience really helps in many different ways and I encourage you to try being patient when you want something, because it will all fall into place.\n\nScore this response from 1 to 5 and provide a brief rationale. Respond in JSON only: {\"score\": int, \"rationale\": str} {\"score\": 5, \"rationale\": \"The response demonstrates a clear understanding of the concept of patience and provides a personal example of how patience was exercised. The student also explains the importance of patience and encourages others to try it. The response is well-written and effectively communicates the message.\"}\n=== END RAW ===\n\nJSON PART:\nYou are an expert evaluator at the Create level of Bloom's Taxonomy. Prompt: Write a story about a time when you, or someone you know, was patient.\n\nStudent Response: What exactly is patience? Patience means to wait calmly for something. One of the times that I had to be patient was when I wanted my ears pierced. I tryed everything for four years, begging, pleading, even cleaning, but it still didn't work. My mom says I have to be @NUM1 before I can get my ears pierced. So instead of annoying her, I sat back and stayed patient. By doing that, I got my mom to change the age to twelve. So all I have to do is wait until soccer is finished and find a place and she'll take me. Patience really helps in many different ways and I encourage you to try being patient when you want something, because it will all fall into place.\n\nScore this response from 1 to 5 and provide a brief rationale. Respond in JSON only: {\"score\": int, \"rationale\": str} {\"score\": 5, \"rationale\": \"The response demonstrates a clear understanding of the concept of patience and provides a personal example of how patience was exercised. The student also explains the importance of patience and encourages others to try it. The response is well-written and effectively communicates the message.\"}\n\nPARSE FAILED: Expecting value: line 1 column 11 (char 10)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/asap-aes/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\nprint(df.groupby('essay_set')['domain1_score'].agg(['min', 'max']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T13:43:24.312642Z","iopub.execute_input":"2025-11-04T13:43:24.312974Z","iopub.status.idle":"2025-11-04T13:43:24.693638Z","shell.execute_reply.started":"2025-11-04T13:43:24.312949Z","shell.execute_reply":"2025-11-04T13:43:24.692955Z"}},"outputs":[{"name":"stdout","text":"           min  max\nessay_set          \n1            2   12\n2            1    6\n3            0    3\n4            0    3\n5            0    4\n6            0    4\n7            2   24\n8           10   60\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# FULL FINAL CODE — RUN THIS\nimport pandas as pd\nimport torch\nimport re\nimport json\nfrom tqdm import tqdm\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load test data\ntest_df = pd.read_pickle(\"/kaggle/working/checkpoint_splits_20251103_211458.pkl\")['test']\n\n# Load 3 fine-tuned models\ndef load_lora(base_id, lora_path):\n    print(f\"Loading {lora_path}...\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_id, load_in_4bit=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n    )\n    model = PeftModel.from_pretrained(base, lora_path)\n    tokenizer = AutoTokenizer.from_pretrained(base_id)\n    tokenizer.pad_token = tokenizer.eos_token\n    return model, tokenizer\n\n# phi, phi_tok = load_lora(\"microsoft/Phi-3-mini-4k-instruct\", \"models/phi3_lora/checkpoint_300\")\n# mistral, mistral_tok = load_lora(\"mistralai/Mistral-7B-Instruct-v0.3\", \"models/mistral_lora/checkpoint_300\")\n# qwen, qwen_tok = load_lora(\"Qwen/Qwen2.5-7B-Instruct\", \"models/qwen_lora/checkpoint_300\")\n\nmodels = [(phi, phi_tok, \"Phi\"), (mistral, mistral_tok, \"Mistral\"), (qwen, qwen_tok, \"Qwen\")]\n\n# Extract score from model output\ndef extract_score(text):\n    try:\n        j = re.findall(r'\\{.*\\}', text)[-1]\n        data = json.loads(j)\n        return int(data.get(\"score\", 0))\n    except:\n        return 0\n\n# Jury scoring\ndef jury_score(row):\n    scores = []\n    for model, tok, name in models:\n        prompt = f\"\"\"<|system|>You are an expert evaluator at the {row['bloom_level']} level of Bloom's Taxonomy.<|end|>\n<|user|>Prompt: {row['prompt']}\\n\\nStudent Response: {row['response']}\\n\\nScore this response from 1 to 5. Respond in JSON only: {{\"score\": int, \"rationale\": str}}<|end|>\n<|assistant|>\"\"\"\n        inputs = tok(prompt, return_tensors=\"pt\").to(\"cuda\")\n        with torch.no_grad():\n            out = model.generate(**inputs, max_new_tokens=80, temperature=0.0, do_sample=False)\n        resp = tok.decode(out[0], skip_special_tokens=True)\n        score = extract_score(resp)\n        scores.append(score)\n    return round(sum(scores)/len(scores)) if scores else 0\n\n# Run jury\njury_scores = []\nfor _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Jury Evaluation\"):\n    jury_scores.append(jury_score(row))\n\n# Normalize 1–5 → ASAP scale per essay_set\nscale_map = {\n    1: (2, 12), 2: (1, 6), 3: (0, 3), 4: (0, 3),\n    5: (0, 4), 6: (0, 4), 7: (2, 24), 8: (10, 60)\n}\n\ndef normalize(jury_1to5, essay_set):\n    min_s, max_s = scale_map[essay_set]\n    return round(min_s + (jury_1to5 - 1) * (max_s - min_s) / 4)\n\njury_normalized = [\n    normalize(j, r['essay_set']) for j, (_, r) in zip(jury_scores, test_df.iterrows())\n]\n\n# Compute final Spearman ρ\nrho = test_df['essay_score'].corr(pd.Series(jury_normalized), method='spearman')\nprint(f\"\\nFINAL JURY Spearman ρ: {rho:.3f}\")\n\n# Save results\nresults = pd.DataFrame({\n    'essay_set': test_df['essay_set'],\n    'true_score': test_df['essay_score'],\n    'jury_1to5': jury_scores,\n    'jury_normalized': jury_normalized\n})\nresults.to_csv(\"/kaggle/working/jury_final_normalized.csv\", index=False)\nprint(\"Results saved to jury_final_normalized.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T13:48:27.800565Z","iopub.execute_input":"2025-11-04T13:48:27.801246Z","iopub.status.idle":"2025-11-04T14:11:14.176913Z","shell.execute_reply.started":"2025-11-04T13:48:27.801221Z","shell.execute_reply":"2025-11-04T14:11:14.176126Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Jury Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   1%|          | 1/100 [00:14<23:51, 14.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   2%|▏         | 2/100 [00:25<19:53, 12.18s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   3%|▎         | 3/100 [00:39<21:31, 13.31s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   4%|▍         | 4/100 [00:48<18:42, 11.70s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   5%|▌         | 5/100 [01:03<19:53, 12.56s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   6%|▌         | 6/100 [01:18<21:23, 13.65s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   7%|▋         | 7/100 [01:32<21:25, 13.82s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   8%|▊         | 8/100 [01:47<21:36, 14.09s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   9%|▉         | 9/100 [02:01<21:24, 14.12s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  10%|█         | 10/100 [02:15<21:09, 14.10s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  11%|█         | 11/100 [02:26<19:20, 13.03s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  12%|█▏        | 12/100 [02:39<19:06, 13.03s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  13%|█▎        | 13/100 [02:48<17:14, 11.89s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  14%|█▍        | 14/100 [03:03<18:16, 12.75s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  15%|█▌        | 15/100 [03:17<18:31, 13.08s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  16%|█▌        | 16/100 [03:32<19:02, 13.60s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  17%|█▋        | 17/100 [03:47<19:24, 14.03s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  18%|█▊        | 18/100 [04:00<18:50, 13.78s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  19%|█▉        | 19/100 [04:15<19:04, 14.13s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  20%|██        | 20/100 [04:30<19:23, 14.54s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  21%|██        | 21/100 [04:40<17:05, 12.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  22%|██▏       | 22/100 [04:54<17:34, 13.53s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  23%|██▎       | 23/100 [05:12<18:53, 14.72s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  24%|██▍       | 24/100 [05:29<19:26, 15.35s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  25%|██▌       | 25/100 [05:39<17:23, 13.91s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  26%|██▌       | 26/100 [05:54<17:23, 14.10s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  27%|██▋       | 27/100 [06:09<17:23, 14.30s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  28%|██▊       | 28/100 [06:23<17:21, 14.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  29%|██▉       | 29/100 [06:39<17:23, 14.69s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  30%|███       | 30/100 [06:53<17:09, 14.71s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  31%|███       | 31/100 [07:03<15:07, 13.16s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  32%|███▏      | 32/100 [07:15<14:30, 12.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  33%|███▎      | 33/100 [07:30<15:02, 13.47s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  34%|███▍      | 34/100 [07:42<14:21, 13.05s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  35%|███▌      | 35/100 [07:56<14:32, 13.42s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  36%|███▌      | 36/100 [08:06<13:01, 12.21s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  37%|███▋      | 37/100 [08:20<13:27, 12.81s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  38%|███▊      | 38/100 [08:35<14:00, 13.56s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  39%|███▉      | 39/100 [08:51<14:21, 14.13s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  40%|████      | 40/100 [09:02<13:12, 13.21s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  41%|████      | 41/100 [09:16<13:17, 13.51s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  42%|████▏     | 42/100 [09:30<13:14, 13.69s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  43%|████▎     | 43/100 [09:46<13:33, 14.27s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  44%|████▍     | 44/100 [09:55<11:58, 12.83s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  45%|████▌     | 45/100 [10:09<12:09, 13.26s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  46%|████▌     | 46/100 [10:25<12:33, 13.95s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  47%|████▋     | 47/100 [10:36<11:27, 12.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  48%|████▊     | 48/100 [10:54<12:34, 14.51s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  49%|████▉     | 49/100 [11:05<11:27, 13.49s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  50%|█████     | 50/100 [11:20<11:34, 13.89s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  51%|█████     | 51/100 [11:32<10:58, 13.45s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  52%|█████▏    | 52/100 [11:43<10:05, 12.62s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  53%|█████▎    | 53/100 [11:58<10:31, 13.44s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  54%|█████▍    | 54/100 [12:13<10:40, 13.92s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  55%|█████▌    | 55/100 [12:28<10:33, 14.08s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  56%|█████▌    | 56/100 [12:42<10:28, 14.29s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  57%|█████▋    | 57/100 [12:57<10:15, 14.31s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  58%|█████▊    | 58/100 [13:06<08:50, 12.62s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  59%|█████▉    | 59/100 [13:15<07:58, 11.67s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  60%|██████    | 60/100 [13:29<08:18, 12.47s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  61%|██████    | 61/100 [13:40<07:49, 12.05s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  62%|██████▏   | 62/100 [13:55<08:09, 12.89s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  63%|██████▎   | 63/100 [14:12<08:39, 14.05s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  64%|██████▍   | 64/100 [14:27<08:35, 14.33s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  65%|██████▌   | 65/100 [14:45<09:02, 15.49s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  66%|██████▌   | 66/100 [14:57<08:05, 14.29s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  67%|██████▋   | 67/100 [15:12<07:58, 14.51s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  68%|██████▊   | 68/100 [15:23<07:15, 13.61s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  69%|██████▉   | 69/100 [15:34<06:34, 12.71s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  70%|███████   | 70/100 [15:45<06:09, 12.32s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  71%|███████   | 71/100 [15:59<06:13, 12.87s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  72%|███████▏  | 72/100 [16:13<06:10, 13.25s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  73%|███████▎  | 73/100 [16:28<06:05, 13.55s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  74%|███████▍  | 74/100 [16:43<06:02, 13.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  75%|███████▌  | 75/100 [16:58<05:57, 14.30s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  76%|███████▌  | 76/100 [17:12<05:45, 14.40s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  77%|███████▋  | 77/100 [17:27<05:33, 14.50s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  78%|███████▊  | 78/100 [17:42<05:21, 14.62s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  79%|███████▉  | 79/100 [17:56<05:05, 14.54s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  80%|████████  | 80/100 [18:08<04:32, 13.64s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  81%|████████  | 81/100 [18:22<04:22, 13.83s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  82%|████████▏ | 82/100 [18:36<04:11, 13.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  83%|████████▎ | 83/100 [18:50<03:57, 13.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  84%|████████▍ | 84/100 [19:05<03:48, 14.27s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  85%|████████▌ | 85/100 [19:20<03:34, 14.31s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  86%|████████▌ | 86/100 [19:33<03:14, 13.87s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  87%|████████▋ | 87/100 [19:49<03:10, 14.62s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  88%|████████▊ | 88/100 [20:04<02:57, 14.76s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  89%|████████▉ | 89/100 [20:16<02:32, 13.89s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  90%|█████████ | 90/100 [20:32<02:24, 14.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  91%|█████████ | 91/100 [20:45<02:07, 14.12s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  92%|█████████▏| 92/100 [21:02<02:00, 15.04s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  93%|█████████▎| 93/100 [21:14<01:38, 14.10s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  94%|█████████▍| 94/100 [21:29<01:26, 14.34s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  95%|█████████▌| 95/100 [21:39<01:05, 13.07s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  96%|█████████▌| 96/100 [21:53<00:53, 13.43s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  97%|█████████▋| 97/100 [22:08<00:41, 13.73s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  98%|█████████▊| 98/100 [22:22<00:27, 13.90s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  99%|█████████▉| 99/100 [22:33<00:12, 12.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation: 100%|██████████| 100/100 [22:46<00:00, 13.66s/it]","output_type":"stream"},{"name":"stdout","text":"\nFINAL JURY Spearman ρ: -0.431\nResults saved to jury_final_normalized.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"def extract_score(text):\n    try:\n        # Find ALL JSON blocks\n        json_blocks = re.findall(r'\\{.*?\\}', text, re.DOTALL)\n        if not json_blocks:\n            return 0, \"No JSON\"\n        # Take the LAST one (model's actual answer)\n        j = json_blocks[-1]\n        data = json.loads(j)\n        return int(data.get('score', 0)), data.get('rationale', '')\n    except Exception as e:\n        return 0, f\"Parse error: {e}\"\n\ndef jury_score(row):\n    scores = []\n    for model, tok, name in models:\n        messages = [\n            {\"role\": \"system\", \"content\": f\"You are an expert evaluator at the {row['bloom_level']} level of Bloom's Taxonomy.\"},\n            {\"role\": \"user\", \"content\": f\"Prompt: {row['prompt']}\\n\\nStudent Response: {row['response']}\\n\\nScore this response from 1 to 5 and provide a brief rationale. Respond in JSON only: {{\\\"score\\\": int, \\\"rationale\\\": str}}\"}\n        ]\n        inputs = tok.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n\n        with torch.no_grad():\n            out = model.generate(\n                inputs,\n                max_new_tokens=100,\n                temperature=0.1,\n                do_sample=False\n            )\n        resp = tok.decode(out[0], skip_special_tokens=True)\n        \n        score, _ = extract_score(resp)\n        scores.append(score)\n    \n    return round(sum(scores)/len(scores)) if scores else 0\n\nrow = test_df.iloc[0]\nscore = jury_score(row)\nprint(f\"Jury score: {score}\")  # Should be 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:01:56.206863Z","iopub.execute_input":"2025-11-04T12:01:56.207439Z","iopub.status.idle":"2025-11-04T12:02:02.241611Z","shell.execute_reply.started":"2025-11-04T12:01:56.207416Z","shell.execute_reply":"2025-11-04T12:02:02.240881Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Jury score: 2\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def jury_score(row):\n    scores = []\n    for model, tok, name in models:\n        messages = [\n            {\"role\": \"system\", \"content\": f\"You are an expert evaluator at the {row['bloom_level']} level.\"},\n            {\"role\": \"user\", \"content\": f\"\"\"Prompt: {row['prompt']}\\n\\nResponse: {row['response']}\\n\\nScore 1–5. JSON only.\"\"\"}\n        ]\n        \n        # Use chat template + force JSON\n        inputs = tok.apply_chat_template(\n            messages,\n            return_tensors=\"pt\",\n            add_generation_prompt=True,\n            tokenize=True,\n            return_dict=True\n        ).to(\"cuda\")\n        \n        # Force JSON output\n        generate_kwargs = dict(\n            inputs,\n            max_new_tokens=100,\n            temperature=0.0,\n            do_sample=False,\n            response_format={\"type\": \"json_object\"}  # ← Critical\n        )\n        \n        with torch.no_grad():\n            out = model.generate(**generate_kwargs)\n        \n        resp = tok.decode(out[0], skip_special_tokens=True)\n        \n        # Extract JSON\n        try:\n            j = re.search(r'\\{.*\\}', resp, re.DOTALL).group()\n            data = json.loads(j)\n            score = int(data.get(\"score\", 0))\n        except:\n            score = 0\n        scores.append(score)\n    \n    return round(sum(scores)/len(scores)) if scores else 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T14:17:43.755552Z","iopub.execute_input":"2025-11-04T14:17:43.756222Z","iopub.status.idle":"2025-11-04T14:17:43.764601Z","shell.execute_reply.started":"2025-11-04T14:17:43.756189Z","shell.execute_reply":"2025-11-04T14:17:43.763846Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# 1. Imports (run once – they are already imported in your notebook)\n# ------------------------------------------------------------------\nimport pandas as pd\nimport torch\nimport re\nimport json\nfrom tqdm import tqdm\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# ------------------------------------------------------------------\n# 2. Load test data (first 2 rows only)\n# ------------------------------------------------------------------\ntest_df = pd.read_pickle(\"/kaggle/working/checkpoint_splits_20251103_211458.pkl\")['test']\ntest_df = test_df.head(2).reset_index(drop=True)      # <-- ONLY FIRST 2 ROWS\nprint(\"=== TEST ROWS ===\")\nprint(test_df[['essay_set', 'essay_score', 'bloom_level']].to_string(index=False))\nprint(\"\\n\")\n\n# ------------------------------------------------------------------\n# 3. Load the three fine-tuned adapters\n# ------------------------------------------------------------------\ndef load_lora(base_id, lora_path):\n    print(f\"Loading {lora_path} …\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_id, load_in_4bit=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n    )\n    model = PeftModel.from_pretrained(base, lora_path)\n    tokenizer = AutoTokenizer.from_pretrained(base_id)\n    tokenizer.pad_token = tokenizer.eos_token\n    return model, tokenizer\n\n# phi,   phi_tok   = load_lora(\"microsoft/Phi-3-mini-4k-instruct\",\n#                             \"models/phi3_lora/checkpoint_300\")\n# mistral, mistral_tok = load_lora(\"mistralai/Mistral-7B-Instruct-v0.3\",\n#                                  \"models/mistral_lora/checkpoint_300\")\n# qwen,   qwen_tok   = load_lora(\"Qwen/Qwen2.5-7B-Instruct\",\n#                                \"models/qwen_lora/checkpoint_300\")\n\nmodels = [(phi, phi_tok, \"Phi\"),\n          (mistral, mistral_tok, \"Mistral\"),\n          (qwen, qwen_tok, \"Qwen\")]\n\n# ------------------------------------------------------------------\n# 4. JSON extraction (robust)\n# ------------------------------------------------------------------\ndef extract_score(raw_text):\n    \"\"\"Return (score, raw_json_string) – score=0 on failure.\"\"\"\n    try:\n        # Grab the last {...} block\n        matches = re.findall(r'\\{.*\\}', raw_text, re.DOTALL)\n        if not matches:\n            return 0, \"\"\n        j = matches[-1]\n        data = json.loads(j)\n        return int(data.get(\"score\", 0)), j\n    except Exception as e:\n        return 0, f\"PARSE ERROR: {e}\"\n\n# ------------------------------------------------------------------\n# 5. Jury scoring (one row)\n# ------------------------------------------------------------------\ndef jury_score(row):\n    per_model = {}\n    for model, tok, name in models:\n        messages = [\n            {\"role\": \"system\",\n             \"content\": f\"You are an expert evaluator at the {row['bloom_level']} level of Bloom's Taxonomy.\"},\n            {\"role\": \"user\",\n             \"content\": f\"Prompt: {row['prompt']}\\n\\nStudent Response: {row['response']}\\n\\n\"\n                        f\"Score this response from 1 to 5 and provide a brief rationale. \"\n                        f\"Respond in JSON only: {{\\\"score\\\": int, \\\"rationale\\\": str}}\"}\n        ]\n\n        inputs = tok.apply_chat_template(\n            messages,\n            return_tensors=\"pt\",\n            add_generation_prompt=True,\n            tokenize=True,\n            return_dict=True\n        ).to(\"cuda\")\n\n        with torch.no_grad():\n            out = model.generate(\n                **inputs,\n                max_new_tokens=120,\n                temperature=0.0,\n                do_sample=False\n            )\n        resp = tok.decode(out[0], skip_special_tokens=True)\n\n        score, json_blob = extract_score(resp)\n        per_model[name] = {\"score\": score, \"json\": json_blob, \"raw\": resp[-400:]}  # last 400 chars\n\n    # Aggregate\n    scores = [v[\"score\"] for v in per_model.values()]\n    jury_1to5 = round(sum(scores) / len(scores)) if scores else 0\n    return jury_1to5, per_model\n\n# ------------------------------------------------------------------\n# 6. Run on first 2 rows\n# ------------------------------------------------------------------\nscale_map = {1: (2,12), 2:(1,6), 3:(0,3), 4:(0,3),\n             5:(0,4), 6:(0,4), 7:(2,24), 8:(10,60)}\n\ndef normalize(jury_1to5, essay_set):\n    mn, mx = scale_map.get(essay_set, (0,12))\n    return round(mn + (jury_1to5-1)*(mx-mn)/4)\n\nprint(\"\\n=== JURY TEST (first 2 rows) ===\")\nfor idx, row in test_df.iterrows():\n    jury_1to5, details = jury_score(row)\n    jury_norm = normalize(jury_1to5, row['essay_set'])\n\n    print(f\"\\n--- ROW {idx} (Set {row['essay_set']}) ---\")\n    print(f\"True score : {row['essay_score']}\")\n    print(f\"Jury 1-5   : {jury_1to5}\")\n    print(f\"Jury norm  : {jury_norm}\")\n\n    for name, info in details.items():\n        print(f\"\\n  [{name}] score = {info['score']}\")\n        print(f\"  JSON   : {info['json'][:200]}{'...' if len(info['json'])>200 else ''}\")\n        # Uncomment next line if you want the *full* raw output\n        print(f\"  RAW    : {info['raw']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T14:20:22.614337Z","iopub.execute_input":"2025-11-04T14:20:22.614632Z","iopub.status.idle":"2025-11-04T14:20:37.727147Z","shell.execute_reply.started":"2025-11-04T14:20:22.614610Z","shell.execute_reply":"2025-11-04T14:20:37.726454Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"=== TEST ROWS ===\n essay_set  essay_score bloom_level\n         7           20      Create\n         2            2      Create\n\n\n\n=== JURY TEST (first 2 rows) ===\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"\n--- ROW 0 (Set 7) ---\nTrue score : 20\nJury 1-5   : 0\nJury norm  : -4\n\n  [Phi] score = 0\n  JSON   : PARSE ERROR: Expecting value: line 1 column 11 (char 10)\n  RAW    : de a brief rationale. Respond in JSON only: {\"score\": int, \"rationale\": str} {\"score\": 5, \"rationale\": \"The response demonstrates a clear understanding of the concept of patience and provides a personal example of how patience was exercised. The student also explains the importance of patience and encourages others to try it. The response is well-written and effectively communicates the message.\"}\n\n  [Mistral] score = 0\n  JSON   : PARSE ERROR: Expecting value: line 1 column 11 (char 10)\n  RAW    : , I got my mom to change the age to twelve. So all I have to do is wait until soccer is finished and find a place and she'll take me. Patience really helps in many different ways and I encourage you to try being patient when you want something, because it will all fall into place.\n\nScore this response from 1 to 5 and provide a brief rationale. Respond in JSON only: {\"score\": int, \"rationale\": str}\n\n  [Qwen] score = 0\n  JSON   : PARSE ERROR: Expecting value: line 1 column 11 (char 10)\n  RAW    : mom to change the age to twelve. So all I have to do is wait until soccer is finished and find a place and she'll take me. Patience really helps in many different ways and I encourage you to try being patient when you want something, because it will all fall into place.\n\nScore this response from 1 to 5 and provide a brief rationale. Respond in JSON only: {\"score\": int, \"rationale\": str}\nassistant\n\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"\n--- ROW 1 (Set 2) ---\nTrue score : 2\nJury 1-5   : 0\nJury norm  : 0\n\n  [Phi] score = 0\n  JSON   : PARSE ERROR: Expecting value: line 1 column 11 (char 10)\n  RAW    :  understanding of the prompt and provides some valid points about the importance of censorship in libraries. However, the response lacks organization and clarity, making it difficult to follow. The student's arguments are not well-developed and are often presented in a confusing manner. The response would benefit from a more structured approach and a clearer explanation of the points being made.\"}\n\n  [Mistral] score = 0\n  JSON   : PARSE ERROR: Expecting value: line 1 column 11 (char 10)\n  RAW    : ake a lot mof monoey.     So that why i think they should not take stuff off because that is how we are loseing all hour money.The people are useing the that they would for movies or books and stuff they are useing for drugs and other stuff that can get them sent to jail and stuff\n\nScore this response from 1 to 5 and provide a brief rationale. Respond in JSON only: {\"score\": int, \"rationale\": str}\n\n  [Qwen] score = 0\n  JSON   : PARSE ERROR: Expecting value: line 1 column 11 (char 10)\n  RAW    : of monoey.     So that why i think they should not take stuff off because that is how we are loseing all hour money.The people are useing the that they would for movies or books and stuff they are useing for drugs and other stuff that can get them sent to jail and stuff\n\nScore this response from 1 to 5 and provide a brief rationale. Respond in JSON only: {\"score\": int, \"rationale\": str}\nassistant\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"train_df = pd.read_pickle(\"/kaggle/working/checkpoint_splits_20251103_211458.pkl\")['train']\ntrain_df.head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T14:27:54.063298Z","iopub.execute_input":"2025-11-04T14:27:54.063590Z","iopub.status.idle":"2025-11-04T14:27:54.075147Z","shell.execute_reply.started":"2025-11-04T14:27:54.063567Z","shell.execute_reply":"2025-11-04T14:27:54.074467Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"                                                prompt  \\\n55   The writers had to write a persuasive essay re...   \n487  Write a story about a time when you, or someon...   \n\n                                              response  essay_score  \\\n55   I think that libraries should not remove mater...            3   \n487  Patientce is hard for example when you walk in...           10   \n\n     essay_set bloom_level  label_score  \\\n55           2      Create            5   \n487          7      Create            5   \n\n                                             rationale  \n55   The question asks students to produce a new pi...  \n487  The question asks the user to generate a new n...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response</th>\n      <th>essay_score</th>\n      <th>essay_set</th>\n      <th>bloom_level</th>\n      <th>label_score</th>\n      <th>rationale</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>55</th>\n      <td>The writers had to write a persuasive essay re...</td>\n      <td>I think that libraries should not remove mater...</td>\n      <td>3</td>\n      <td>2</td>\n      <td>Create</td>\n      <td>5</td>\n      <td>The question asks students to produce a new pi...</td>\n    </tr>\n    <tr>\n      <th>487</th>\n      <td>Write a story about a time when you, or someon...</td>\n      <td>Patientce is hard for example when you walk in...</td>\n      <td>10</td>\n      <td>7</td>\n      <td>Create</td>\n      <td>5</td>\n      <td>The question asks the user to generate a new n...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"train_df['label_score'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T14:27:56.565690Z","iopub.execute_input":"2025-11-04T14:27:56.566249Z","iopub.status.idle":"2025-11-04T14:27:56.571904Z","shell.execute_reply.started":"2025-11-04T14:27:56.566223Z","shell.execute_reply":"2025-11-04T14:27:56.571218Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"label_score\n5    300\nName: count, dtype: int64"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"print(\"label_score distribution:\")\nprint(train_df['label_score'].value_counts())\nprint(\"\\nessay_score distribution:\")\nprint(train_df['essay_score'].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T14:29:04.586095Z","iopub.execute_input":"2025-11-04T14:29:04.586378Z","iopub.status.idle":"2025-11-04T14:29:04.598527Z","shell.execute_reply.started":"2025-11-04T14:29:04.586355Z","shell.execute_reply":"2025-11-04T14:29:04.597965Z"}},"outputs":[{"name":"stdout","text":"label_score distribution:\nlabel_score\n5    300\nName: count, dtype: int64\n\nessay_score distribution:\ncount    300.000000\nmean       7.320000\nstd        9.301577\nmin        0.000000\n25%        2.000000\n50%        3.000000\n75%        8.250000\nmax       45.000000\nName: essay_score, dtype: float64\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"## Final","metadata":{}},{"cell_type":"code","source":"# FINAL CODE — RUN THIS\nimport pandas as pd\nimport torch\nimport re\nimport json\nfrom tqdm import tqdm\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load test data\ntest_df = pd.read_pickle(\"/kaggle/working/checkpoint_splits_20251103_211458.pkl\")['test']\n\n# Load 3 fine-tuned models\ndef load_lora(base_id, lora_path):\n    print(f\"Loading {lora_path}...\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_id, load_in_4bit=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n    )\n    model = PeftModel.from_pretrained(base, lora_path)\n    tokenizer = AutoTokenizer.from_pretrained(base_id)\n    tokenizer.pad_token = tokenizer.eos_token\n    return model, tokenizer\n\n# phi, phi_tok = load_lora(\"microsoft/Phi-3-mini-4k-instruct\", \"models/phi3_lora/checkpoint_300\")\n# mistral, mistral_tok = load_lora(\"mistralai/Mistral-7B-Instruct-v0.3\", \"models/mistral_lora/checkpoint_300\")\n# qwen, qwen_tok = load_lora(\"Qwen/Qwen2.5-7B-Instruct\", \"models/qwen_lora/checkpoint_300\")\n\nmodels = [(phi, phi_tok, \"Phi\"), (mistral, mistral_tok, \"Mistral\"), (qwen, qwen_tok, \"Qwen\")]\n\n# Extract score from model output\ndef extract_score(text):\n    try:\n        j = re.findall(r'\\{.*\\}', text, re.DOTALL)[-1]\n        data = json.loads(j)\n        return int(data.get(\"score\", 0))\n    except:\n        return 0\n\n# Jury scoring\ndef jury_score(row):\n    scores = []\n    for model, tok, name in models:\n        messages = [\n            {\"role\": \"system\", \"content\": f\"You are an expert evaluator at the {row['bloom_level']} level of Bloom's Taxonomy.\"},\n            {\"role\": \"user\", \"content\": f\"Prompt: {row['prompt']}\\n\\nStudent Response: {row['response']}\\n\\nScore this response from 1 to 5 and provide a brief rationale. Respond in JSON only: {{\\\"score\\\": int, \\\"rationale\\\": str}}\"}\n        ]\n        inputs = tok.apply_chat_template(\n            messages,\n            return_tensors=\"pt\",\n            add_generation_prompt=True,\n            tokenize=True,\n            return_dict=True\n        ).to(\"cuda\")\n\n        with torch.no_grad():\n            out = model.generate(\n                **inputs,\n                max_new_tokens=100,\n                temperature=0.0,\n                do_sample=False\n            )\n        resp = tok.decode(out[0], skip_special_tokens=True)\n        score = extract_score(resp)\n        scores.append(score)\n    return round(sum(scores)/len(scores)) if scores else 0\n\n# Run jury\njury_scores = []\nfor _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Jury Evaluation\"):\n    jury_scores.append(jury_score(row))\n\n# Normalize 1–5 → ASAP scale per essay_set (CORRECTED)\nscale_map = {\n    1: (2, 12),   # Set 1\n    2: (1, 6),    # Set 2\n    3: (0, 3),    # Set 3\n    4: (0, 3),    # Set 4\n    5: (0, 4),    # Set 5\n    6: (0, 4),    # Set 6\n    7: (2, 24),   # Set 7\n    8: (10, 60)   # Set 8\n}\n\ndef normalize(jury_1to5, essay_set):\n    min_s, max_s = scale_map.get(essay_set, (0, 12))\n    return round(min_s + (jury_1to5 - 1) * (max_s - min_s) / 4)\n\njury_normalized = [\n    normalize(j, r['essay_set']) for j, (_, r) in zip(jury_scores, test_df.iterrows())\n]\n\n# Compute final Spearman ρ\nrho = test_df['essay_score'].corr(pd.Series(jury_normalized), method='spearman')\nprint(f\"\\nFINAL JURY Spearman ρ: {rho:.3f}\")\n\n# Save results\nresults = pd.DataFrame({\n    'essay_set': test_df['essay_set'],\n    'true_score': test_df['essay_score'],\n    'jury_1to5': jury_scores,\n    'jury_normalized': jury_normalized\n})\nresults.to_csv(\"/kaggle/working/jury_final_normalized.csv\", index=False)\nprint(\"Results saved to jury_final_normalized.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T14:31:26.480209Z","iopub.execute_input":"2025-11-04T14:31:26.480496Z","iopub.status.idle":"2025-11-04T14:44:30.332670Z","shell.execute_reply.started":"2025-11-04T14:31:26.480473Z","shell.execute_reply":"2025-11-04T14:44:30.331922Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Jury Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   1%|          | 1/100 [00:05<09:53,  5.99s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   2%|▏         | 2/100 [00:14<12:34,  7.70s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   3%|▎         | 3/100 [00:20<10:53,  6.74s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   4%|▍         | 4/100 [00:28<11:24,  7.13s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   5%|▌         | 5/100 [00:34<10:54,  6.89s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   6%|▌         | 6/100 [00:43<11:52,  7.58s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   7%|▋         | 7/100 [00:51<11:53,  7.67s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   8%|▊         | 8/100 [00:59<11:57,  7.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:   9%|▉         | 9/100 [01:07<11:50,  7.81s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  10%|█         | 10/100 [01:13<10:54,  7.27s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  11%|█         | 11/100 [01:22<11:35,  7.82s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  12%|█▏        | 12/100 [01:33<13:02,  8.89s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  13%|█▎        | 13/100 [01:40<12:05,  8.34s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  14%|█▍        | 14/100 [01:47<11:22,  7.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  15%|█▌        | 15/100 [01:55<10:59,  7.76s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  16%|█▌        | 16/100 [02:03<11:05,  7.92s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  17%|█▋        | 17/100 [02:12<11:11,  8.10s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  18%|█▊        | 18/100 [02:17<10:03,  7.35s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  19%|█▉        | 19/100 [02:24<09:37,  7.13s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  20%|██        | 20/100 [02:33<10:16,  7.71s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  21%|██        | 21/100 [02:40<10:05,  7.67s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  22%|██▏       | 22/100 [02:49<10:14,  7.88s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  23%|██▎       | 23/100 [03:00<11:19,  8.83s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  24%|██▍       | 24/100 [03:10<11:50,  9.35s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  25%|██▌       | 25/100 [03:19<11:28,  9.18s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  26%|██▌       | 26/100 [03:27<10:39,  8.64s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  27%|██▋       | 27/100 [03:35<10:24,  8.56s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  28%|██▊       | 28/100 [03:43<10:12,  8.51s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  29%|██▉       | 29/100 [03:52<10:10,  8.59s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  30%|███       | 30/100 [04:00<09:50,  8.44s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  31%|███       | 31/100 [04:08<09:23,  8.16s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  32%|███▏      | 32/100 [04:17<09:42,  8.57s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  33%|███▎      | 33/100 [04:25<09:12,  8.24s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  34%|███▍      | 34/100 [04:34<09:31,  8.66s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  35%|███▌      | 35/100 [04:42<09:08,  8.45s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  36%|███▌      | 36/100 [04:50<08:49,  8.27s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  37%|███▋      | 37/100 [04:58<08:30,  8.11s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  38%|███▊      | 38/100 [05:05<08:10,  7.91s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  39%|███▉      | 39/100 [05:14<08:14,  8.11s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  40%|████      | 40/100 [05:23<08:22,  8.38s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  41%|████      | 41/100 [05:31<08:04,  8.21s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  42%|████▏     | 42/100 [05:38<07:48,  8.07s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  43%|████▎     | 43/100 [05:48<07:58,  8.40s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  44%|████▍     | 44/100 [05:56<07:47,  8.35s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  45%|████▌     | 45/100 [06:04<07:29,  8.18s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  46%|████▌     | 46/100 [06:12<07:27,  8.28s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  47%|████▋     | 47/100 [06:15<05:56,  6.73s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  48%|████▊     | 48/100 [06:27<07:09,  8.26s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  49%|████▉     | 49/100 [06:30<05:43,  6.73s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  50%|█████     | 50/100 [06:39<06:00,  7.22s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  51%|█████     | 51/100 [06:48<06:24,  7.84s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  52%|█████▏    | 52/100 [06:51<05:08,  6.43s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  53%|█████▎    | 53/100 [06:59<05:30,  7.03s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  54%|█████▍    | 54/100 [07:08<05:43,  7.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  55%|█████▌    | 55/100 [07:14<05:15,  7.00s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  56%|█████▌    | 56/100 [07:21<05:14,  7.15s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  57%|█████▋    | 57/100 [07:29<05:16,  7.36s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  58%|█████▊    | 58/100 [07:37<05:13,  7.48s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  59%|█████▉    | 59/100 [07:45<05:11,  7.60s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  60%|██████    | 60/100 [07:53<05:06,  7.67s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  61%|██████    | 61/100 [07:56<04:06,  6.31s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  62%|██████▏   | 62/100 [08:04<04:21,  6.87s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  63%|██████▎   | 63/100 [08:14<04:50,  7.86s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  64%|██████▍   | 64/100 [08:22<04:39,  7.77s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  65%|██████▌   | 65/100 [08:32<05:02,  8.65s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  66%|██████▌   | 66/100 [08:39<04:32,  8.02s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  67%|██████▋   | 67/100 [08:48<04:29,  8.17s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  68%|██████▊   | 68/100 [08:57<04:38,  8.72s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  69%|██████▉   | 69/100 [09:06<04:26,  8.61s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  70%|███████   | 70/100 [09:13<04:07,  8.25s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  71%|███████   | 71/100 [09:21<03:54,  8.10s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  72%|███████▏  | 72/100 [09:29<03:43,  7.99s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  73%|███████▎  | 73/100 [09:37<03:34,  7.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  74%|███████▍  | 74/100 [09:41<02:55,  6.76s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  75%|███████▌  | 75/100 [09:49<02:58,  7.15s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  76%|███████▌  | 76/100 [09:57<03:00,  7.52s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  77%|███████▋  | 77/100 [10:06<02:59,  7.81s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  78%|███████▊  | 78/100 [10:08<02:17,  6.24s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  79%|███████▉  | 79/100 [10:15<02:15,  6.43s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  80%|████████  | 80/100 [10:19<01:53,  5.68s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  81%|████████  | 81/100 [10:27<01:59,  6.31s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  82%|████████▏ | 82/100 [10:34<02:00,  6.68s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  83%|████████▎ | 83/100 [10:42<01:58,  6.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  84%|████████▍ | 84/100 [10:50<01:56,  7.28s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  85%|████████▌ | 85/100 [10:58<01:51,  7.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  86%|████████▌ | 86/100 [11:09<01:58,  8.49s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  87%|████████▋ | 87/100 [11:18<01:53,  8.76s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  88%|████████▊ | 88/100 [11:26<01:42,  8.55s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  89%|████████▉ | 89/100 [11:36<01:39,  9.02s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  90%|█████████ | 90/100 [11:45<01:29,  8.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  91%|█████████ | 91/100 [11:55<01:24,  9.37s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  92%|█████████▏| 92/100 [12:05<01:15,  9.45s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  93%|█████████▎| 93/100 [12:15<01:07,  9.69s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  94%|█████████▍| 94/100 [12:24<00:56,  9.34s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  95%|█████████▌| 95/100 [12:32<00:45,  9.11s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  96%|█████████▌| 96/100 [12:39<00:33,  8.27s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  97%|█████████▋| 97/100 [12:46<00:23,  7.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  98%|█████████▊| 98/100 [12:53<00:15,  7.79s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation:  99%|█████████▉| 99/100 [12:56<00:06,  6.39s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nJury Evaluation: 100%|██████████| 100/100 [13:03<00:00,  7.84s/it]","output_type":"stream"},{"name":"stdout","text":"\nFINAL JURY Spearman ρ: -0.431\nResults saved to jury_final_normalized.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T14:46:03.919265Z","iopub.execute_input":"2025-11-04T14:46:03.919869Z","iopub.status.idle":"2025-11-04T14:46:03.928808Z","shell.execute_reply.started":"2025-11-04T14:46:03.919844Z","shell.execute_reply":"2025-11-04T14:46:03.927968Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"     essay_set  true_score  jury_1to5  jury_normalized\n393          7          20          0               -4\n309          2           2          0                0\n3            7          17          0               -4\n129          7          14          0               -4\n404          4           0          0               -1\n..         ...         ...        ...              ...\n333          4           3          0               -1\n85           7           8          0               -4\n116          7          20          0               -4\n427          2           3          0                0\n419          5           1          0               -1\n\n[100 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_set</th>\n      <th>true_score</th>\n      <th>jury_1to5</th>\n      <th>jury_normalized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>393</th>\n      <td>7</td>\n      <td>20</td>\n      <td>0</td>\n      <td>-4</td>\n    </tr>\n    <tr>\n      <th>309</th>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>17</td>\n      <td>0</td>\n      <td>-4</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>7</td>\n      <td>14</td>\n      <td>0</td>\n      <td>-4</td>\n    </tr>\n    <tr>\n      <th>404</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>333</th>\n      <td>4</td>\n      <td>3</td>\n      <td>0</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>7</td>\n      <td>8</td>\n      <td>0</td>\n      <td>-4</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>7</td>\n      <td>20</td>\n      <td>0</td>\n      <td>-4</td>\n    </tr>\n    <tr>\n      <th>427</th>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>419</th>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"# FINAL CODE — RUN THIS\nimport pandas as pd\nimport torch\nimport re\nimport json\nfrom tqdm import tqdm\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntest_df = pd.read_pickle(\"/kaggle/working/checkpoint_splits_20251103_211458.pkl\")['test']\n\ndef load_lora(base_id, lora_path):\n    base = AutoModelForCausalLM.from_pretrained(\n        base_id, load_in_4bit=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n    )\n    model = PeftModel.from_pretrained(base, lora_path)\n    tokenizer = AutoTokenizer.from_pretrained(base_id)\n    tokenizer.pad_token = tokenizer.eos_token\n    return model, tokenizer\n\n# phi, phi_tok = load_lora(\"microsoft/Phi-3-mini-4k-instruct\", \"models/phi3_lora/checkpoint_300\")\n# mistral, mistral_tok = load_lora(\"mistralai/Mistral-7B-Instruct-v0.3\", \"models/mistral_lora/checkpoint_300\")\n# qwen, qwen_tok = load_lora(\"Qwen/Qwen2.5-7B-Instruct\", \"models/qwen_lora/checkpoint_300\")\n\nmodels = [(phi, phi_tok, \"Phi\"), (mistral, mistral_tok, \"Mistral\"), (qwen, qwen_tok, \"Qwen\")]\n\ndef jury_score(row):\n    scores = []\n    for model, tok, name in models:\n        messages = [\n            {\"role\": \"system\", \"content\": f\"You are an expert evaluator at the {row['bloom_level']} level.\"},\n            {\"role\": \"user\", \"content\": f\"Prompt: {row['prompt']}\\n\\nResponse: {row['response']}\\n\\nScore 1–5. JSON only.\"}\n        ]\n        inputs = tok.apply_chat_template(\n            messages,\n            return_tensors=\"pt\",\n            add_generation_prompt=True\n        ).to(\"cuda\")\n\n        with torch.no_grad():\n            out = model.generate(\n                input_ids=inputs,\n                max_new_tokens=80,\n                temperature=0.0,\n                do_sample=False\n            )\n        resp = tok.decode(out[0], skip_special_tokens=True)\n        try:\n            j = re.search(r'\\{.*\\}', resp, re.DOTALL).group()\n            data = json.loads(j)\n            score = int(data.get(\"score\", 0))\n        except:\n            score = 0\n        scores.append(score)\n    return round(sum(scores)/len(scores)) if scores else 0\n\njury_scores = [jury_score(row) for _, row in tqdm(test_df.iterrows(), total=100)]\n\n# Normalize\nscale_map = {1: (2,12), 2:(1,6), 3:(0,3), 4:(0,3), 5:(0,4), 6:(0,4), 7:(2,24), 8:(10,60)}\ndef normalize(j, s): \n    mn, mx = scale_map[s]\n    return round(mn + (j-1)*(mx-mn)/4)\n\njury_normalized = [normalize(j, r['essay_set']) for j, (_, r) in zip(jury_scores, test_df.iterrows())]\n\nrho = test_df['essay_score'].corr(pd.Series(jury_normalized), method='spearman')\nprint(f\"FINAL ρ: {rho:.3f}\")\n\nresults = pd.DataFrame({'true_score': test_df['essay_score'], 'jury_normalized': jury_normalized})\nresults.to_csv(\"jury_final.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T14:58:27.493101Z","iopub.execute_input":"2025-11-04T14:58:27.493418Z","iopub.status.idle":"2025-11-04T15:09:19.775811Z","shell.execute_reply.started":"2025-11-04T14:58:27.493396Z","shell.execute_reply":"2025-11-04T15:09:19.774967Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"\n  0%|          | 0/100 [00:00<?, ?it/s]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n  1%|          | 1/100 [00:10<17:08, 10.39s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n  2%|▏         | 2/100 [00:13<10:02,  6.15s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n  3%|▎         | 3/100 [00:22<11:42,  7.24s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n  4%|▍         | 4/100 [00:30<12:18,  7.69s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n  5%|▌         | 5/100 [00:32<08:54,  5.63s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n  6%|▌         | 6/100 [00:36<07:46,  4.96s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n  7%|▋         | 7/100 [00:38<06:08,  3.96s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n  8%|▊         | 8/100 [00:40<05:18,  3.46s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n  9%|▉         | 9/100 [00:48<07:39,  5.05s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 10%|█         | 10/100 [00:55<08:19,  5.55s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 11%|█         | 11/100 [01:03<09:14,  6.23s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 12%|█▏        | 12/100 [01:09<09:07,  6.23s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 13%|█▎        | 13/100 [01:16<09:17,  6.41s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 14%|█▍        | 14/100 [01:23<09:32,  6.66s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 15%|█▌        | 15/100 [01:33<10:49,  7.64s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 16%|█▌        | 16/100 [01:38<09:24,  6.72s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 17%|█▋        | 17/100 [01:45<09:40,  7.00s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 18%|█▊        | 18/100 [01:51<09:09,  6.70s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 19%|█▉        | 19/100 [01:55<07:46,  5.76s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 20%|██        | 20/100 [01:58<06:29,  4.87s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 21%|██        | 21/100 [02:04<07:00,  5.32s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 22%|██▏       | 22/100 [02:15<09:02,  6.95s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 23%|██▎       | 23/100 [02:27<10:58,  8.55s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 24%|██▍       | 24/100 [02:43<13:26, 10.61s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 25%|██▌       | 25/100 [02:47<10:47,  8.63s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 26%|██▌       | 26/100 [02:48<08:10,  6.63s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 27%|██▋       | 27/100 [02:56<08:22,  6.88s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 28%|██▊       | 28/100 [03:03<08:28,  7.06s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 29%|██▉       | 29/100 [03:06<06:45,  5.72s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 30%|███       | 30/100 [03:13<07:13,  6.19s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 31%|███       | 31/100 [03:22<08:00,  6.96s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 32%|███▏      | 32/100 [03:36<10:24,  9.19s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 33%|███▎      | 33/100 [03:39<08:02,  7.19s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 34%|███▍      | 34/100 [03:44<07:07,  6.47s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 35%|███▌      | 35/100 [03:50<06:53,  6.36s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 36%|███▌      | 36/100 [03:57<06:56,  6.51s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 37%|███▋      | 37/100 [04:03<06:46,  6.45s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 38%|███▊      | 38/100 [04:06<05:31,  5.35s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 39%|███▉      | 39/100 [04:11<05:19,  5.24s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 40%|████      | 40/100 [04:26<08:13,  8.22s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 41%|████      | 41/100 [04:33<07:39,  7.79s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 42%|████▏     | 42/100 [04:37<06:24,  6.63s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 43%|████▎     | 43/100 [04:45<06:40,  7.03s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 44%|████▍     | 44/100 [04:47<05:07,  5.49s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 45%|████▌     | 45/100 [04:58<06:46,  7.39s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 46%|████▌     | 46/100 [05:06<06:43,  7.48s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 47%|████▋     | 47/100 [05:09<05:28,  6.19s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 48%|████▊     | 48/100 [05:27<08:20,  9.62s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 49%|████▉     | 49/100 [05:31<06:51,  8.08s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 50%|█████     | 50/100 [05:38<06:29,  7.79s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 51%|█████     | 51/100 [05:48<06:43,  8.24s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 52%|█████▏    | 52/100 [05:56<06:32,  8.17s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 53%|█████▎    | 53/100 [06:03<06:10,  7.89s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 54%|█████▍    | 54/100 [06:06<04:52,  6.35s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 55%|█████▌    | 55/100 [06:08<03:45,  5.01s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 56%|█████▌    | 56/100 [06:10<03:05,  4.22s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 57%|█████▋    | 57/100 [06:12<02:32,  3.55s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 58%|█████▊    | 58/100 [06:17<02:43,  3.89s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 59%|█████▉    | 59/100 [06:19<02:16,  3.33s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 60%|██████    | 60/100 [06:29<03:32,  5.31s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 61%|██████    | 61/100 [06:32<03:03,  4.69s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 62%|██████▏   | 62/100 [06:34<02:26,  3.87s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 63%|██████▎   | 63/100 [06:49<04:31,  7.33s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 64%|██████▍   | 64/100 [06:52<03:31,  5.88s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 65%|██████▌   | 65/100 [06:58<03:26,  5.90s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 66%|██████▌   | 66/100 [07:09<04:14,  7.48s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 67%|██████▋   | 67/100 [07:11<03:17,  5.98s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 68%|██████▊   | 68/100 [07:20<03:39,  6.87s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 69%|██████▉   | 69/100 [07:28<03:44,  7.25s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 70%|███████   | 70/100 [07:37<03:50,  7.68s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 71%|███████   | 71/100 [07:44<03:33,  7.37s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 72%|███████▏  | 72/100 [07:51<03:21,  7.20s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 73%|███████▎  | 73/100 [07:53<02:36,  5.78s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 74%|███████▍  | 74/100 [08:00<02:41,  6.21s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 75%|███████▌  | 75/100 [08:03<02:07,  5.12s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 76%|███████▌  | 76/100 [08:05<01:40,  4.18s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 77%|███████▋  | 77/100 [08:07<01:24,  3.67s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 78%|███████▊  | 78/100 [08:10<01:12,  3.31s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 79%|███████▉  | 79/100 [08:16<01:30,  4.30s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 80%|████████  | 80/100 [08:26<01:59,  5.99s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 81%|████████  | 81/100 [08:35<02:11,  6.90s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 82%|████████▏ | 82/100 [08:43<02:08,  7.11s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 83%|████████▎ | 83/100 [08:51<02:05,  7.36s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 84%|████████▍ | 84/100 [08:54<01:39,  6.22s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 85%|████████▌ | 85/100 [08:57<01:15,  5.04s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 86%|████████▌ | 86/100 [09:19<02:24, 10.33s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 87%|████████▋ | 87/100 [09:23<01:47,  8.28s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 88%|████████▊ | 88/100 [09:25<01:18,  6.54s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 89%|████████▉ | 89/100 [09:31<01:09,  6.30s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 90%|█████████ | 90/100 [09:35<00:55,  5.57s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 91%|█████████ | 91/100 [09:48<01:10,  7.79s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 92%|█████████▏| 92/100 [09:57<01:06,  8.30s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 93%|█████████▎| 93/100 [10:06<00:59,  8.46s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 94%|█████████▍| 94/100 [10:14<00:48,  8.13s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 95%|█████████▌| 95/100 [10:21<00:39,  7.85s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 96%|█████████▌| 96/100 [10:25<00:26,  6.69s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 97%|█████████▋| 97/100 [10:28<00:16,  5.58s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 98%|█████████▊| 98/100 [10:35<00:11,  5.96s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n 99%|█████████▉| 99/100 [10:43<00:06,  6.64s/it]\u001b[AThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n\n100%|██████████| 100/100 [10:52<00:00,  6.52s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"FINAL ρ: -0.162\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"def jury_score(row):\n    scores = []\n    for model, tok, name in models:\n        messages = [\n            {\"role\": \"system\", \"content\": f\"You are an expert evaluator at the {row['bloom_level']} level.\"},\n            {\"role\": \"user\", \"content\": f\"Prompt: {row['prompt']}\\n\\nResponse: {row['response']}\\n\\nScore 1–5. JSON only.\"}\n        ]\n        inputs = tok.apply_chat_template(\n            messages,\n            return_tensors=\"pt\",\n            add_generation_prompt=True\n        ).to(\"cuda\")\n\n        # Force JSON\n        generate_kwargs = dict(\n            input_ids=inputs,\n            max_new_tokens=80,\n            temperature=0.0,\n            do_sample=False,\n            response_format={\"type\": \"json_object\"}\n        )\n        \n        with torch.no_grad():\n            out = model.generate(**generate_kwargs)\n        \n        resp = tok.decode(out[0], skip_special_tokens=True)\n        \n        try:\n            j = re.search(r'\\{.*\\}', resp, re.DOTALL).group()\n            data = json.loads(j)\n            score = int(data.get(\"score\", 0))\n        except:\n            score = 0\n        scores.append(score)\n    \n    return round(sum(scores)/len(scores)) if scores else 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T15:10:36.714014Z","iopub.execute_input":"2025-11-04T15:10:36.714588Z","iopub.status.idle":"2025-11-04T15:10:36.721327Z","shell.execute_reply.started":"2025-11-04T15:10:36.714564Z","shell.execute_reply":"2025-11-04T15:10:36.720421Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# FINAL CODE — RUN THIS\nimport pandas as pd\nimport torch\nimport re\nimport json\nfrom tqdm import tqdm\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load test data\ntest_df = pd.read_pickle(\"/kaggle/working/checkpoint_splits_20251103_211458.pkl\")['test']\n\n# Load 3 fine-tuned models\ndef load_lora(base_id, lora_path):\n    base = AutoModelForCausalLM.from_pretrained(\n        base_id, load_in_4bit=True, device_map=\"auto\", torch_dtype=torch.bfloat16\n    )\n    # Force load from checkpoint\n    model = PeftModel.from_pretrained(base, lora_path, is_trainable=False)\n    tokenizer = AutoTokenizer.from_pretrained(base_id)\n    tokenizer.pad_token = tokenizer.eos_token\n    return model, tokenizer\n\nphi, phi_tok = load_lora(\"microsoft/Phi-3-mini-4k-instruct\", \"models/phi3_lora/checkpoint_300\")\nmistral, mistral_tok = load_lora(\"mistralai/Mistral-7B-Instruct-v0.3\", \"models/mistral_lora/checkpoint_300\")\nqwen, qwen_tok = load_lora(\"Qwen/Qwen2.5-7B-Instruct\", \"models/qwen_lora/checkpoint_300\")\n\nmodels = [(phi, phi_tok, \"Phi\"), (mistral, mistral_tok, \"Mistral\"), (qwen, qwen_tok, \"Qwen\")]\n\n# Extract score from model output\ndef extract_score(text):\n    try:\n        j = re.findall(r'\\{.*\\}', text, re.DOTALL)[-1]\n        data = json.loads(j)\n        return int(data.get(\"score\", 0))\n    except:\n        return 0\n\n# Jury scoring\ndef jury_score(row):\n    scores = []\n    for model, tok, name in models:\n        messages = [\n            {\"role\": \"system\", \"content\": f\"You are an expert evaluator at the {row['bloom_level']} level of Bloom's Taxonomy.\"},\n            {\"role\": \"user\", \"content\": f\"Prompt: {row['prompt']}\\n\\nStudent Response: {row['response']}\\n\\nScore this response from 1 to 5. Respond in JSON only: {{\\\"score\\\": int, \\\"rationale\\\": str}}\"}\n        ]\n        inputs = tok.apply_chat_template(\n            messages,\n            return_tensors=\"pt\",\n            add_generation_prompt=True,\n            tokenize=True,\n            return_dict=True\n        )[\"input_ids\"].to(\"cuda\")\n\n        with torch.no_grad():\n            out = model.generate(\n                input_ids=inputs,\n                max_new_tokens=100,\n                temperature=0.0,\n                do_sample=False\n            )\n        resp = tok.decode(out[0], skip_special_tokens=True)\n        score = extract_score(resp)\n        scores.append(score)\n    return round(sum(scores)/len(scores)) if scores else 0\n\n# Run jury\njury_scores = []\nfor _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Jury Evaluation\"):\n    jury_scores.append(jury_score(row))\n\n# Normalize 1–5 → ASAP scale per essay_set (CORRECTED)\nscale_map = {\n    1: (2, 12),   # Set 1\n    2: (1, 6),    # Set 2\n    3: (0, 3),    # Set 3\n    4: (0, 3),    # Set 4\n    5: (0, 4),    # Set 5\n    6: (0, 4),    # Set 6\n    7: (2, 24),   # Set 7\n    8: (10, 60)   # Set 8\n}\n\ndef normalize(jury_1to5, essay_set):\n    min_s, max_s = scale_map.get(essay_set, (0, 12))\n    return round(min_s + (jury_1to5 - 1) * (max_s - min_s) / 4)\n\njury_normalized = [\n    normalize(j, r['essay_set']) for j, (_, r) in zip(jury_scores, test_df.iterrows())\n]\n\n# Compute final Spearman ρ\nrho = test_df['essay_score'].corr(pd.Series(jury_normalized), method='spearman')\nprint(f\"\\nFINAL JURY Spearman ρ: {rho:.3f}\")\n\n# Save results\nresults = pd.DataFrame({\n    'essay_set': test_df['essay_set'],\n    'true_score': test_df['essay_score'],\n    'jury_1to5': jury_scores,\n    'jury_normalized': jury_normalized\n})\nresults.to_csv(\"/kaggle/working/jury_final_normalized.csv\", index=False)\nprint(\"Results saved to jury_final_normalized.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T15:21:59.873994Z","iopub.execute_input":"2025-11-04T15:21:59.874278Z","iopub.status.idle":"2025-11-04T15:22:07.919901Z","shell.execute_reply.started":"2025-11-04T15:21:59.874256Z","shell.execute_reply":"2025-11-04T15:22:07.919030Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d06a744a58bb43dea980750bf71ce19c"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    263\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'models/phi3_lora/checkpoint_300'. Use `repo_type` argument if needed.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_146/1529929531.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_lora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"microsoft/Phi-3-mini-4k-instruct\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models/phi3_lora/checkpoint_300\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mmistral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmistral_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_lora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mistralai/Mistral-7B-Instruct-v0.3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models/mistral_lora/checkpoint_300\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mqwen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqwen_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_lora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Qwen/Qwen2.5-7B-Instruct\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models/qwen_lora/checkpoint_300\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_146/1529929531.py\u001b[0m in \u001b[0;36mload_lora\u001b[0;34m(base_id, lora_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Force load from checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_trainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             config = PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 440\u001b[0;31m                 PeftConfig._get_peft_type(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subfolder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 )\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{model_id}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'models/phi3_lora/checkpoint_300'"],"ename":"ValueError","evalue":"Can't find 'adapter_config.json' at 'models/phi3_lora/checkpoint_300'","output_type":"error"}],"execution_count":50},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'/kaggle/working/kaggle_working_directory.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T15:34:21.822434Z","iopub.execute_input":"2025-11-04T15:34:21.822738Z","iopub.status.idle":"2025-11-04T15:34:21.827854Z","shell.execute_reply.started":"2025-11-04T15:34:21.822715Z","shell.execute_reply":"2025-11-04T15:34:21.827279Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/kaggle_working_directory.zip","text/html":"<a href='/kaggle/working/kaggle_working_directory.zip' target='_blank'>/kaggle/working/kaggle_working_directory.zip</a><br>"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"!zip -r -s 2g /kaggle/working/working_split.zip /kaggle/working/kaggle_working_directory.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T15:37:36.471624Z","iopub.execute_input":"2025-11-04T15:37:36.472489Z","iopub.status.idle":"2025-11-04T15:38:12.779845Z","shell.execute_reply.started":"2025-11-04T15:37:36.472457Z","shell.execute_reply":"2025-11-04T15:38:12.778855Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/kaggle_working_directory.zip","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (stored 0%)\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:08:43.430789Z","iopub.execute_input":"2025-11-04T16:08:43.431048Z","iopub.status.idle":"2025-11-04T16:08:43.850551Z","shell.execute_reply.started":"2025-11-04T16:08:43.431031Z","shell.execute_reply":"2025-11-04T16:08:43.849761Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"\"\"\"\nReduce Test Set to 50 Essays\n=============================\nThis creates a smaller test set for faster evaluation.\n\"\"\"\n\nimport pandas as pd\n\nprint(\"=\"*80)\nprint(\"REDUCING TEST SET TO 50 ESSAYS\")\nprint(\"=\"*80)\n\n# Load original data\noriginal_path = '/kaggle/working/checkpoint_splits_20251103_211458.pkl'\nprint(f\"\\nLoading: {original_path}\")\n\ndata = pd.read_pickle(original_path)\ntest_df = data['test']\n\nprint(f\"Original test set size: {len(test_df)} essays\")\n\n# Sample 50 essays (stratified by bloom_level if possible)\nif 'bloom_level' in test_df.columns:\n    # Try to get balanced sample\n    bloom_counts = test_df['bloom_level'].value_counts()\n    print(f\"\\nBloom level distribution in original:\")\n    print(bloom_counts)\n    \n    # Sample proportionally\n    n_per_level = min(25, len(test_df) // len(bloom_counts))\n    sampled = []\n    \n    for level in test_df['bloom_level'].unique():\n        level_df = test_df[test_df['bloom_level'] == level]\n        sample_size = min(n_per_level, len(level_df))\n        sampled.append(level_df.sample(sample_size, random_state=42))\n    \n    test_small = pd.concat(sampled).sample(frac=1, random_state=42)  # Shuffle\n    \n    # If we got less than 50, fill up randomly\n    if len(test_small) < 50:\n        remaining = test_df[~test_df.index.isin(test_small.index)]\n        additional = remaining.sample(min(50 - len(test_small), len(remaining)), random_state=42)\n        test_small = pd.concat([test_small, additional])\nelse:\n    # Simple random sample\n    test_small = test_df.sample(min(50, len(test_df)), random_state=42)\n\nprint(f\"\\nReduced test set size: {len(test_small)} essays\")\n\nif 'bloom_level' in test_small.columns:\n    print(f\"\\nBloom level distribution in reduced set:\")\n    print(test_small['bloom_level'].value_counts())\n\n# Save\ndata['test'] = test_small\noutput_path = '/kaggle/working/checkpoint_splits_SMALL.pkl'\npd.to_pickle(data, output_path)\n\nprint(f\"\\n✓ Saved to: {output_path}\")\n\n# Estimate time\nessays = len(test_small)\nmodels = 3\nseconds_per_essay = 60\ntotal_minutes = (essays * models * seconds_per_essay) / 60\ntotal_hours = total_minutes / 60\n\nprint(f\"\\n\" + \"=\"*80)\nprint(\"TIME ESTIMATE\")\nprint(\"=\"*80)\nprint(f\"  Essays: {essays}\")\nprint(f\"  Models: {models}\")\nprint(f\"  Speed: ~{seconds_per_essay}s per essay\")\nprint(f\"  Total: ~{total_minutes:.0f} minutes (~{total_hours:.1f} hours)\")\nprint(f\"\\n  With 8 hours GPU time remaining, you have {8 - total_hours:.1f} hours buffer\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:07:36.814580Z","iopub.execute_input":"2025-11-04T16:07:36.814914Z","iopub.status.idle":"2025-11-04T16:07:36.870658Z","shell.execute_reply.started":"2025-11-04T16:07:36.814890Z","shell.execute_reply":"2025-11-04T16:07:36.869955Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nREDUCING TEST SET TO 50 ESSAYS\n================================================================================\n\nLoading: /kaggle/working/checkpoint_splits_20251103_211458.pkl\nOriginal test set size: 100 essays\n\nBloom level distribution in original:\nbloom_level\nAnalyze    55\nCreate     45\nName: count, dtype: int64\n\nReduced test set size: 50 essays\n\nBloom level distribution in reduced set:\nbloom_level\nCreate     25\nAnalyze    25\nName: count, dtype: int64\n\n✓ Saved to: /kaggle/working/checkpoint_splits_SMALL.pkl\n\n================================================================================\nTIME ESTIMATE\n================================================================================\n  Essays: 50\n  Models: 3\n  Speed: ~60s per essay\n  Total: ~150 minutes (~2.5 hours)\n\n  With 8 hours GPU time remaining, you have 5.5 hours buffer\n================================================================================\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"\"\"\"\nBloomLLM-Jury: MEMORY-EFFICIENT Evaluation Script\n==================================================\nThis version loads ONE model at a time to avoid GPU OOM errors.\n\nKey difference from original:\n- Load model 1 → Evaluate all essays → Unload → Clear memory\n- Load model 2 → Evaluate all essays → Unload → Clear memory\n- Load model 3 → Evaluate all essays → Unload → Clear memory\n- Then aggregate results\n\nThis is slower but works with limited GPU memory.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport json\nimport torch\nimport gc\nfrom tqdm import tqdm\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom scipy.stats import spearmanr, mannwhitneyu, pearsonr\nfrom sklearn.metrics import cohen_kappa_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict, Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nCONFIG = {\n    'models': [\n        {\n            'name': 'Phi',\n            'base_id': 'microsoft/Phi-3-mini-4k-instruct',\n            'lora_path': '/kaggle/working/models/phi3_lora'\n        },\n        {\n            'name': 'Mistral',\n            'base_id': 'mistralai/Mistral-7B-Instruct-v0.3',\n            'lora_path': '/kaggle/working/models/mistral_lora'\n        },\n        {\n            'name': 'Qwen',\n            'base_id': 'Qwen/Qwen2.5-7B-Instruct',\n            'lora_path': '/kaggle/working/models/qwen_lora'\n        }\n    ],\n    'test_data_path': '/kaggle/working/checkpoint_splits_SMALL.pkl',  # UPDATED to use smaller set\n    'output_dir': '/kaggle/working',\n    'generation_params': {\n        'max_new_tokens': 250,\n        'do_sample': False,  # Greedy decoding for consistency\n    },\n}\n\n# ASAP dataset score ranges per essay set (from official documentation)\nASAP_SCORE_RANGES = {\n    1: (1, 6),    # Argumentative\n    2: (1, 6),    # Argumentative\n    3: (0, 3),    # Source-Dependent\n    4: (0, 3),    # Source-Dependent\n    5: (0, 4),    # Source-Dependent\n    6: (0, 4),    # Source-Dependent\n    7: (0, 3),    # Narrative\n    8: (1, 6),    # Narrative\n}\n\n# ============================================================================\n# MODEL LOADING (ONE AT A TIME)\n# ============================================================================\n\ndef load_single_model(base_id, lora_path):\n    \"\"\"Load a single model with proper memory management.\"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"Loading {base_id}...\")\n    print(f\"{'='*80}\")\n    \n    # Clear any existing models\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # Proper 4-bit quantization config\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n    )\n    \n    # Load base model\n    base_model = AutoModelForCausalLM.from_pretrained(\n        base_id,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n    )\n    \n    print(f\"  ✓ Base model loaded\")\n    \n    # Apply LoRA\n    model = PeftModel.from_pretrained(base_model, lora_path)\n    print(f\"  ✓ LoRA adapter applied from {lora_path}\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(base_id, trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    print(f\"  ✓ Tokenizer loaded\")\n    \n    # Print memory usage\n    if torch.cuda.is_available():\n        mem_allocated = torch.cuda.memory_allocated(0) / 1e9\n        mem_reserved = torch.cuda.memory_reserved(0) / 1e9\n        print(f\"  📊 GPU Memory: {mem_allocated:.2f}GB allocated, {mem_reserved:.2f}GB reserved\")\n    \n    return model, tokenizer\n\n\ndef unload_model(model, tokenizer):\n    \"\"\"Properly unload model and free memory.\"\"\"\n    del model\n    del tokenizer\n    torch.cuda.empty_cache()\n    gc.collect()\n    print(f\"  ✓ Model unloaded, memory cleared\")\n\n\n# ============================================================================\n# SCORE EXTRACTION\n# ============================================================================\n\ndef extract_score_robust(text):\n    \"\"\"Extract score and rationale from model output.\"\"\"\n    original_text = text\n    \n    # Clean up text\n    assistant_markers = ['<|assistant|>', '[/INST]', 'assistant:', 'Assistant:']\n    for marker in assistant_markers:\n        if marker in text:\n            text = text.split(marker)[-1]\n    \n    text = text.strip()\n    \n    # Method 1: JSON\n    json_pattern = r'\\{[^}]*\"score\"\\s*:\\s*\\d+[^}]*\\}'\n    json_matches = re.findall(json_pattern, text, re.DOTALL | re.IGNORECASE)\n    \n    for match in json_matches:\n        try:\n            data = json.loads(match)\n            score = int(data.get('score', 0))\n            \n            if 1 <= score <= 5:\n                rationale = data.get('rationale', data.get('reason', data.get('explanation', '')))\n                return score, rationale, \"JSON\"\n        except:\n            continue\n    \n    # Method 2: Patterns\n    score_patterns = [\n        r'\"score\"\\s*:\\s*(\\d+)',\n        r'score\\s*:\\s*(\\d+)',\n        r'Score\\s*:\\s*(\\d+)',\n        r'\\*\\*Score\\*\\*\\s*:\\s*(\\d+)',\n    ]\n    \n    for pattern in score_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            score = int(match.group(1))\n            if 1 <= score <= 5:\n                return score, \"\", \"Pattern\"\n    \n    # Method 3: Ratio\n    ratio_patterns = [r'\\b([1-5])\\s*/\\s*5\\b', r'\\b([1-5])\\s+out\\s+of\\s+5\\b']\n    for pattern in ratio_patterns:\n        match = re.search(pattern, text, re.IGNORECASE)\n        if match:\n            return int(match.group(1)), \"\", \"Ratio\"\n    \n    # Method 4: Any digit 1-5\n    digit_matches = re.findall(r'\\b([1-5])\\b', text)\n    if digit_matches:\n        return int(digit_matches[0]), \"\", \"Weak\"\n    \n    # Failed\n    return 3, \"Extraction failed\", \"Failed\"\n\n\n# ============================================================================\n# INFERENCE\n# ============================================================================\n\ndef construct_prompt(row):\n    \"\"\"Construct evaluation prompt.\"\"\"\n    system_msg = f\"You are an expert educational evaluator specializing in Bloom's Taxonomy. You evaluate student responses for {row['bloom_level']}-level cognitive skills.\"\n    \n    user_msg = f\"\"\"Evaluate this student response for {row['bloom_level']}-level thinking according to Bloom's Taxonomy.\n\n**Prompt**: {row['prompt']}\n\n**Student Response**: {row['response']}\n\nProvide your evaluation in this EXACT JSON format:\n{{\"score\": <integer 1-5>, \"rationale\": \"<brief explanation>\"}}\n\n**Scoring Guide**:\n- 5: Excellent - Fully demonstrates {row['bloom_level']}-level thinking\n- 4: Good - Strong demonstration with minor gaps\n- 3: Adequate - Meets basic requirements\n- 2: Weak - Minimal {row['bloom_level']}-level thinking\n- 1: Poor - Does not demonstrate {row['bloom_level']}-level thinking\n\nRespond ONLY with the JSON object.\"\"\"\n\n    return [\n        {\"role\": \"system\", \"content\": system_msg},\n        {\"role\": \"user\", \"content\": user_msg}\n    ]\n\n\ndef evaluate_with_model(model, tokenizer, row, config):\n    \"\"\"Evaluate a single essay with a model.\"\"\"\n    messages = construct_prompt(row)\n    \n    try:\n        inputs = tokenizer.apply_chat_template(\n            messages,\n            return_tensors=\"pt\",\n            add_generation_prompt=True,\n            padding=True,\n            truncation=True\n        ).to(model.device)\n    except Exception as e:\n        return {\n            'score': 3,\n            'rationale': f\"Tokenization error: {e}\",\n            'extraction_method': 'Error',\n            'raw_output': ''\n        }\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs,\n            max_new_tokens=config['generation_params']['max_new_tokens'],\n            do_sample=config['generation_params']['do_sample'],\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n            use_cache=False,  # Fix for DynamicCache error\n        )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    score, rationale, method = extract_score_robust(response)\n    \n    return {\n        'score': score,\n        'rationale': rationale,\n        'extraction_method': method,\n        'raw_output': response\n    }\n\n\n# ============================================================================\n# MAIN EVALUATION (ONE MODEL AT A TIME)\n# ============================================================================\n\ndef evaluate_jury_memory_efficient(test_df, model_configs, config):\n    \"\"\"\n    Evaluate jury by loading ONE model at a time.\n    This avoids GPU OOM errors.\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"MEMORY-EFFICIENT JURY EVALUATION\")\n    print(\"=\"*80)\n    print(\"\\nStrategy: Load → Evaluate → Unload → Repeat for each model\")\n    print(\"This is slower but works with limited GPU memory.\\n\")\n    \n    # Storage for all results\n    all_model_results = {}  # {model_name: [scores for each essay]}\n    all_rationales = {}     # {model_name: [rationales for each essay]}\n    all_extraction_methods = {}  # {model_name: [methods for each essay]}\n    \n    # Evaluate with each model\n    for model_config in model_configs:\n        model_name = model_config['name']\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"EVALUATING WITH {model_name}\")\n        print(f\"{'='*80}\")\n        \n        # Load this model\n        model, tokenizer = load_single_model(\n            model_config['base_id'],\n            model_config['lora_path']\n        )\n        \n        # Evaluate all essays with this model\n        model_scores = []\n        model_rationales = []\n        model_methods = []\n        \n        print(f\"\\nEvaluating {len(test_df)} essays...\")\n        \n        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=f\"{model_name}\"):\n            result = evaluate_with_model(model, tokenizer, row, config)\n            \n            model_scores.append(result['score'])\n            model_rationales.append(result['rationale'])\n            model_methods.append(result['extraction_method'])\n        \n        # Store results\n        all_model_results[model_name] = model_scores\n        all_rationales[model_name] = model_rationales\n        all_extraction_methods[model_name] = model_methods\n        \n        # Print summary\n        extraction_success = sum(1 for m in model_methods if m in ['JSON', 'Pattern']) / len(model_methods) * 100\n        print(f\"\\n  📊 {model_name} Summary:\")\n        print(f\"     Extraction success: {extraction_success:.1f}%\")\n        print(f\"     Mean score: {np.mean(model_scores):.2f} ± {np.std(model_scores):.2f}\")\n        print(f\"     Score distribution: {Counter(model_scores)}\")\n        \n        # Unload this model before loading next\n        unload_model(model, tokenizer)\n    \n    # Aggregate jury results\n    print(f\"\\n{'='*80}\")\n    print(\"AGGREGATING JURY DECISIONS\")\n    print(f\"{'='*80}\")\n    \n    jury_scores = []\n    for i in range(len(test_df)):\n        essay_scores = [all_model_results[name][i] for name in all_model_results.keys()]\n        jury_score = round(np.mean(essay_scores))\n        jury_scores.append(jury_score)\n    \n    # Compile results\n    results = {\n        'individual_scores': all_model_results,\n        'jury_scores': jury_scores,\n        'rationales': [],\n        'extraction_methods': all_extraction_methods,\n        'true_scores': test_df['essay_score'].tolist(),\n        'bloom_levels': test_df['bloom_level'].tolist(),\n        'essay_sets': test_df['essay_set'].tolist() if 'essay_set' in test_df.columns else [None] * len(test_df),\n    }\n    \n    # Format rationales\n    for i in range(len(test_df)):\n        essay_rationales = {name: all_rationales[name][i] for name in all_rationales.keys()}\n        results['rationales'].append(essay_rationales)\n    \n    return results\n\n\n# ============================================================================\n# ANALYSIS FUNCTIONS (same as before)\n# ============================================================================\n\ndef calculate_inter_jury_agreement(results, model_names):\n    \"\"\"Calculate Cohen's kappa between models.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"INTER-JURY AGREEMENT\")\n    print(\"=\"*80)\n    \n    pairwise_kappas = {}\n    for i, name1 in enumerate(model_names):\n        for j, name2 in enumerate(model_names):\n            if i < j:\n                kappa = cohen_kappa_score(\n                    results['individual_scores'][name1],\n                    results['individual_scores'][name2],\n                    weights='linear'\n                )\n                pairwise_kappas[f\"{name1}-{name2}\"] = kappa\n                print(f\"  {name1:10s} vs {name2:10s}: κ = {kappa:.3f}\")\n    \n    mean_kappa = np.mean(list(pairwise_kappas.values()))\n    print(f\"\\n  📊 Mean κ: {mean_kappa:.3f}\")\n    \n    return {'mean_kappa': mean_kappa, 'pairwise_kappas': pairwise_kappas}\n\n\ndef analyze_bloom_discrimination(results):\n    \"\"\"Test Bloom level discrimination.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"BLOOM'S LEVEL DISCRIMINATION\")\n    print(\"=\"*80)\n    \n    df = pd.DataFrame({\n        'jury_score': results['jury_scores'],\n        'bloom_level': results['bloom_levels']\n    })\n    \n    print(\"\\nScore Distribution by Bloom Level:\")\n    print(df.groupby('bloom_level')['jury_score'].describe())\n    \n    bloom_levels = df['bloom_level'].unique()\n    \n    if len(bloom_levels) == 2:\n        level1, level2 = bloom_levels\n        scores1 = df[df['bloom_level'] == level1]['jury_score']\n        scores2 = df[df['bloom_level'] == level2]['jury_score']\n        \n        statistic, p_value = mannwhitneyu(scores1, scores2, alternative='two-sided')\n        \n        print(f\"\\n📊 Mann-Whitney U Test: {level1} vs {level2}\")\n        print(f\"  p-value: {p_value:.4f}\")\n        print(f\"  {'Significant!' if p_value < 0.05 else 'Not significant'}\")\n        \n        return {'p_value': p_value, 'significant': p_value < 0.05}\n    \n    return None\n\n\ndef calculate_asap_correlation(results):\n    \"\"\"Calculate normalized correlation with ASAP.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ASAP CORRELATION\")\n    print(\"=\"*80)\n    \n    jury_norm = [(s - 1) / 4 for s in results['jury_scores']]\n    asap_norm = []\n    \n    for score, essay_set in zip(results['true_scores'], results['essay_sets']):\n        if essay_set and essay_set in ASAP_SCORE_RANGES:\n            min_s, max_s = ASAP_SCORE_RANGES[essay_set]\n            norm = (score - min_s) / (max_s - min_s)\n            asap_norm.append(max(0, min(1, norm)))\n        else:\n            asap_norm.append(score / max(results['true_scores']))\n    \n    rho, p = spearmanr(asap_norm, jury_norm)\n    \n    print(f\"\\n📊 Spearman ρ: {rho:.3f} (p={p:.4f})\")\n    print(f\"  {'Significant!' if p < 0.05 else 'Not significant'}\")\n    \n    return {'rho': rho, 'p_value': p}\n\n\ndef export_results(results, model_names, output_dir):\n    \"\"\"Export results to CSV.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPORTING RESULTS\")\n    print(\"=\"*80)\n    \n    # Main results\n    results_df = pd.DataFrame({\n        'true_score': results['true_scores'],\n        'jury_score': results['jury_scores'],\n        'bloom_level': results['bloom_levels'],\n        **{f'{name}_score': results['individual_scores'][name] for name in model_names}\n    })\n    \n    results_df.to_csv(f\"{output_dir}/jury_results_complete.csv\", index=False)\n    print(f\"  ✓ Saved: jury_results_complete.csv\")\n\n\n# ============================================================================\n# MAIN\n# ============================================================================\n\ndef main():\n    \"\"\"Main execution.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"BloomLLM-Jury: Memory-Efficient Evaluation\")\n    print(\"=\"*80)\n    \n    # Load test data\n    print(\"\\n[1/5] Loading test data...\")\n    test_df = pd.read_pickle(CONFIG['test_data_path'])['test']\n    print(f\"  ✓ Loaded {len(test_df)} essays\")\n    \n    # Evaluate jury (one model at a time)\n    print(\"\\n[2/5] Evaluating jury...\")\n    results = evaluate_jury_memory_efficient(test_df, CONFIG['models'], CONFIG)\n    \n    # Get model names\n    model_names = [m['name'] for m in CONFIG['models']]\n    \n    # Calculate metrics\n    print(\"\\n[3/5] Calculating metrics...\")\n    agreement = calculate_inter_jury_agreement(results, model_names)\n    bloom = analyze_bloom_discrimination(results)\n    asap = calculate_asap_correlation(results)\n    \n    # Export\n    print(\"\\n[4/5] Exporting results...\")\n    export_results(results, model_names, CONFIG['output_dir'])\n    \n    # Summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"SUMMARY\")\n    print(\"=\"*80)\n    print(f\"  Inter-jury κ: {agreement['mean_kappa']:.3f}\")\n    if bloom:\n        print(f\"  Bloom p-value: {bloom['p_value']:.4f}\")\n    print(f\"  ASAP ρ: {asap['rho']:.3f}\")\n    print(\"\\n✓ Evaluation complete!\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:08:51.797609Z","iopub.execute_input":"2025-11-04T16:08:51.798263Z","iopub.status.idle":"2025-11-04T17:25:09.669681Z","shell.execute_reply.started":"2025-11-04T16:08:51.798237Z","shell.execute_reply":"2025-11-04T17:25:09.668552Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nBloomLLM-Jury: Memory-Efficient Evaluation\n================================================================================\n\n[1/5] Loading test data...\n  ✓ Loaded 50 essays\n\n[2/5] Evaluating jury...\n\n================================================================================\nMEMORY-EFFICIENT JURY EVALUATION\n================================================================================\n\nStrategy: Load → Evaluate → Unload → Repeat for each model\nThis is slower but works with limited GPU memory.\n\n\n================================================================================\nEVALUATING WITH Phi\n================================================================================\n\n================================================================================\nLoading microsoft/Phi-3-mini-4k-instruct...\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"568b27fbb25a4fc0bcec420a032b8a1a"}},"metadata":{}},{"name":"stdout","text":"  ✓ Base model loaded\n  ✓ LoRA adapter applied from /kaggle/working/models/phi3_lora\n  ✓ Tokenizer loaded\n  📊 GPU Memory: 15.12GB allocated, 15.53GB reserved\n\nEvaluating 50 essays...\n","output_type":"stream"},{"name":"stderr","text":"Phi: 100%|██████████| 50/50 [1:16:05<00:00, 91.32s/it] \n","output_type":"stream"},{"name":"stdout","text":"\n  📊 Phi Summary:\n     Extraction success: 100.0%\n     Mean score: 3.44 ± 0.92\n     Score distribution: Counter({4: 31, 2: 13, 3: 4, 5: 2})\n  ✓ Model unloaded, memory cleared\n\n================================================================================\nEVALUATING WITH Mistral\n================================================================================\n\n================================================================================\nLoading mistralai/Mistral-7B-Instruct-v0.3...\n================================================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_146/2944258838.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_146/2944258838.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Evaluate jury (one model at a time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n[2/5] Evaluating jury...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_jury_memory_efficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;31m# Get model names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_146/2944258838.py\u001b[0m in \u001b[0;36mevaluate_jury_memory_efficient\u001b[0;34m(test_df, model_configs, config)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# Load this model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         model, tokenizer = load_single_model(\n\u001b[0m\u001b[1;32m    300\u001b[0m             \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'base_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lora_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_146/2944258838.py\u001b[0m in \u001b[0;36mload_single_model\u001b[0;34m(base_id, lora_path)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# Load base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     base_model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mbase_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    601\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4818\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4820\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4822\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    118\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "],"ename":"ValueError","evalue":"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ","output_type":"error"}],"execution_count":81},{"cell_type":"code","source":"!rm -f /kaggle/working/kaggle_working_directory.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T17:30:23.068598Z","iopub.execute_input":"2025-11-04T17:30:23.069245Z","iopub.status.idle":"2025-11-04T17:30:23.685998Z","shell.execute_reply.started":"2025-11-04T17:30:23.069219Z","shell.execute_reply":"2025-11-04T17:30:23.685156Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"!rm -f /kaggle/working/working_split.z*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T17:30:38.565951Z","iopub.execute_input":"2025-11-04T17:30:38.566871Z","iopub.status.idle":"2025-11-04T17:30:39.786885Z","shell.execute_reply.started":"2025-11-04T17:30:38.566822Z","shell.execute_reply":"2025-11-04T17:30:39.785756Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"!du -sh /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T17:32:10.600448Z","iopub.execute_input":"2025-11-04T17:32:10.601203Z","iopub.status.idle":"2025-11-04T17:32:10.931532Z","shell.execute_reply.started":"2025-11-04T17:32:10.601173Z","shell.execute_reply":"2025-11-04T17:32:10.930825Z"}},"outputs":[{"name":"stdout","text":"166M\t/kaggle/working/\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"!tar -czf backup.tar.gz -C /kaggle/working .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T17:34:38.709224Z","iopub.execute_input":"2025-11-04T17:34:38.709997Z","iopub.status.idle":"2025-11-04T17:34:47.902514Z","shell.execute_reply.started":"2025-11-04T17:34:38.709966Z","shell.execute_reply":"2025-11-04T17:34:47.901750Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"tar: .: file changed as we read it\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"!ls -lh /kaggle/working/backup.tar.gz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T17:35:26.549134Z","iopub.execute_input":"2025-11-04T17:35:26.549803Z","iopub.status.idle":"2025-11-04T17:35:26.867781Z","shell.execute_reply.started":"2025-11-04T17:35:26.549775Z","shell.execute_reply":"2025-11-04T17:35:26.867057Z"}},"outputs":[{"name":"stdout","text":"-rw-r--r-- 1 root root 129M Nov  4 17:34 /kaggle/working/backup.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"!tar -tzf /kaggle/working/backup.tar.gz | head -30","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T17:35:37.035968Z","iopub.execute_input":"2025-11-04T17:35:37.036274Z","iopub.status.idle":"2025-11-04T17:35:38.877909Z","shell.execute_reply.started":"2025-11-04T17:35:37.036248Z","shell.execute_reply":"2025-11-04T17:35:38.877151Z"}},"outputs":[{"name":"stdout","text":"./\n./bcls.csv\n./data/\n./data/blooms_taxonomy_cognitive_levels.csv\n./wandb/\n./wandb/run-20251103_223127-5ke5qob7/\n./wandb/run-20251103_223127-5ke5qob7/logs/\n./wandb/run-20251103_223127-5ke5qob7/logs/debug.log\n./wandb/run-20251103_223127-5ke5qob7/logs/debug-internal.log\n./wandb/run-20251103_223127-5ke5qob7/files/\n./wandb/run-20251103_223127-5ke5qob7/files/wandb-summary.json\n./wandb/run-20251103_223127-5ke5qob7/files/output.log\n./wandb/run-20251103_223127-5ke5qob7/files/wandb-metadata.json\n./wandb/run-20251103_223127-5ke5qob7/files/requirements.txt\n./wandb/run-20251103_223127-5ke5qob7/files/config.yaml\n./wandb/run-20251103_223127-5ke5qob7/run-5ke5qob7.wandb\n./wandb/run-20251103_225045-dluzy0il/\n./wandb/run-20251103_225045-dluzy0il/run-dluzy0il.wandb\n./wandb/run-20251103_225045-dluzy0il/logs/\n./wandb/run-20251103_225045-dluzy0il/logs/debug.log\n./wandb/run-20251103_225045-dluzy0il/logs/debug-internal.log\n./wandb/run-20251103_225045-dluzy0il/files/\n./wandb/run-20251103_225045-dluzy0il/files/wandb-summary.json\n./wandb/run-20251103_225045-dluzy0il/files/output.log\n./wandb/run-20251103_225045-dluzy0il/files/wandb-metadata.json\n./wandb/run-20251103_225045-dluzy0il/files/requirements.txt\n./wandb/run-20251103_225045-dluzy0il/files/config.yaml\n./wandb/run-20251103_223143-xy700bpe/\n./wandb/run-20251103_223143-xy700bpe/logs/\n./wandb/run-20251103_223143-xy700bpe/logs/debug.log\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"tar: stdout: write error\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"!tar -tzf /kaggle/working/backup.tar.gz | wc -l","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T17:35:56.875370Z","iopub.execute_input":"2025-11-04T17:35:56.876197Z","iopub.status.idle":"2025-11-04T17:35:58.653928Z","shell.execute_reply.started":"2025-11-04T17:35:56.876161Z","shell.execute_reply":"2025-11-04T17:35:58.652966Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"397\n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"!pip install -q PyDrive","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T17:36:23.196684Z","iopub.execute_input":"2025-11-04T17:36:23.197404Z","iopub.status.idle":"2025-11-04T17:36:27.611541Z","shell.execute_reply.started":"2025-11-04T17:36:23.197375Z","shell.execute_reply":"2025-11-04T17:36:27.610551Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"# Method 1: Using gdown (reverse - upload instead of download)\n# This won't work either, so use Method 2\n\n# Method 2: Use Google Drive REST API directly\n!pip install -q google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n\nfrom google.colab import auth\nauth.authenticate_user()\n\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\n\n# Build Drive API\ndrive_service = build('drive', 'v3')\n\n# Create folder\nfolder_metadata = {\n    'name': 'BloomLLM_Kaggle_Backup',\n    'mimeType': 'application/vnd.google-apps.folder'\n}\nfolder = drive_service.files().create(body=folder_metadata, fields='id').execute()\nfolder_id = folder.get('id')\n\nprint(f\"✓ Created folder ID: {folder_id}\")\n\n# Upload file\nprint(\"\\nUploading backup.tar.gz...\")\nfile_metadata = {\n    'name': 'backup.tar.gz',\n    'parents': [folder_id]\n}\nmedia = MediaFileUpload('/kaggle/working/backup.tar.gz', resumable=True)\nfile = drive_service.files().create(\n    body=file_metadata,\n    media_body=media,\n    fields='id'\n).execute()\n\nprint(f\"\\n✅ UPLOAD COMPLETE!\")\nprint(f\"📁 View at: https://drive.google.com/drive/folders/{folder_id}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T17:39:13.449430Z","iopub.execute_input":"2025-11-04T17:39:13.449978Z","iopub.status.idle":"2025-11-04T17:39:26.944969Z","shell.execute_reply.started":"2025-11-04T17:39:13.449952Z","shell.execute_reply":"2025-11-04T17:39:26.944225Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"✓ Created folder ID: 1kwQNheHg_PFVMvr1oAGSmmgmJX5dT82E\n\nUploading backup.tar.gz...\n\n✅ UPLOAD COMPLETE!\n📁 View at: https://drive.google.com/drive/folders/1kwQNheHg_PFVMvr1oAGSmmgmJX5dT82E\n","output_type":"stream"}],"execution_count":92}]}