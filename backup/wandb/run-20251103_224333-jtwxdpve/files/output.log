Loading model in 4-bit...

============================================================
GPU PROCESS CHECK
============================================================
Mon Nov  3 22:43:51 2025
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |
| N/A   66C    P0             30W /   70W |   11591MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |
| N/A   61C    P0             29W /   70W |     103MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+


PyTorch GPU Memory:
GPU 0:
  Total: 14.74 GB
  Allocated: 7.48 GB
  Reserved: 10.88 GB
  Free: 7.26 GB
GPU 1:
  Total: 14.74 GB
  Allocated: 0.00 GB
  Reserved: 0.00 GB
  Free: 14.74 GB

============================================================
TENSORS ON GPU
============================================================
  Tensor: torch.Size([32768, 4096]), torch.bfloat16, 256.00 MB
  Tensor: torch.Size([4096]), torch.bfloat16, 0.01 MB
  Tensor: torch.Size([29360128, 1]), torch.uint8, 28.00 MB
  Tensor: torch.Size([29360128, 1]), torch.uint8, 28.00 MB
  Tensor: torch.Size([29360128, 1]), torch.uint8, 28.00 MB
  Tensor: torch.Size([4096]), torch.bfloat16, 0.01 MB
  Tensor: torch.Size([2097152, 1]), torch.uint8, 2.00 MB
  Tensor: torch.Size([8388608, 1]), torch.uint8, 8.00 MB
  Tensor: torch.Size([8388608, 1]), torch.uint8, 8.00 MB
  Tensor: torch.Size([2097152, 1]), torch.uint8, 2.00 MB
/usr/local/lib/python3.11/dist-packages/torch/__init__.py:1113: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(obj, torch.Tensor)

Total GPU tensors: 500
Total size: 7.46 GB
Using GPU: Tesla T4
Available memory: 14.74 GB
Currently allocated: 7.48 GB
Using GPU: Tesla T4
Available memory: 14.74 GB
Currently allocated: 7.48 GB
Using GPU: Tesla T4
Available memory: 14.74 GB
Currently allocated: 7.48 GB

============================================================
STARTING FINE-TUNING PIPELINE (Fixed Quantization)
============================================================
Training on 300 samples
Models to train: 3
Effective batch size: 16 (1 Ã— 16 grad accumulation)
Max sequence length: 256 tokens
LoRA rank: 4, alpha: 8
============================================================


############################################################
MODEL 1/3: Qwen/Qwen2.5-7B-Instruct
############################################################

============================================================
Fine-tuning Qwen/Qwen2.5-7B-Instruct...
============================================================
