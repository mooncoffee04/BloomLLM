Loading model in 4-bit...

============================================================
STARTING FINE-TUNING PIPELINE (Fixed Quantization)
============================================================
Training on 300 samples
Models to train: 3
Effective batch size: 16 (1 Ã— 16 grad accumulation)
Max sequence length: 256 tokens
LoRA rank: 4, alpha: 8
============================================================


############################################################
MODEL 1/3: Qwen/Qwen2.5-7B-Instruct
############################################################

============================================================
Fine-tuning Qwen/Qwen2.5-7B-Instruct...
============================================================
