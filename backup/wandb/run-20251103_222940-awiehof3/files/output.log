Loading model in 4-bit...
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.

============================================================
STARTING FINE-TUNING PIPELINE (Memory Optimized)
============================================================
Training on 300 samples
Models to train: 3
Batch size: 1, Grad accumulation: 16 (effective batch = 16)
Max length: 256 tokens (reduced for memory)
============================================================


############################################################
MODEL 1/3: Qwen/Qwen2.5-7B-Instruct
############################################################

============================================================
Fine-tuning Qwen/Qwen2.5-7B-Instruct...
============================================================
