% Core LLM-as-a-Judge Research

@article{gu2024survey,
  title={A Survey on LLM-as-a-Judge},
  author={Gu, Jiawei and others},
  journal={arXiv preprint arXiv:2411.15594},
  year={2024},
  note={\url{https://arxiv.org/abs/2411.15594}}
}

@inproceedings{li2024generation,
  title={From Generation to Judgment: Opportunities and Challenges of LLM-as-a-Judge},
  author={Li, Dawei and others},
  booktitle={Proceedings of EMNLP},
  year={2025},
  note={\url{https://arxiv.org/abs/2411.16594}}
}

@article{ye2025justice,
  title={Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge},
  author={Ye, Jiawen and others},
  journal={OpenReview},
  year={2025},
  note={\url{https://openreview.net/forum?id=3GTtZFiajM}}
}

@article{zheng2024judging,
  title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},
  author={Zheng, Lianmin and others},
  journal={arXiv preprint arXiv:2306.05685},
  year={2024}
}

% LLM-as-a-Jury Research

@article{verga2024replacing,
  title={Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models},
  author={Verga, Pat and others},
  journal={arXiv preprint arXiv:2404.18796},
  year={2024},
  note={\url{https://arxiv.org/abs/2404.18796}}
}

@misc{chan2025jury,
  title={LLM-as-a-Jury: What It Is and How to Implement},
  author={Chan, Jason and Arize AI Team},
  howpublished={Arize AI Blog},
  year={2025},
  note={\url{https://arize.com/llm-as-a-jury/}}
}

@article{elsevier2025jury,
  title={LLMs as a Jury: Bringing Quality to Quantity in GenAI R\&D},
  author={Elsevier Connect},
  journal={Elsevier Connect},
  year={2025},
  note={\url{https://www.elsevier.com/connect/llms-as-a-jury-bringing-quality-to-quantity-in-genai-aided-r-and-d}}
}

@misc{atlaai2025judge,
  title={Judge or Jury: What's the Right Approach for LLM Evaluation?},
  author={Atla-AI Team},
  howpublished={Atla-AI Blog},
  year={2025},
  note={\url{https://www.atla-ai.com/post/judge-or-jury-whats-the-right-approach-for-llm-evaluation}}
}

% Agent-as-a-Judge Research

@article{zhuge2025agent,
  title={The Rise of Agent-as-a-Judge Evaluation for LLMs},
  author={Zhuge, Ming and others},
  journal={arXiv preprint arXiv:2508.02994},
  year={2025},
  note={\url{https://arxiv.org/abs/2508.02994}}
}

@inproceedings{icml2025agent,
  title={Agent-as-a-Judge: Evaluate Agents with Agents},
  author={Anonymous},
  booktitle={ICML 2025 Poster},
  year={2025},
  note={Poster 45485, \url{https://icml.cc/virtual/2025/poster/45485}}
}

% Educational Domain Applications

@article{wang2025education,
  title={LLM Applications in Medical Education},
  author={Wang, Xiaoming and others},
  journal={medRxiv preprint},
  year={2025},
  note={\url{https://www.medrxiv.org/content/10.1101/2025.04.22.25326219v3}}
}

@article{thompson2025legal,
  title={Legal Domain LLM Evaluation},
  author={Thompson, Richard and others},
  journal={arXiv preprint arXiv:2510.07243},
  year={2025},
  note={\url{https://arxiv.org/html/2510.07243v1}}
}

% Bloom's Taxonomy

@book{bloom1956taxonomy,
  title={Taxonomy of Educational Objectives: The Classification of Educational Goals},
  author={Bloom, Benjamin S and others},
  year={1956},
  publisher={David McKay Company},
  address={New York}
}

@book{anderson2001taxonomy,
  title={A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom's Taxonomy of Educational Objectives},
  author={Anderson, Lorin W and Krathwohl, David R and others},
  year={2001},
  publisher={Longman},
  address={New York}
}

@article{su2016blooms,
  title={Bloom's Taxonomy Cognitive Levels Data Set},
  author={Su, Yu and others},
  journal={ResearchGate},
  year={2016},
  note={\url{https://www.researchgate.net/publication/303608228_Bloom's_Taxonomy_Cognitive_Levels_Data_Set}}
}

% ASAP Dataset and Essay Scoring

@misc{asap2012kaggle,
  title={The Hewlett Foundation: Automated Essay Scoring},
  author={{The Hewlett Foundation}},
  howpublished={Kaggle Competition},
  year={2012},
  note={\url{https://www.kaggle.com/competitions/asap-aes}}
}

@inproceedings{yannakoudakis2011asap,
  title={A New Dataset and Method for Automatically Grading {ESOL} Texts},
  author={Yannakoudakis, Helen and Briscoe, Ted and Medlock, Ben},
  booktitle={Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  pages={180--189},
  year={2011}
}

@inproceedings{rodriguez2019asap,
  title={Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input},
  author={Rodriguez, Piotr and others},
  booktitle={Proceedings of NAACL-HLT},
  pages={263--271},
  year={2019}
}

% LLMs for Labeling and Classification

@inproceedings{abdelrahman2025llm,
  title={Choice of LLMs over NLP for Educational Content Labeling},
  author={Abdelrahman, Ghader and others},
  booktitle={Proceedings of the Workshop on Innovative Use of NLP for Building Educational Applications (BEA)},
  year={2025},
  note={\url{https://aclanthology.org/2025.bea-1.32.pdf}}
}

@inproceedings{bea2018prompts,
  title={Characterizing Educational Questions using Multi-Dimensional Item Response Theory},
  author={Benedetto, Luca and others},
  booktitle={Proceedings of the Workshop on Innovative Use of NLP for Building Educational Applications (BEA)},
  year={2018},
  note={\url{https://aclanthology.org/L18-1187.pdf}}
}

% Parameter-Efficient Fine-Tuning

@inproceedings{hu2021lora,
  title={{LoRA}: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2022},
  note={\url{https://arxiv.org/abs/2106.09685}}
}

% Model Architectures

@article{phi3,
  title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
  author={{Microsoft}},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024},
  note={\url{https://arxiv.org/abs/2404.14219}}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023},
  note={\url{https://arxiv.org/abs/2310.06825}}
}

@misc{qwen2023,
  title={Qwen2.5: A Party of Foundation Models},
  author={{Qwen Team}},
  howpublished={Alibaba Cloud},
  year={2023},
  note={\url{https://qwenlm.github.io/blog/qwen2.5/}}
}

% General Educational AI

@article{kasneci2023chatgpt,
  title={{ChatGPT} for Good? On Opportunities and Challenges of Large Language Models for Education},
  author={Kasneci, Enkelejda and Sessler, Kathrin and K{\"u}chemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and G{\"u}nnemann, Stephan and H{\"u}llermeier, Eyke and others},
  journal={Learning and Individual Differences},
  volume={103},
  pages={102274},
  year={2023},
  publisher={Elsevier}
}

% Benchmarks and Resources

@misc{github2024awesome,
  title={Awesome-LLM-as-a-Judge},
  author={{LLM-as-a-Judge Community}},
  howpublished={GitHub Repository},
  year={2024},
  note={\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge}}
}

@misc{github2024haitao,
  title={Awesome-LLMs-as-Judges},
  author={Haitao, CS and others},
  howpublished={GitHub Repository},
  year={2024},
  note={\url{https://github.com/CSHaitao/Awesome-LLMs-as-Judges}}
}

% Additional Educational Resources

@misc{osf2024blooms,
  title={Bloom's Taxonomy Educational Dataset},
  author={{Open Science Framework}},
  howpublished={OSF Repository},
  year={2024},
  note={\url{https://osf.io/9fdrw/files}}
}

% Code Evaluation

@article{ieee2025code,
  title={On the Effectiveness of LLM-as-a-Judge for Code Evaluation},
  author={Zhang, Wei and others},
  journal={IEEE Transactions on Software Engineering},
  year={2025},
  note={\url{https://www.computer.org/csdl/journal/ts/2025/08/11071936/2851vlBjr9e}}
}

% Large-Scale Empirical Studies

@inproceedings{acl2025empirical,
  title={LLMs Instead of Human Judges? A Large-Scale Empirical Study},
  author={Liu, Yang and others},
  booktitle={Proceedings of ACL},
  year={2025},
  note={\url{https://aclanthology.org/2025.acl-short.20.pdf}}
}

% Search and Information Retrieval

@article{frontiers2025search,
  title={LLM-as-a-Judge: Automated Evaluation of Search Query Interpretation},
  author={Chen, Michael and others},
  journal={Frontiers in Big Data},
  volume={8},
  year={2025},
  note={\url{https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2025.1611389/full}}
}

% Meta-Evaluation Frameworks

@misc{arize2025meta,
  title={Meta-Judge Pipeline and JudgeBench},
  author={{Arize AI}},
  howpublished={Arize AI Technical Report},
  year={2025},
  note={\url{https://arize.com/blog/meta-judge-pipeline/}}
}

% Additional LLM Evaluation Resources

@misc{evidentlyai2024guide,
  title={LLM Evaluation Guide: LLM-as-a-Judge},
  author={{Evidently AI}},
  howpublished={Evidently AI Documentation},
  year={2024},
  note={\url{https://www.evidentlyai.com/llm-guide/llm-as-a-judge}}
}

@misc{emergentmind2024,
  title={LLM-as-a-Judge Evaluation: Papers and Insights},
  author={{Emergent Mind}},
  howpublished={Emergent Mind Topic Page},
  year={2024},
  note={\url{https://www.emergentmind.com/topics/llm-as-a-judge-evaluation}}
}

% Gemini API Reference

@misc{google2024gemini,
  title={Gemini API Documentation},
  author={{Google DeepMind}},
  howpublished={Google AI Studio},
  year={2024},
  note={\url{https://ai.google.dev/gemini-api}}
}

% Weights & Biases

@misc{wandb2024,
  title={Weights \& Biases: Developer Tools for ML},
  author={{Weights \& Biases}},
  howpublished={wandb.ai},
  year={2024},
  note={\url{https://wandb.ai}}
}

% Hugging Face Resources

@misc{huggingface2024transformers,
  title={Transformers: State-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX},
  author={{Hugging Face}},
  howpublished={Hugging Face Documentation},
  year={2024},
  note={\url{https://huggingface.co/docs/transformers}}
}

@misc{huggingface2024peft,
  title={{PEFT}: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware},
  author={{Hugging Face}},
  howpublished={Hugging Face Library},
  year={2024},
  note={\url{https://github.com/huggingface/peft}}
}
